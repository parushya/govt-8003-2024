[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference and Research Design",
    "section": "",
    "text": "Why are we here?\n\nThis web book is meant to be one stop-resource for course related labs and some interesting stuff. The detailed course material is available on the Canvas Page.\nYou can find here all the code, links to replication data, and notes from the lab session. We will be using R for all the analysis work in this course.\nI will update the session wise pages as we go through them.\nEveryone should have the following on their systems:\n\nR - Statistical Programming Language.\nRStudio - Interactive Development Environment for R.\n\\(\\LaTeX\\) (pronounced “LAY-tek” or “LAH-Tek) Typesetting tool for preparing high-quality professional documents.\n\nYou can check the installation of \\(\\LaTeX\\) for R by following the instructions here.\nIt would be useful if you are familiar with tidyverse framework. If not, or if you need a refresher, the online book here by Hadley Wickham is a great resource.\nIf you are fairly confident, the amazing song below is nevertheless a good recall heuristic!",
    "crumbs": [
      "Why are we here?"
    ]
  },
  {
    "objectID": "latex-quarto.html",
    "href": "latex-quarto.html",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "",
    "text": "Today’s Lab",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#todays-lab",
    "href": "latex-quarto.html#todays-lab",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "",
    "text": "Good Coding\n\\(\\LaTeX\\)\nQuarto\n\n\nYAML\nCode Chunks\nMarkdown text\n\n\nR Projects\n\n\nhere package\n\n\nFolder Structure",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#good-coding",
    "href": "latex-quarto.html#good-coding",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Good Coding",
    "text": "Good Coding\nGood programming or coding is closely related to the idea of Literate Statistical Programming. As Donald Knuth (1984) defines, it is a way to write programs that focuses on explaining to human readers what we want the computers to do, rather than just instructing the computers to do so.\nStatistical Programming , hence, is about formalizing your thinking about how you treat the data and using functional programming to automate such formalized tasks to be done repetitively. It improves efficiency, enhances reproducibility, and boosts creativity when it comes to finding new patterns in your data.\nGuidelines for data and statistical analyses:1\n\nAccuracy: Write a code that reduces the chances of making an error and lets you catch one if it occurs.\nEfficiency: If you are doing it twice, see the pattern of your decision-making and formalize it in your code. Difference between Excel and coding\nReplicate-and-Reproduce: Ability to repeat the computational process which reflects your thinking and decisions that you took along the way. Improves transparency and forces one to be deliberate and responsible about choices during analyses.\n\nHuman Interpretability: Writing code is not just about analyzing but allowing yourself and then others to be able to understand your analytic choices.\n\nPublic Good: Research is a public good. And the code allows your research to be truly accessible. This means you write a code that anyone else who understands the language can read, reuse, and recreate without you being present. We essentially ensure that by writing a readable and ideally publicly accessible code.\n\nFurther, writing good code could also benefit from some common guidelines used across coders. A good starting point is the tidyverse style guide.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#latex",
    "href": "latex-quarto.html#latex",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\n\\(\\LaTeX\\) (pronounced “LAY-tek” or “LAH-Tek) is a typesetting tool for preparing high-quality professional documents. It is the preferred typesetting tool used in high-end scientific documentation task.It is not a word-processing tool. It is a simple tool without too many priors about how the document should look like.\n\\(\\LaTeX\\) gives us superior control over how your document look like, has enhanced capabilities to write technical specifications (Maths, stats, proofs, etc.), include code, and produces readily editable back-end documents.\nThere are many interfaces that allow you to work with \\(\\LaTeX\\). Overleaf is a widely used online platform and Texmaker is a popular offline application.\nHowever, RStudio has in-built capability to double as a \\(\\LaTeX\\) editor. Previously RMarkdown and now Quarto have capabilities that you can harness to achieve professional and beautifully typeset documents.\nThink of writing an equation like:\n\\[\nViolence_{i,j} = \\beta_0 + \\beta_1EthnicFractionalization_i + \\gamma_j + \\epsilon_i\n\\] In Latex, using quarto, you have to write something like the following:\n$Violence_{i,j} = \\\\beta_0 + \\\\beta_1EthnicFractionalization_i + \\\\gamma_j + \\\\epsilon_i$\nFor a single line of text we encapsulate code by $ sign.\nFor multi-line code we use $$.\nRead more about \\(\\LaTeX\\) here\nThe box folder has some detailed resources for helping with typesetting in \\(\\LaTeX\\).\n\n\n\n\n\n\nTo Do\nFollow these instructions to install library(tinytex).\n\n\n\nThis can also happen, btw!",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#quarto",
    "href": "latex-quarto.html#quarto",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a literate statistical programming tool.\nQuarto can include code from not just R, but also Python, Julia, Stata and many other languages/tools.\nQuarto allows you to include the good coding guidelines that we discussed above. It provides you with capability to write code and perform data analysis using R, write text that is part of any professional communication, and include mathematical symbols and equations in a well typeset format. Essentially, it allows you to work on a manuscript with data analysis at one place.\nHere is some cool stuff that you can do with quarto.\n\n\n\n\n\n\nExercise 1\n\nOpen a new quarto document by File &gt; New File &gt; Quarto Document.\nUse Render button on top on scripts panel to save and get a .pdf output.\n\n\n\n\nA Quarto document is saved as a .qmd file. You can edit this file in two ways: Programmatically by being in source button and visually by choosing the Visual button, both button on top left corner of the .qmd window. More details about workign with Quarto can be found on the quarto website here.\nThere are three building blocks in a .qmd file:\n\n\nYAML\nShort for Yet-Another-Markup-Languge\nThis is the part we see sandwiched between two --- at the strat of .qmd file. Here we define different global settings for the particular document.\nCurrently, we see\n---\ntitle: \"Untitled\"\nformat: html\n---\nWe can add many more options here to modify the details to appear at the start of the document. Here’s an example from quarto reference site\n\n\n\n\n\n\n---\ntitle: \"Toward a Unified Theory of High-Energy Metaphysics\"\ndate: 2008-02-29\nauthor:\n  - name: Josiah Carberry\n    id: jc\n    orcid: 0000-0002-1825-0097\n    email: josiah@psychoceramics.org\n    affiliation: \n      - name: Brown University\n        city: Providence\n        state: RI\n        url: www.brown.edu\nabstract: &gt; \n  The characteristic theme of the works of Stone is \n  the bridge between culture and society. ...\nkeywords:\n  - Metaphysics\n  - String Theory\nlicense: \"CC BY\"\ncopyright: \n  holder: Josiah Carberry\n  year: 2008\ncitation: \n  container-title: Journal of Psychoceramics\n  volume: 1\n  issue: 1\n  doi: 10.5555/12345678\nfunding: \"The author received no specific funding for this work.\"\n---\n\n\n\nOr, global settings for different formats of outputs like html or pdf, as follows\n\n\n\n\n\n\n---\ntitle: \"My Document\"\nformat: \n  html:\n    fig-width: 8\n    fig-height: 6\n  pdf:\n    fig-width: 7\n    fig-height: 5\n---\n\n\n\n\n\nCode Chunks\nYou can start a new R code chunk by pressing cmd + option + I or ctrl + alt + I.\nYou can also do this with the Insert button icon in the editor toolbar or by manually typing the chunk delimiters ```{r} and ```.\nTry to use the keyboard shortcut more often as it will save you a ton of time later.\nR code chunks are surrounded by ```{r} and ```.\nYou can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter.\n#| eval: true # Do evaluate this chunk\n#| echo: true # Do show this chunk in the final rendered document\n#| output: true # Do show the output / results of this chunk in the rendered document\n\nprint(\"Dont run this code\")\nRStudio executes the code and displays the results below the code.\nIf you don’t like seeing your plots and output in your document and would rather make use of RStudio’s Console and Plot panes, you can click on the gear icon next to “Render” and switch to “Chunk Output in Console”.\nA chunk should be relatively self-contained, and focused around a single task.\n\n\n\n\n\n\nExercise\n\nAdd a code chunk at the bottom of the .qmd file you created.\nAdd some simple mathematical operations.\nRun the code chunk separately, and then the whole file by pressing Render button from the top.\n\n\n\n\nCode chunk options are included in a special comment at the top of the block (lines at the top prefaced with #| are considered options). More on code chunk options here\nOptions available for customizing output include:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output).\n\n\necho\nInclude the source code in output\n\n\noutput\nInclude the results of executing the code in the output (true, false, or asis to indicate that the output is raw markdown and should not have any of Quarto’s standard enclosing markdown).\n\n\nwarning\nInclude warnings in the output.\n\n\nerror\nInclude errors in the output (note that this implies that errors executing code will not halt processing of the document).\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block).\n\n\n\nYou can also add these options as global options in the YAML by writing them under execute option like:\n---\nexecute: \n  echo: true\n  inlcude: false\n---\nThe following table summarizes which types of output each option suppresses:2\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nRun code\nShow code\nOutput\nPlots\nMessages\nWarnings\n\n\n\n\neval: false\nX\n\nX\nX\nX\nX\n\n\ninclude: false\n\nX\nX\nX\nX\nX\n\n\necho: false\n\nX\n\n\n\n\n\n\nresults: hide\n\n\nX\n\n\n\n\n\nfig-show: hide\n\n\n\nX\n\n\n\n\nmessage: false\n\n\n\n\nX\n\n\n\nwarning: false\n\n\n\n\n\nX\n\n\n\n\nInline code\nWe can also embed R code into a Quarto document: directly into the text, with: ```{r} &lt;code&gt; ```.\nFor example: ```{r} (2+2)```.\n\n\n\nMarkdown Text\nMarkdown text is like any other text just with some special considerations.\nYou can see the help section from R to see some of the basic formatting tips.\n\n\n\nR Markdown Help\n\n\n\n\n\n\n\n\nThese are some of the regularly used formatting options in RMarkdown/Quarto Titles and subtitles ————————————————————\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nPractice\nLet’s try all that we learnt\n\nExercise\n\nDelete the existing code, except yaml on top, in the .qmd file that we created today.\nAdd some simple mathematical operations like addition, subtraction, or mutliplication. Now, in the chunk set the options differently. You could play with different options that we learnt above and their values. Use &lt;TAB&gt; button to see different values that you can provide to chunk options.\n\n```{r}\n#| echo: true\n#| output: asis\n\n1 + 1\n```\n[1] 2\n\nAdd two separate R chunks. In one, load the datset from the paper that you want to replicate. In second, add a simple select or filter functionality.\n\n```{r}\n#| echo: false \n#| message: false\n#| warning: false\n\n# Loading packages\nlibrary(tidyverse) # For tidyverse\nlibrary(janitor) # For Janitor\n\n# Loading Dataset\nvdem_df &lt;- readRDS(\"Datasets-mathcamp/V-Dem-CY-Full+Others-v12.rds\") %&gt;% \nclean_names() \n\n# ` %&gt;% ` is the piping operator from tidyverse universe\n\n# `clean_names` cleans the names of columns and standardizes them | from Janitor package\n```\n```{r}\n#| echo: false  # Toggle with options as well the paramters like true/false etc\n#| message: false\n#| warning: false\nvdem_2021 &lt;- vdem_df %&gt;% \n  filter(year == 2021) %&gt;%  # To filter values according to one variable\n  select(year, country_name, v2x_libdem, e_gdppc, e_pop) # To select particular variables\n```\n\nWrite the model specification that is mentioned in your paper in \\(\\LaTeX\\) in your quarto document. An example is given below. You can use the resources on latex from course Canvas page.\n\n\nModel EquationModel Latex Code\n\n\n\\(LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i\\)\n\n\n$LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i$\n\n\n\n\n\nRender the whole document into a .pdf with your name in the YAML and today’s date. You may use the following YAML with modifications.\n\n\n\n\n\n\nBelow is a yaml that you can use in your assignments and documents with modifications.\n---\ntitle: My First Latex Document\nsubtitle: Govt 8003\nauthor: &lt;Your Name&gt;\ndate: today\nformat:\n  pdf:\n    highlight-style: kate\n    citation_package: natbib\n  docx: default\nalways_allow_html: true\ngeometry: margin=1.2in\nfontsize: 12pt\nlinestretch: 1.5\nlinkcolor: blue\ntoc: true\nlink-citations: true\neditor_options: \n  chunk_output_type: console\nexecute: \n  fig-height: 6\n  fig-width: 8.5\n  fig-pos: \"!t\"\nkeep_tex: true\nwhitespace: small\n---\n\n“If you think your thought is not making sense, write it in** \\(\\LaTeX\\).\n\n\nIt will at now not make sense in a beautiful way\n\n\n-Buddha (500 B.C.E.)",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#exercise",
    "href": "latex-quarto.html#exercise",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Exercise",
    "text": "Exercise\n\nDelete the existing code, except yaml on top, in the .qmd file that we created today.\nAdd some simple mathematical operations like addition, subtraction, or mutliplication. Now, in the chunk set the options differently. You could play with different options that we learnt above and their values. Use &lt;TAB&gt; button to see different values that you can provide to chunk options.\n\n```{r}\n#| echo: true\n#| output: asis\n\n1 + 1\n```\n[1] 2\n\nAdd two separate R chunks. In one, load the datset from the paper that you want to replicate. In second, add a simple select or filter functionality.\n\n```{r}\n#| echo: false \n#| message: false\n#| warning: false\n\n# Loading packages\nlibrary(tidyverse) # For tidyverse\nlibrary(janitor) # For Janitor\n\n# Loading Dataset\nvdem_df &lt;- readRDS(\"Datasets-mathcamp/V-Dem-CY-Full+Others-v12.rds\") %&gt;% \nclean_names() \n\n# ` %&gt;% ` is the piping operator from tidyverse universe\n\n# `clean_names` cleans the names of columns and standardizes them | from Janitor package\n```\n```{r}\n#| echo: false  # Toggle with options as well the paramters like true/false etc\n#| message: false\n#| warning: false\nvdem_2021 &lt;- vdem_df %&gt;% \n  filter(year == 2021) %&gt;%  # To filter values according to one variable\n  select(year, country_name, v2x_libdem, e_gdppc, e_pop) # To select particular variables\n```\n\nWrite the model specification that is mentioned in your paper in \\(\\LaTeX\\) in your quarto document. An example is given below. You can use the resources on latex from course Canvas page.\n\n\nModel EquationModel Latex Code\n\n\n\\(LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i\\)\n\n\n$LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i$\n\n\n\n\n\nRender the whole document into a .pdf with your name in the YAML and today’s date. You may use the following YAML with modifications.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#r-projects",
    "href": "latex-quarto.html#r-projects",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "R Projects",
    "text": "R Projects\nWe often use the setwd() command to trace the files we need in our work. As work expands, projects will have multiple datasets to be loaded, different subsidiary scripts to be used, and multiple outputs to be saved.\nA first order problem related to both file management and reproducability of code is the usage of file paths. Using absolute paths, like ~/User/MyName/Documents/..... becomes cumbersome and also inhibits efficiency of reproducability. Every time someone else runs the script, they will have to change the file paths in all the instances in Rscripts or .qmd file to locate the related datasets as well as other objects. Similarly, there would be issues with saving objects in new places. A partially efficient way we use involves using setwd() to direct R to a new working directory; this is also called usage of relative paths\nR Projects is a built-in mechanism in RStudio for seamless file management and usage of relative paths.\nLet’s start by creating a new project. Click File &gt; New Project. Name the new project govt-8003.\n\n\n\n\n\n\n\n\nFigure 1: To create new project: (top) first click New Directory, then (middle) click New Project, then (bottom) fill in the directory (project) name, choose a good subdirectory for its home and click Create Project. source\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\nClose the new project just created\nGo to the folder on your system, and click the .RProj file.\nStart a new qmd file like we did before. Delete existing code except for YAML. Run getwd() command in console and see the difference.\nStart a new R code chunk (cmd + option + I) and load replication dataset. Notice the change in behavior when you press TAB inside the quotes for selecting path.\n\n\n\n\n\nhere package\nAn efficient file and folder management system is going to be crucial as we move into working with serious projects. As stressed earlier, keeping and using all the files associated with a project in a comprehensible folder system is facilitated by R Projects. You would ideally want to create your own template for folder management that you follow across projects. For starters, the folder structure below is the one created for your data essay assignment in Govt 8001 or Quant 1.\nYou can use the point-and-click functionality in your computers to create this structure. Later today, we will briefly go through an R script that do this programmatically.\n📦 govt-8003\n├─ govt-8003.RProj\n├─ 000-setup.R\n├─ 001-eda.qmd\n├─ 002-analysis.qmd\n└─ 003-manuscript.qmd\n├─ Data\n│  ├─ Raw\n│  │  ├─ Dataset1\n│  │  │  ├─ dataset1.csv\n│  │  │  └─ codebook-dataset1.pdf\n│  │  └─ Dataset2\n│  │     ├─ ...dta\n│  │     └─ codebook-dataset2.pdf\n│  └─ Clean\n│     └─ Merged-df1-df2.csv\n├─ Scripts\n│  ├─ R-scripts\n│  │  ├─ plotting-some-variable.R\n│  │  └─ exploring-different-models.R\n│  ├─ Stata-Scripts\n│  │  └─ seeing-variable-labels.do\n│  └─ Python-Scripts\n│     └─ scraping-data-from-website.py\n└─ Outputs\n   ├─ Plots\n   │  ├─ ...jpeg\n   │  └─ ...png\n   ├─ Tables\n   │  └─ .csv\n   └─ Text\n      └─ ...txt\nSuggested folder structure for a new academic project\nWhile we learnt how to create or associate an .RProj with a folder, integrating it with here() function from the here package, makes things further smoother. Let’s do it with the following exercise.\n\n\n\n\n\n\nExercise\n\nGo the RStudio window with govt-8003 project. Check the extreme upper left corner to see if you are in the correct window.\nIn the qmd file we were working in, add an R chunk.\nLoad the library here with the following code. Run the code line by line\n\n\nlibrary(here)\n\n\n # See the output for each of the following lines | Use your own datasets\nhere()\n\n# Make modification here after copying your dataset to this folder\n\nhere(\"Datasets-mathcamp\",\"V-Dem-CY-Full+Others-v12.rds\")\n\n# syntax is\n\n# here(\"First subfolder from the root folder\", \"second subfolder\",...., \"file\")\n\n\nvdem_new &lt;- readRDS(here(\"Datasets-mathcamp\",\"V-Dem-CY-Full+Others-v12.rds\"))\n\nThis is a cleaner syntax which when coupled with usage of R projects saves time in typing file paths and avoids issues when the project is run on some other computer system.\nNote: here() always notes the path from the main folder or the root directory where your .RProj file is located.\nSave the files and close the govt-8003 project window\n\n\n\nMake it a habit of using R Projects and here() function in your scripts for writing portable code.\nYou can read this quick and informative blogpost on using these two here.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#folder-structure",
    "href": "latex-quarto.html#folder-structure",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Folder Structure",
    "text": "Folder Structure\nWe ideally want a folder structure that is easily understandable to us and others.\n📦 govt-8003\n├─ govt-8003.RProj\n├─ 000-setup.R\n├─ 001-eda.qmd\n├─ 002-analysis.qmd\n└─ 003-manuscript.qmd\n├─ Data\n│  ├─ Raw\n│  │  ├─ Dataset1\n│  │  │  ├─ dataset1.csv\n│  │  │  └─ codebook-dataset1.pdf\n│  │  └─ Dataset2\n│  │     ├─ ...dta\n│  │     └─ codebook-dataset2.pdf\n│  └─ Clean\n│     └─ Merged-df1-df2.csv\n├─ Scripts\n│  ├─ R-scripts\n│  │  ├─ plotting-some-variable.R\n│  │  └─ exploring-different-models.R\n│  ├─ Stata-Scripts\n│  │  └─ seeing-variable-labels.do\n│  └─ Python-Scripts\n│     └─ scraping-data-from-website.py\n└─ Outputs\n   ├─ Plots\n   │  ├─ ...jpeg\n   │  └─ ...png\n   ├─ Tables\n   │  └─ .csv\n   └─ Text\n      └─ ...txt\nWe can create this structure by using point and click system on our laptops. But since we might want to use the same folder structure repetitively it will make sense to be lazy and do it programmatically.\n\n\n\n\n\n\nExercise\n\nDownload the 000-setup.R from here\nPlace it in the govt-8003 folder.\nOpen it in the opened RStudio window.\n\n```{r}\n# Name: 000-setup.R\n# Author: Parushya\n# Purpose: Creates main folders, subfolders in the main project directory\n# Will also ensure that you have basic packages required to run the repository\n# Date Created: 2020/10/07\n\n\n\n# Checking if packages are installed and installing\n\n\n# check.packages function: install and load multiple R packages.\n# Found this function here: https://gist.github.com/smithdanielle/9913897 on 2019/06/17\n# Check to see if packages are installed. Install them if they are not, then load them into the R session.\n\ncheck.packages &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n  sapply(pkg, require, character.only = TRUE)\n}\n\n# Check if packages are installed and loaded:\npackages &lt;- c(\"janitor\",  \"tidyverse\", \"utils\", \"here\")\ncheck.packages(packages)\n\n\n# Setting Directories and creating subfolders\n\n\n# Creating Sub Folders\n\n## Data\ndir.create(file.path(paste0(here(\"Data\")))) # Data Folder\ndir.create(file.path(paste0(here(\"Data\",\"Raw\")))) # Raw Data sub-folder\ndir.create(file.path(paste0(here(\"Data\",\"Clean\")))) # Clean Data sub-folder\n\n\n# Scripts\ndir.create(file.path(paste0(here(\"Scripts\")))) # Scripts Folder\ndir.create(file.path(paste0(here(\"Scripts\",\"RScripts\")))) # RScripts  sub-folder\ndir.create(file.path(paste0(here(\"Scripts\",\"Stata-Scripts\")))) # Stata Scripts sub-folder\ndir.create(file.path(paste0(here(\"Scripts\",\"Python-Scripts\")))) # Python Scripts sub-folder\n\n\n# Output\ndir.create(file.path(paste0(here(\"Outputs\")))) # Outputs Folder\ndir.create(file.path(paste0(here(\"Outputs\",\"figures\")))) # Figures sub-folder\ndir.create(file.path(paste0(here(\"Outputs\",\"tables\")))) # Tables sub-folder\ndir.create(file.path(paste0(here(\"Outputs\",\"text\")))) # Text sub-folder\n\n```\n\nRun the file line-by-line. See the folder structure created in your main folder.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#plan-concept-of-a-plan",
    "href": "latex-quarto.html#plan-concept-of-a-plan",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Plan Concept of a Plan",
    "text": "Plan Concept of a Plan\nHere’s a quick workflow for starting a new project or assignment or paper.\n\nCreate a new Rstudio Project by clicking File &gt; New Project. Name it govt-&lt;coursecode&gt;-&lt;project.\nCheck if now your RStudio Window shows the project name on top right corner. If not, go to folder and double-click the .RProj file.\nPaste the 000-setup.R file in the main project folder. Open it in the same Rstudio window with the project and run the complete file. Your folder structure is created.\nCopy your raw data in Data/Raw folder. Similarly, your scripts in Scripts/RScripts folder\nStart your new .qmd file and save it in the main folder.\nRemember to use here() package extensively in both, scripts and quarto file, when loading or saving the data.\nYou can always zip the whole project folder for sharing. The receiver will just need to unzip and run the code after starting the associated .RProj file, without changing file paths on their computer.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#footnotes",
    "href": "latex-quarto.html#footnotes",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "",
    "text": "Inspired by the summary provided by Prof Aaron Williams’ course on Data Analysis offered at McCourt School. Strongly recommended to learn good coding using R↩︎\nThis section is copied from R4DS book↩︎",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "matching.html",
    "href": "matching.html",
    "title": "Matching",
    "section": "",
    "text": "Building the intuition: Stratification or Subclassification\nFor approaching the causal effects, in the language of DAGs, we have to close all the backdoor paths between our treatment and outcome of interest. Let’s see that with an illustrative example from Causal Inference: The Mixtape (Cunningham, n.d.)\nResearch Question: Would being in First Class in titanic have increased the chances of survival? As a causal effect can we isolate it?\nFirst class was located higher on the ship and therefore likely led to a greater chance of getting into one of the few lifeboats onboard. We could estimate the ATE of being in first class like this:\n# Function to load data from Causal Mixtape github page\nread_data &lt;- function(df)\n{\n  full_path &lt;- paste(\"https://github.com/scunning1975/mixtape/raw/master/\", \n                     df, sep = \"\")\n  print(full_path) # To show what was the final url accessed\n  df &lt;- read_dta(full_path)\n  return(df)\n}\n# load data and create a variable \"d\" for individuals in first class\ntitanic &lt;- read_data(\"titanic.dta\") %&gt;% \n  mutate(d = case_when(class == 1 ~ 1, TRUE ~ 0))\n\n[1] \"https://github.com/scunning1975/mixtape/raw/master/titanic.dta\"\n\n# calculate expected outcome for the treatment group - first class \ney1 &lt;- titanic %&gt;% \n  filter(d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\ney1\n\n[1] 0.6246154\n\n# calculate expected outcome for the control group - all other classes onboard \ney0 &lt;- titanic %&gt;% \n  filter(d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\ney0\n\n[1] 0.2707889\n\n# calculate the simple difference in outcomes \n(sdo &lt;- ey1 - ey0)\n\n[1] 0.3538265\nSay in our research, we learn that women and children were more likely to be in first class because only wealthy families could afford this kind of travel. Additionally, the social norm during the boarding of the lifeboats was that women and children got first priority. Now we have a problem because being a woman or a child are confounders, which means that our estimate of the ATE is biased.\nWe can handle this via stratification by a bit of data wrangling\n# create a variable to indicate each of our strata - two for sex (male = 1), and two for age(adult = 1)\ntitanic = titanic %&gt;%\n  mutate(s = case_when(sex == 0 & age == 1 ~ 1,\n                       sex == 0 & age == 0 ~ 2,\n                       sex == 1 & age == 1 ~ 3,\n                       sex == 1 & age == 0 ~ 4,\n                       TRUE ~ 0))\n# create treatment variable for those in first class\ntitanic &lt;- titanic %&gt;% \n  mutate(d = case_when(class == 1 ~ 1, TRUE ~ 0))\n\n# calculate survival rate for each pair of strata and treatment\ney11 &lt;- titanic %&gt;% \n  filter(s == 1 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney10 &lt;- titanic %&gt;% \n  filter(s == 1 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney21 &lt;- titanic %&gt;% \n  filter(s == 2 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney20 &lt;- titanic %&gt;% \n  filter(s == 2 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney31 &lt;- titanic %&gt;% \n  filter(s == 3 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney30 &lt;- titanic %&gt;% \n  filter(s == 3 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney41 &lt;- titanic %&gt;% \n  filter(s == 4 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney40 &lt;- titanic %&gt;% \n  filter(s == 4 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\n# calculate differences between strata\ndiff1 = ey11 - ey10\ndiff2 = ey21 - ey20\ndiff3 = ey31 - ey30\ndiff4 = ey41 - ey40\n\n# calculate number of control units \nobs = nrow(titanic %&gt;% filter(d == 0))\n\n# create weights for each strata\nwt1 &lt;- titanic %&gt;% \n  filter(s == 1 & d == 0) %&gt;%\n  nrow(.)/obs\n\nwt2 &lt;- titanic %&gt;% \n  filter(s == 2 & d == 0) %&gt;%\n  nrow(.)/obs\n\nwt3 &lt;- titanic %&gt;% \n  filter(s == 3 & d == 0) %&gt;%\n  nrow(.)/obs\n\nwt4 &lt;- titanic %&gt;% \n  filter(s == 4 & d == 0) %&gt;%\n  nrow(.)/obs\n\n# calculate weighted average treatment effect\n(wate = diff1*wt1 + diff2*wt2 + diff3*wt3 + diff4*wt4)\n\n[1] 0.1887847",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#exact-matching",
    "href": "matching.html#exact-matching",
    "title": "Matching",
    "section": "Exact Matching",
    "text": "Exact Matching\nExample from Causal Inference: The Mixtape by Scott Cunningham.\nDo training programs have a causal effect on earnings?.\nBut, in this very simple example, we notice that individuals who participated in the training program are younger on average than those in the control group (What type of bias is this?).\nThis violates the conditional independence assumption. We can address this using exact matching on age as follows:\n\n# load data \ntraining_example &lt;- read_data(\"training_example.dta\") %&gt;% \n  slice(1:20)\n\n[1] \"https://github.com/scunning1975/mixtape/raw/master/training_example.dta\"\n\n\n```{r}\n#| echo: true\n#| output: asis\n\n# view data table to see what matching on age looks like \nView(training_example)\n```\nThis dataset is for illustration where we have exact matches for all units in the treated group on the covariate of age from the control group.\n\n\n\n\n\n\n\n\nLet’s see the distribution of covariate age in the two groups.\n::: {.panel-tabset}\n\nTreatment Group\n\n# histogram of age for treatment group - mean at red line\nggplot(training_example, aes(x=age_treat)) +\n  geom_histogram(bins = 10, na.rm = TRUE) +\n  geom_vline(aes(xintercept = mean(age_treat, na.rm = T)), color = \"red\")\n\n\n\n\n\n\n\n\n\n\nControl Group\n\n# histogram of age for control group - mean at red line\nggplot(training_example, aes(x=age_control)) +\n  geom_histogram(bins = 10, na.rm = TRUE) +\n  geom_vline(aes(xintercept = mean(age_control, na.rm = T)), color = \"red\")\n\n\n\n\n\n\n\n\n::: \nAfter exact matching\n\n# histogram of age for matched group - mean at red line\nggplot(training_example, aes(x=age_matched))+\n  geom_histogram(bins = 10, na.rm = TRUE) +\n  geom_vline(aes(xintercept = mean(age_matched, na.rm = T)), color = \"red\")\n\n\n\n\n\n\n\n# calculate mean wages for treatment group \ntrained = training_example %&gt;% \n  filter(!is.na(age_treat)) %&gt;% \n  mutate(earnings_treat = as.numeric(earnings_treat)) %&gt;% \n  pull(earnings_treat) %&gt;% \n  mean()\n\n# calculate mean wages for matched control group \nnontrained = training_example %&gt;% \n  filter(!is.na(age_matched)) %&gt;% \n  mutate(earnings_matched = as.numeric(earnings_matched)) %&gt;% \n  pull(earnings_matched) %&gt;% \n  mean()\n\natt_train = trained - nontrained # Notice - This is the ATT \natt_train\n\n[1] 1695",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#propensity-score-matching",
    "href": "matching.html#propensity-score-matching",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\nReplication data for Samuels and Shugart’s book “Presidents, Parties and Prime Ministers: How the Separation of Powers Affects Party Organization and Behavior” Cambridge University press, 2010 (Data extended by students from U-Mich).\nTreatment here is the type of system i.e. Presidential or Parlimentary.\n\n# load data\ndata &lt;- read_csv(\"data/matching_data.csv\")\n\n# summarize the data\nsummary(data$purepres) # treatment\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.3068  1.0000  1.0000 \n\nsummary(data$pureparl) # control\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.4205  1.0000  1.0000 \n\n\n\nExploratory Data Analysis\n\n# create a table to see the count of each type of system in our data set\ndata &lt;- data %&gt;% \n  mutate(\n    type = case_when( purepres==1 ~ \"PRES\",\n                      pureparl==1 ~ \"PARL\",\n                      presparlpres==1 | premiprespres==1 |presparlpm == 1 ~ \"MIXED\",\n                      TRUE ~ NA\n                      )\n  )\n\n# Checking Distribution\ntable(data$type)\n\n\nMIXED  PARL  PRES \n    9    37    27 \n\n\n\n\n\n\n\n\nLet’s check the distribution stratified by continents\n```{r}\n\n(data %&gt;% select(type, continent, damerica) %&gt;% \n  group_by(continent, type) %&gt;% \n  summarise(n = n())\n)\n```\n\n\n\n\n\n\n\n\n\nWhy might this distribution create a problem? (Hint: Think about balance/overlap)\n\nWrangling DatasetSummary of Data\n\n\n\n# create dataset of covariates of interest\n\n\nX &lt;- data %&gt;% \n  select(indviol, year_indep, dindep_decol, dindep_seces,\n               chrstprotpct, chrstcatpct, judgenpct,\n               islmgenpct, budgenpct,\n               a_ethnic, a_ling, a_relig,\n               gini,\n               land_kilometers,\n               life_expectency_1800, gdp_per_capita_1800,\n               density,\n               damerica, dsouthamerica, deurope, dafrica,\n               dasia, doceania,\n               colony_esp, colony_gbr, colony_fra, colony_prt, colony_oeu,\n               rugged\n  )\n\n\n# rename the variables \ncolnames(X) = c(\"Violent Independence\", \"Year of Independence\", \"Independency by Decolonization\",\n                \"Independence by Secession\", \"Christian Protestant Pct\", \"Christian Catholic Pct\", \n                \"Jewish Pct\", \"Islam Pct\", \"Budhist Pct\",\n                \"Ethic Fractionalization\", \"Linguistic Fractionalization\", \"Religious Fractionalization\",\n                \"Gini\",\n                \"Land (km)\",\n                \"Life expectancy 1800\", \"GDP per cap 1800\",\n                \"Population density\",\n                \"Country in America\", \"Country in South America\", \"Country in Europe\", \"Country in Africa\", \"Country in Asia\", \"Country in Oceania\",\n                \"Spanish colony\", \"British colony\", \"France colony\", \"Portugal colony\", \"OEU colony\",\n                \"Ruggedness\")\n\n```{r}\n#we summarize X\nsummary(X)\n```\n\n\n\n#we summarize X\nsummary(X)\n\n Violent Independence Year of Independence Independency by Decolonization\n Min.   :0.0000       Min.   : 943         Min.   :0.0000                \n 1st Qu.:0.0000       1st Qu.:1827         1st Qu.:0.0000                \n Median :1.0000       Median :1918         Median :1.0000                \n Mean   :0.5568       Mean   :1843         Mean   :0.5114                \n 3rd Qu.:1.0000       3rd Qu.:1960         3rd Qu.:1.0000                \n Max.   :1.0000       Max.   :1993         Max.   :1.0000                \n                                                                         \n Independence by Secession Christian Protestant Pct Christian Catholic Pct\n Min.   :0.0000            Min.   :0.0000           Min.   :0.00000       \n 1st Qu.:0.0000            1st Qu.:0.0015           1st Qu.:0.00905       \n Median :0.0000            Median :0.0181           Median :0.17000       \n Mean   :0.2727            Mean   :0.1179           Mean   :0.38277       \n 3rd Qu.:1.0000            3rd Qu.:0.0735           3rd Qu.:0.88380       \n Max.   :1.0000            Max.   :0.9900           Max.   :0.98040       \n                           NA's   :1                NA's   :1             \n   Jewish Pct        Islam Pct       Budhist Pct      Ethic Fractionalization\n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.001998       \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.181835       \n Median :0.00020   Median :0.0022   Median :0.00000   Median :0.406181       \n Mean   :0.01297   Mean   :0.1309   Mean   :0.03390   Mean   :0.395852       \n 3rd Qu.:0.00205   3rd Qu.:0.1146   3rd Qu.:0.00035   3rd Qu.:0.591322       \n Max.   :0.87530   Max.   :0.9947   Max.   :0.88540   Max.   :0.879100       \n NA's   :1         NA's   :1        NA's   :1                                \n Linguistic Fractionalization Religious Fractionalization      Gini      \n Min.   :0.002113             Min.   :0.002755            Min.   :19.40  \n 1st Qu.:0.087104             1st Qu.:0.220886            1st Qu.:36.50  \n Median :0.303091             Median :0.408493            Median :45.10  \n Mean   :0.338181             Mean   :0.423653            Mean   :43.65  \n 3rd Qu.:0.546679             3rd Qu.:0.615734            3rd Qu.:51.70  \n Max.   :0.873408             Max.   :0.860260            Max.   :73.90  \n NA's   :1                                                NA's   :7      \n   Land (km)        Life expectancy 1800 GDP per cap 1800 Population density \n Min.   :    2030   Min.   :25.10        Min.   : 368     Min.   :  0.02199  \n 1st Qu.:   60625   1st Qu.:30.16        1st Qu.: 740     1st Qu.:  1.13435  \n Median :  228780   Median :32.90        Median : 976     Median :  8.99563  \n Mean   :  964830   Mean   :32.67        Mean   :1099     Mean   : 15.92361  \n 3rd Qu.:  656352   3rd Qu.:35.73        3rd Qu.:1442     3rd Qu.: 19.66593  \n Max.   :16376870   Max.   :40.00        Max.   :2892     Max.   :121.29887  \n                                         NA's   :1                           \n Country in America Country in South America Country in Europe\n Min.   :0.00       Min.   :0.0000           Min.   :0.0000   \n 1st Qu.:0.00       1st Qu.:0.0000           1st Qu.:0.0000   \n Median :0.00       Median :0.0000           Median :0.0000   \n Mean   :0.25       Mean   :0.1136           Mean   :0.3409   \n 3rd Qu.:0.25       3rd Qu.:0.0000           3rd Qu.:1.0000   \n Max.   :1.00       Max.   :1.0000           Max.   :1.0000   \n                                                              \n Country in Africa Country in Asia  Country in Oceania Spanish colony  \n Min.   :0.0000    Min.   :0.0000   Min.   :0.00000    Min.   :0.0000  \n 1st Qu.:0.0000    1st Qu.:0.0000   1st Qu.:0.00000    1st Qu.:0.0000  \n Median :0.0000    Median :0.0000   Median :0.00000    Median :0.0000  \n Mean   :0.1591    Mean   :0.2045   Mean   :0.04545    Mean   :0.2152  \n 3rd Qu.:0.0000    3rd Qu.:0.0000   3rd Qu.:0.00000    3rd Qu.:0.0000  \n Max.   :1.0000    Max.   :1.0000   Max.   :1.00000    Max.   :1.0000  \n                                                       NA's   :9       \n British colony   France colony     Portugal colony     OEU colony     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.2785   Mean   :0.05063   Mean   :0.02532   Mean   :0.02532  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n NA's   :9        NA's   :9         NA's   :9         NA's   :9        \n   Ruggedness    \n Min.   :0.0370  \n 1st Qu.:0.4255  \n Median :1.0130  \n Mean   :1.2591  \n 3rd Qu.:1.7745  \n Max.   :5.0430  \n NA's   :9       \n\n\n\n\n\n\n\n\nChecking Balance of Covariates\n\n# balance function to calculate normalized distance\n# Look through this function in detail what each of the arguments are doing\n\n\nbalance &lt;- function(x, Tr, xname, alpha=0.05) {\n  Tr = Tr[!is.na(x)]\n  x  =  x[!is.na(x)]     \n  mut = mean(x[Tr==1])\n  muc = mean(x[Tr==0])\n  s2t  = var(x[Tr==1])\n  s2c  = var(x[Tr==0])    \n  delta = (mut - muc)/sqrt((s2t+s2c)/2) # this is the normalized difference\n  \n  ql = quantile(x[Tr==0], probs = alpha/2) # lower tail for control\n  qh = quantile(x[Tr==0], probs = 1-alpha/2) # upper tail for control\n  pi = sum(x[Tr==1] &lt; ql)/(length(Tr==1)) + sum(x[Tr==1] &gt; qh)/(length(Tr==1)) # fraction\n  \n  cat(\"------------------------------------------\\n\")    \n  cat(\"Variable \", xname, \"\\n\")\n  cat(\"Mean Treatment (Pres) \", mut, \"\\n\")\n  cat(\"Mean Control (Parl)\", muc, \"\\n\")\n  cat(\"Normalized difference\", delta, \"\\n\")        \n  cat(\"------------------------------------------\\n\")\n  cat(\"Mass of treated distribution in the control tails\", round(pi*100, 2), \"% \\n\")    \n  cat(\"------------------------------------------\\n\")\n  return(list(mut=mut,muc=muc, s2t=s2t, s2c=s2c, delta=delta, pi=pi))\n}\n\n\nCodeResult\n\n\n```{r}\n# (1) Let's look at normalized differences and Mass of Treated Distribution in Control Tailes\n# Also Recall Rubin's rule: if above |0.25|, then OLS likely too sensitive to misspecification\n\n# create vector for treatment index\nTr = data$purepres\n\n# check for balance on treatment\nresult &lt;- imap(X, ~ balance(x = .x, Tr = Tr, xname = .y))\n```\n\n\n\n\n------------------------------------------\nVariable  Violent Independence \nMean Treatment (Pres)  0.6666667 \nMean Control (Parl) 0.5081967 \nNormalized difference 0.3218476 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Year of Independence \nMean Treatment (Pres)  1872.444 \nMean Control (Parl) 1829.754 \nNormalized difference 0.2299536 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Independency by Decolonization \nMean Treatment (Pres)  0.5925926 \nMean Control (Parl) 0.4754098 \nNormalized difference 0.2333725 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Independence by Secession \nMean Treatment (Pres)  0.3703704 \nMean Control (Parl) 0.2295082 \nNormalized difference 0.3066766 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Christian Protestant Pct \nMean Treatment (Pres)  0.04634815 \nMean Control (Parl) 0.1500733 \nNormalized difference -0.5113226 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Christian Catholic Pct \nMean Treatment (Pres)  0.6333444 \nMean Control (Parl) 0.2700083 \nNormalized difference 0.974737 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Jewish Pct \nMean Treatment (Pres)  0.003403704 \nMean Control (Parl) 0.01728167 \nNormalized difference -0.173385 \n------------------------------------------\nMass of treated distribution in the control tails 2.3 % \n------------------------------------------\n------------------------------------------\nVariable  Islam Pct \nMean Treatment (Pres)  0.08145926 \nMean Control (Parl) 0.15313 \nNormalized difference -0.2808659 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Budhist Pct \nMean Treatment (Pres)  0.006037037 \nMean Control (Parl) 0.046435 \nNormalized difference -0.3403212 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Ethic Fractionalization \nMean Treatment (Pres)  0.4554665 \nMean Control (Parl) 0.3694653 \nNormalized difference 0.3641619 \n------------------------------------------\nMass of treated distribution in the control tails 3.41 % \n------------------------------------------\n------------------------------------------\nVariable  Linguistic Fractionalization \nMean Treatment (Pres)  0.3310051 \nMean Control (Parl) 0.3412398 \nNormalized difference -0.03638428 \n------------------------------------------\nMass of treated distribution in the control tails 4.6 % \n------------------------------------------\n------------------------------------------\nVariable  Religious Fractionalization \nMean Treatment (Pres)  0.3815061 \nMean Control (Parl) 0.4423086 \nNormalized difference -0.2694968 \n------------------------------------------\nMass of treated distribution in the control tails 2.27 % \n------------------------------------------\n------------------------------------------\nVariable  Gini \nMean Treatment (Pres)  49.35769 \nMean Control (Parl) 40.95091 \nNormalized difference 0.759161 \n------------------------------------------\nMass of treated distribution in the control tails 1.23 % \n------------------------------------------\n------------------------------------------\nVariable  Land (km) \nMean Treatment (Pres)  1139389 \nMean Control (Parl) 887566 \nNormalized difference 0.1041268 \n------------------------------------------\nMass of treated distribution in the control tails 1.14 % \n------------------------------------------\n------------------------------------------\nVariable  Life expectancy 1800 \nMean Treatment (Pres)  31.2861 \nMean Control (Parl) 33.27707 \nNormalized difference -0.5445587 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  GDP per cap 1800 \nMean Treatment (Pres)  918.2593 \nMean Control (Parl) 1180.317 \nNormalized difference -0.5806425 \n------------------------------------------\nMass of treated distribution in the control tails 1.15 % \n------------------------------------------\n------------------------------------------\nVariable  Population density \nMean Treatment (Pres)  7.215313 \nMean Control (Parl) 19.77811 \nNormalized difference -0.6009871 \n------------------------------------------\nMass of treated distribution in the control tails 2.27 % \n------------------------------------------\n------------------------------------------\nVariable  Country in America \nMean Treatment (Pres)  0.6666667 \nMean Control (Parl) 0.06557377 \nNormalized difference 1.570273 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in South America \nMean Treatment (Pres)  0.3333333 \nMean Control (Parl) 0.01639344 \nNormalized difference 0.9015721 \n------------------------------------------\nMass of treated distribution in the control tails 10.23 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Europe \nMean Treatment (Pres)  0 \nMean Control (Parl) 0.4918033 \nNormalized difference -1.379766 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Africa \nMean Treatment (Pres)  0.1851852 \nMean Control (Parl) 0.147541 \nNormalized difference 0.09979829 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Asia \nMean Treatment (Pres)  0.1481481 \nMean Control (Parl) 0.2295082 \nNormalized difference -0.2063768 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Oceania \nMean Treatment (Pres)  0 \nMean Control (Parl) 0.06557377 \nNormalized difference -0.3715509 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Spanish colony \nMean Treatment (Pres)  0.6666667 \nMean Control (Parl) 0.01818182 \nNormalized difference 1.833951 \n------------------------------------------\nMass of treated distribution in the control tails 20.25 % \n------------------------------------------\n------------------------------------------\nVariable  British colony \nMean Treatment (Pres)  0.2083333 \nMean Control (Parl) 0.3090909 \nNormalized difference -0.2282852 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  France colony \nMean Treatment (Pres)  0.04166667 \nMean Control (Parl) 0.05454545 \nNormalized difference -0.05934487 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Portugal colony \nMean Treatment (Pres)  0.04166667 \nMean Control (Parl) 0.01818182 \nNormalized difference 0.1357614 \n------------------------------------------\nMass of treated distribution in the control tails 1.27 % \n------------------------------------------\n------------------------------------------\nVariable  OEU colony \nMean Treatment (Pres)  0.04166667 \nMean Control (Parl) 0.01818182 \nNormalized difference 0.1357614 \n------------------------------------------\nMass of treated distribution in the control tails 1.27 % \n------------------------------------------\n------------------------------------------\nVariable  Ruggedness \nMean Treatment (Pres)  1.204625 \nMean Control (Parl) 1.282836 \nNormalized difference -0.08265628 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n\n\n\n\n\n\nThe “Mass of treated distribution in the control tails” is a percentage that indicates how much of the treated group’s data lies in the extreme tails of the control group’s distribution. A high value suggests imbalance between the two groups, as it indicates that the treated group has many values in regions of the distribution that are less typical for the control group.\nWhy Is This Important?\nIn balance checks meant for attribution of effect to treatment, as done under assumpotion of uncoundedness or selection on observables, we want the treated and control groups to be similar in distribution for all covariates (except for the treatment assignment). When there’s a large portion of the treated group in the tails of the control distribution, it suggests that the two groups are not well balanced for that variable, meaning there may be substantial differences in their distribution. This would raise concerns about whether we can attribute the differences in outcomes to the treatment alone, rather than to the covariates being imbalanced.\n\n\nPropensity Score Matching on Treatment and Control\n\nCreating Propensity ScoresPlot of Propensity ScoresIdeal Overlap Simulation\n\n\n\n# calculate the propensity scores with full data set \npscore&lt;-glm(Tr~as.matrix(X),family=binomial(link=logit)) # why the warnings?\n\n# create smaller X vector - taking out variables like continent and colonial history \n\n\nX2 &lt;- data %&gt;%\n  select(\n    Violent_Independence = indviol,\n    Year_of_Independence = year_indep,\n    Independency_by_Decolonization = dindep_decol,\n    Independence_by_Secession = dindep_seces,\n    Christian_Protestant_Pct = chrstprotpct,\n    Christian_Catholic_Pct = chrstcatpct,\n    Jewish_Pct = judgenpct,\n    Islam_Pct = islmgenpct,\n    Budhist_Pct = budgenpct,\n    Ethnic_Fractionalization = a_ethnic,\n    Linguistic_Fractionalization = a_ling,\n    Religious_Fractionalization = a_relig,\n    gini,\n    Land_kilometers = land_kilometers,\n    Life_expectancy_1800 = life_expectency_1800,\n    GDP_per_capita_1800 = gdp_per_capita_1800,\n    Population_density = density,\n    Country_in_Africa = dafrica,\n    Country_in_Asia = dasia,\n    Ruggedness = rugged\n  )\n\n\n# create index for complete observations and select those \nindx = complete.cases(X2)\n\n# filter our treatment vector on complete cases\nTr2 = Tr[indx]\n\n# calculate propensity score on this subset \npscore&lt;-glm(Tr2 ~ ., data= X2[indx,],family=binomial(link=logit))\n\n# pull out propensity score for graphing\nphat&lt;-pscore$fitted.values\n\n\n\n\n# histogram of propensity scores \nggplot(data.frame(phat), aes(x = phat)) +\n  geom_histogram(binwidth = 0.05, fill = \"darkgreen\", color = \"black\") +\n  labs(title = \"Histogram of PScores\", \n       x = \"Predicted Propensity Scores (phat)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# plot pscore by treatment\nggplot(data)+\n  geom_density(data=data.frame(x=phat[Tr2==1]), aes(x = x, fill=\"Tr\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\")+\n  geom_density(data=data.frame(x=phat[Tr2==0]), aes(x = x, fill=\"Co\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\")+\n  scale_fill_manual(values=c(\"Tr\"=\"blue\", \"Co\"=\"red\"), labels = c(\"Presidential (T = 1) \", \"Parliamentary (T = 0)\"))+\n  theme_bw()+\n  labs(\n    x = \"Probability of Being in Treatment\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nCompare to what the histogram would look under simulated “random assignment” if both groups were identical on average\n\n# Generate T using runif\nT0 &lt;- as.numeric(runif(n = length(Tr2), min = 0, max = 1) &gt;= 0.5) # Create T variable\n\n# Create a data frame combining phat and T\ndata2 &lt;- data.frame(phat = phat, group = factor(T0, labels = c(\"Parliamentary\", \"Presidential\")))\n\n# Plot the density using ggplot2\nggplot(data2, aes(x = phat, fill = group)) +\n  geom_density(alpha = 0.5, color = \"black\") +\n  xlim(-0.3, 1.3) +\n  ylim(0, 10) +\n  labs(title = \"Density of Estimated Propensity Score\", \n       x = \"Predicted Propensity Score (phat)\", \n       y = \"Density\") +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  theme(legend.position = \"topright\")\n\n\n\n\n\n\n\n\nThe simulation shows ideal random assignment.\nComparing this to actual plots from propensity scores reveals the problem.\n\n\n\n\n\n\nTaking a different (smaller) set of covariates\n\n## Now include fewer variables to avoid perfect separation\nX2 &lt;- data %&gt;% \n  select(indviol, year_indep, dindep_decol, dindep_seces,\n           a_ethnic, a_ling, a_relig,\n           life_expectency_1800, gdp_per_capita_1800,\n           damerica, dsouthamerica, deurope, dafrica,\n         dasia, doceania)\n\ncolnames(X2) = c(\"Violent Independenc\", \"Year of Independence\", \"Independency by Decolonization\", \"Independence by Secession\",\n                 \"Ethic Fractionalization\", \"Linguistic Fractionalization\", \"Religious Fractionalization\",\n                 \"Life expectancy 1800\", \"GDP per cap 1800\",\n                 \"Country in America\", \"Country in South America\", \"Country in Europe\", \"Country in Africa\", \"Country in Asia\", \"Country in Oceania\")\n\n# index on complete cases\nindx = complete.cases(X2)\n# index treatment vector on complete cases\nTr2 = Tr[indx]\n# calculate propensity score\npscore&lt;-glm(Tr2~ . , data = X2[indx,],family=binomial(link=logit)) \nphat&lt;-pscore$fitted.values\n\nggplot(data.frame(phat), aes(x = phat)) +\n  geom_histogram(binwidth = 0.05, fill = \"pink\", color = \"black\") +\n  labs(title = \"Histogram of PScores\", \n       x = \"Predicted Propensity Scores (phat)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nComparing again\n\n# Create a data frame combining phat and Tr2\ndata3 &lt;- data.frame(phat = phat, Tr2 = factor(Tr2, labels = c(\"Parliamentary (Co)\", \"Presidential (Tr)\")))\n\n# Set up the plot layout\n# First plot: density plots\np1 &lt;- ggplot(data3, aes(x = phat, fill = Tr2)) +\n  geom_density(alpha = 0.5, position = \"identity\") +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"Density of Predicted Propensity Scores\", \n       x = \"Predicted Propensity Score (phat)\", \n       y = \"Density\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n# Second plot: boxplot\np2 &lt;- ggplot(data3, aes(x = Tr2, y = phat)) +\n  geom_boxplot(outlier.shape = NA) + # Suppressing outlier shapes for clarity\n  geom_jitter(width = 0.2, aes(color = Tr2), alpha = 0.6) + # Adding jitter for visibility\n  labs(title = \"Boxplot of Predicted Propensity Scores by Group\", \n       x = \"Group\", \n       y = \"Predicted Propensity Score (phat)\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal()+\n  theme(\n    legend.position = \"bottom\"\n  )\n\n# Combine the two plots using gridExtra\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n# These are all different ways to see balance on propensity scores\n\n\n\nPropesnity Score Density\n\n# Pscore Density plots\ndat &lt;- data.frame(phat = c(phat[Tr2==1], phat[Tr2==0]), Tr=Tr2, groups = as.character(c(rep(\"Tr\", length(Tr2==1)), rep(\"Co\", length(Tr2==0)))))\n\nggplot(dat)+\n  geom_density(data=data.frame(x=phat[Tr2==1]), aes(x = x, fill=\"Tr\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\") +\n  geom_density(data=data.frame(x=phat[Tr2==0]), aes(x = x, fill=\"Co\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\") +\n  scale_fill_manual(values=c(\"Tr\"=\"red\", \"Co\"=\"blue\"), labels = c(\"Presidential (T = 1) \", \"Parliamentary (T = 0)\"))+\n  theme_bw()+\n  labs(\n    x = \"Probability of Being in Treatment\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# balance in propensity score\nresult &lt;- balance(x=phat, Tr= Tr2, xname=\"phat\")\n\n------------------------------------------\nVariable  phat \nMean Treatment (Pres)  0.6811678 \nMean Control (Parl) 0.1381606 \nNormalized difference 2.205072 \n------------------------------------------\nMass of treated distribution in the control tails 19.77 % \n------------------------------------------\n\n\nAs you can see, it is still quite unbalanced.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#how-to-use-matching",
    "href": "matching.html#how-to-use-matching",
    "title": "Matching",
    "section": "How to Use Matching",
    "text": "How to Use Matching\n\nMatch on Covariates\n\n# set up \n# install.packages(\"Matching\")\nlibrary(Matching)\n\n# match on covariates\nmout1 &lt;- Match(Tr=Tr[indx], X = X2[indx,], estimand=\"ATT\", ties=FALSE)\nsummary(mout1)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  26 \nMatched number of observations...............  26 \nMatched number of observations  (unweighted).  26 \n\n\n\nRunning MatchBalance()Output\n\n\n```{r}\nMatchBalance(Tr[indx] ~ ., data = X2[indx,], match.out=mout1)\n\n# What does the output mean?\n# Let's look at the documentation of ?MatchBalance\n\n# Let's look at the results again\n# See what rows of value for covariates change\n```\n\n\n\n\n\n***** (V1) `Violent Independenc` *****\n                       Before Matching       After Matching\nmean treatment........    0.65385           0.65385 \nmean control..........        0.5           0.69231 \nstd mean diff.........      31.71           -7.9275 \n\nmean raw eQQ diff.....    0.15385          0.038462 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.076923          0.019231 \nmed  eCDF diff........   0.076923          0.019231 \nmax  eCDF diff........    0.15385          0.038462 \n\nvar ratio (Tr/Co).....    0.92585            1.0625 \nT-test p-value........    0.18817           0.65734 \n\n\n***** (V2) `Year of Independence` *****\n                       Before Matching       After Matching\nmean treatment........     1873.7            1873.7 \nmean control..........     1833.8            1900.4 \nstd mean diff.........     61.639           -41.192 \n\nmean raw eQQ diff.....     130.81            27.077 \nmed  raw eQQ diff.....       63.5                 6 \nmax  raw eQQ diff.....        840                82 \n\nmean eCDF diff........    0.16815           0.10743 \nmed  eCDF diff........    0.15769          0.076923 \nmax  eCDF diff........    0.36538           0.30769 \n\nvar ratio (Tr/Co).....   0.064641            1.1792 \nT-test p-value........    0.26113          0.030679 \nKS Bootstrap p-value..      0.006             0.114 \nKS Naive p-value......  0.0091303           0.13814 \nKS Statistic..........    0.36538           0.30769 \n\n\n***** (V3) `Independency by Decolonization` *****\n                       Before Matching       After Matching\nmean treatment........    0.61538           0.61538 \nmean control..........    0.48333           0.73077 \nstd mean diff.........     26.616           -23.257 \n\nmean raw eQQ diff.....    0.15385           0.11538 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.066026          0.057692 \nmed  eCDF diff........   0.066026          0.057692 \nmax  eCDF diff........    0.13205           0.11538 \n\nvar ratio (Tr/Co).....    0.96928             1.203 \nT-test p-value........    0.26482          0.077437 \n\n\n***** (V4) `Independence by Secession` *****\n                       Before Matching       After Matching\nmean treatment........    0.34615           0.34615 \nmean control..........    0.23333           0.26923 \nstd mean diff.........     23.254            15.855 \n\nmean raw eQQ diff.....    0.11538          0.076923 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.05641          0.038462 \nmed  eCDF diff........    0.05641          0.038462 \nmax  eCDF diff........    0.11282          0.076923 \n\nvar ratio (Tr/Co).....     1.2939            1.1504 \nT-test p-value........    0.31057           0.31759 \n\n\n***** (V5) `Ethic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.46538           0.46538 \nmean control..........    0.37542           0.48415 \nstd mean diff.........     37.869           -7.9049 \n\nmean raw eQQ diff.....    0.10566          0.066566 \nmed  raw eQQ diff.....   0.099907          0.061492 \nmax  raw eQQ diff.....       0.21           0.14438 \n\nmean eCDF diff........    0.11999          0.077963 \nmed  eCDF diff........    0.10769          0.076923 \nmax  eCDF diff........    0.28718           0.23077 \n\nvar ratio (Tr/Co).....     1.0573            1.0826 \nT-test p-value........    0.11067           0.69009 \nKS Bootstrap p-value..       0.07             0.404 \nKS Naive p-value......   0.079403           0.46218 \nKS Statistic..........    0.28718           0.23077 \n\n\n***** (V6) `Linguistic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.33101           0.33101 \nmean control..........    0.34663           0.40658 \nstd mean diff.........    -5.0586           -24.468 \n\nmean raw eQQ diff.....   0.070562          0.099725 \nmed  raw eQQ diff.....   0.070461          0.059251 \nmax  raw eQQ diff.....    0.18848           0.28053 \n\nmean eCDF diff........   0.081112           0.11435 \nmed  eCDF diff........   0.080769          0.076923 \nmax  eCDF diff........    0.20128           0.34615 \n\nvar ratio (Tr/Co).....     1.5366            1.3935 \nT-test p-value........    0.82096          0.030313 \nKS Bootstrap p-value..      0.334             0.062 \nKS Naive p-value......    0.39378          0.065032 \nKS Statistic..........    0.20128           0.34615 \n\n\n***** (V7) `Religious Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.38249           0.38249 \nmean control..........    0.44067           0.34795 \nstd mean diff.........    -25.495            15.139 \n\nmean raw eQQ diff.....   0.080581          0.055022 \nmed  raw eQQ diff.....   0.072014           0.04163 \nmax  raw eQQ diff.....    0.20034           0.16627 \n\nmean eCDF diff........   0.092695           0.11331 \nmed  eCDF diff........   0.064103          0.076923 \nmax  eCDF diff........    0.29744           0.38462 \n\nvar ratio (Tr/Co).....    0.99339            0.8127 \nT-test p-value........    0.28348           0.39388 \nKS Bootstrap p-value..      0.054             0.022 \nKS Naive p-value......   0.063197          0.035888 \nKS Statistic..........    0.29744           0.38462 \n\n\n***** (V8) `Life expectancy 1800` *****\n                       Before Matching       After Matching\nmean treatment........     31.386            31.386 \nmean control..........     33.225            32.671 \nstd mean diff.........    -53.291           -37.245 \n\nmean raw eQQ diff.....     1.7649            1.8473 \nmed  raw eQQ diff.....       1.83              2.05 \nmax  raw eQQ diff.....        3.4              3.51 \n\nmean eCDF diff........    0.14984           0.15012 \nmed  eCDF diff........    0.12821          0.076923 \nmax  eCDF diff........    0.38462           0.46154 \n\nvar ratio (Tr/Co).....    0.78917           0.87149 \nT-test p-value........   0.033452          0.051485 \nKS Bootstrap p-value..      0.006             0.006 \nKS Naive p-value......  0.0053601         0.0056297 \nKS Statistic..........    0.38462           0.46154 \n\n\n***** (V9) `GDP per cap 1800` *****\n                       Before Matching       After Matching\nmean treatment........     914.12            914.12 \nmean control..........     1180.3            1009.1 \nstd mean diff.........    -81.331           -29.025 \n\nmean raw eQQ diff.....     286.88            130.38 \nmed  raw eQQ diff.....        193               113 \nmax  raw eQQ diff.....        796               452 \n\nmean eCDF diff........    0.14666            0.1237 \nmed  eCDF diff........   0.094872           0.11538 \nmax  eCDF diff........    0.40128           0.34615 \n\nvar ratio (Tr/Co).....     0.3525            1.5045 \nT-test p-value........  0.0069009           0.14223 \nKS Bootstrap p-value..      0.002              0.07 \nKS Naive p-value......  0.0038375          0.068177 \nKS Statistic..........    0.40128           0.34615 \n\n\n***** (V10) `Country in America` *****\n                       Before Matching       After Matching\nmean treatment........    0.65385           0.65385 \nmean control..........   0.066667           0.42308 \nstd mean diff.........     121.03            47.565 \n\nmean raw eQQ diff.....    0.57692           0.23077 \nmed  raw eQQ diff.....          1                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.29359           0.11538 \nmed  eCDF diff........    0.29359           0.11538 \nmax  eCDF diff........    0.58718           0.23077 \n\nvar ratio (Tr/Co).....     3.7199           0.92727 \nT-test p-value........ 1.9372e-06          0.009874 \n\n\n***** (V11) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.34615           0.34615 \nmean control..........   0.016667           0.34615 \nstd mean diff.........     67.912                 0 \n\nmean raw eQQ diff.....    0.30769                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.16474                 0 \nmed  eCDF diff........    0.16474                 0 \nmax  eCDF diff........    0.32949                 0 \n\nvar ratio (Tr/Co).....     14.123                 1 \nT-test p-value........  0.0020845                 1 \n\n\n***** (V12) `Country in Europe` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........        0.5          0.076923 \nstd mean diff.........       -Inf              -Inf \n\nmean raw eQQ diff.....        0.5          0.076923 \nmed  raw eQQ diff.....        0.5                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........       0.25          0.038462 \nmed  eCDF diff........       0.25          0.038462 \nmax  eCDF diff........        0.5          0.076923 \n\nvar ratio (Tr/Co).....          0                 0 \nT-test p-value........ 1.8965e-10           0.15351 \n\n\n***** (V13) `Country in Africa` *****\n                       Before Matching       After Matching\nmean treatment........    0.19231           0.19231 \nmean control..........       0.15           0.19231 \nstd mean diff.........     10.526                 0 \n\nmean raw eQQ diff.....   0.038462                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.021154                 0 \nmed  eCDF diff........   0.021154                 0 \nmax  eCDF diff........   0.042308                 0 \n\nvar ratio (Tr/Co).....     1.2459                 1 \nT-test p-value........    0.64616                 1 \n\n\n***** (V14) `Country in Asia` *****\n                       Before Matching       After Matching\nmean treatment........    0.15385           0.15385 \nmean control..........    0.21667           0.30769 \nstd mean diff.........    -17.073           -41.812 \n\nmean raw eQQ diff.....   0.076923           0.15385 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.03141          0.076923 \nmed  eCDF diff........    0.03141          0.076923 \nmax  eCDF diff........   0.062821           0.15385 \n\nvar ratio (Tr/Co).....    0.78439           0.61111 \nT-test p-value........    0.48777           0.03936 \n\n\n***** (V15) `Country in Oceania` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........   0.066667                 0 \nstd mean diff.........       -Inf                 0 \n\nmean raw eQQ diff.....   0.076923                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.033333                 0 \nmed  eCDF diff........   0.033333                 0 \nmax  eCDF diff........   0.066667                 0 \n\nvar ratio (Tr/Co).....          0               NaN \nT-test p-value........   0.044528                 1 \n\n\nBefore Matching Minimum p.value: 1.8965e-10 \nVariable Name(s): `Country in Europe`  Number(s): 12 \n\nAfter Matching Minimum p.value: 0.006 \nVariable Name(s): `Life expectancy 1800`  Number(s): 8 \n\n\n\n\n\n\n\n\nMatch on Propensity Scores\n\n# match on propensity score\nmout2 &lt;- Match(Tr=Tr[indx], X=phat, estimand=\"ATT\", ties=FALSE)\nsummary(mout2)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  26 \nMatched number of observations...............  26 \nMatched number of observations  (unweighted).  26 \n\nMatchBalance(Tr[indx] ~ phat, match.out=mout2)\n\n\n***** (V1) phat *****\n                       Before Matching       After Matching\nmean treatment........    0.68117           0.68117 \nmean control..........    0.13816           0.67938 \nstd mean diff.........     196.78           0.64781 \n\nmean raw eQQ diff.....     0.5399          0.045756 \nmed  raw eQQ diff.....    0.54148          0.036657 \nmax  raw eQQ diff.....    0.84298            0.1225 \n\nmean eCDF diff........    0.41987          0.090498 \nmed  eCDF diff........    0.48333          0.038462 \nmax  eCDF diff........    0.71154           0.34615 \n\nvar ratio (Tr/Co).....     1.6869           0.97716 \nT-test p-value........ 6.1436e-11           0.87615 \nKS Bootstrap p-value.. &lt; 2.22e-16             0.064 \nKS Naive p-value...... 2.0099e-09           0.07132 \nKS Statistic..........    0.71154           0.34615 \n\n\n\n\nExplore the matches\n\n## examine matches \nmdataTr = data[indx,][mout2$index.treated,]\nmdataCo = data[indx,][mout2$index.control,]\n\n```{r}\nView(data.frame(mdataTr$country, mdataCo$country))\n\n```\nNote how Peru appears as the chosen control country for every Latin American country\n\n\n\n\n\n\n\n\nThat is because Peru is coded as not being pure presidential.\n\ndata$purepres[data$country==\"PERU\"]\n\n[1] 0\n\ndata$presparlpm[data$country==\"PERU\"]\n\n[1] 1\n\n\nIncluding Mixed Systems\nHowever, Peru is a variation of presidential system, so it may not be adequate to classify as control.\nLet’s code mixed systems as treated.\n\nTr = (data$purepres == 1 | data$presparlpm==1)\nmout4 &lt;- Match(Tr=Tr[indx], X=X2[indx,], estimand=\"ATT\")\nsummary(mout4)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  30 \nMatched number of observations...............  30 \nMatched number of observations  (unweighted).  30 \n\nmdataTr = data[indx,][mout4$index.treated,]\nmdataCo = data[indx,][mout4$index.control,]\n\n```{r}\nView(data.frame(mdataTr$country, mdataCo$country))\n```\n\n\n\n\n\n\n\n\n\nMatchBalance CodeMatchBalance Output\n\n\n```{r}\nMatchBalance(Tr[indx] ~ ., data =  X2[indx,], match.out=mout4, nboots=10)\n```\n\n\n\n\n\n***** (V1) `Violent Independenc` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333           0.63333 \nmean control..........        0.5               0.4 \nstd mean diff.........     27.204            47.606 \n\nmean raw eQQ diff.....    0.13333           0.23333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.066667           0.11667 \nmed  eCDF diff........   0.066667           0.11667 \nmax  eCDF diff........    0.13333           0.23333 \n\nvar ratio (Tr/Co).....    0.94376           0.96759 \nT-test p-value........    0.23865         0.0052095 \n\n\n***** (V2) `Year of Independence` *****\n                       Before Matching       After Matching\nmean treatment........     1882.7            1882.7 \nmean control..........     1826.1            1946.2 \nstd mean diff.........     81.576           -91.426 \n\nmean raw eQQ diff.....     134.53            63.467 \nmed  raw eQQ diff.....         53              92.5 \nmax  raw eQQ diff.....        840               122 \n\nmean eCDF diff........    0.13314           0.24902 \nmed  eCDF diff........    0.11667              0.25 \nmax  eCDF diff........    0.31667           0.56667 \n\nvar ratio (Tr/Co).....   0.070412             5.908 \nT-test p-value........    0.13242        3.9478e-05 \nKS Bootstrap p-value..        0.1        &lt; 2.22e-16 \nKS Naive p-value......   0.023689        5.8786e-05 \nKS Statistic..........    0.31667           0.56667 \n\n\n***** (V3) `Independency by Decolonization` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333           0.63333 \nmean control..........    0.46429           0.66667 \nstd mean diff.........      34.49           -6.8009 \n\nmean raw eQQ diff.....    0.16667          0.033333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.084524          0.016667 \nmed  eCDF diff........   0.084524          0.016667 \nmax  eCDF diff........    0.16905          0.033333 \n\nvar ratio (Tr/Co).....     0.9486             1.045 \nT-test p-value........    0.13617           0.31752 \n\n\n***** (V4) `Independence by Secession` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333           0.33333 \nmean control..........    0.23214           0.33333 \nstd mean diff.........     21.105                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.050595                 0 \nmed  eCDF diff........   0.050595                 0 \nmax  eCDF diff........    0.10119                 0 \n\nvar ratio (Tr/Co).....     1.2666                 1 \nT-test p-value........    0.33686                 1 \n\n\n***** (V5) `Ethic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.48523           0.48523 \nmean control..........    0.35836           0.42003 \nstd mean diff.........     55.492            28.518 \n\nmean raw eQQ diff.....     0.1415          0.076679 \nmed  raw eQQ diff.....    0.12276          0.072789 \nmax  raw eQQ diff.....     0.2612            0.1832 \n\nmean eCDF diff........    0.16375           0.10725 \nmed  eCDF diff........    0.15714          0.066667 \nmax  eCDF diff........    0.37857           0.33333 \n\nvar ratio (Tr/Co).....    0.99957            1.0037 \nT-test p-value........   0.017142          0.044288 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.1 \nKS Naive p-value......  0.0050673          0.058602 \nKS Statistic..........    0.37857           0.33333 \n\n\n***** (V6) `Linguistic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.36042           0.36042 \nmean control..........    0.33199            0.3762 \nstd mean diff.........      9.407           -5.2182 \n\nmean raw eQQ diff.....   0.052711            0.0509 \nmed  raw eQQ diff.....   0.023094          0.045404 \nmax  raw eQQ diff.....    0.18848           0.11195 \n\nmean eCDF diff........   0.055482          0.073913 \nmed  eCDF diff........   0.039881          0.066667 \nmax  eCDF diff........    0.20833           0.23333 \n\nvar ratio (Tr/Co).....     1.4833           0.94082 \nT-test p-value........    0.66065             0.568 \nKS Bootstrap p-value..        0.2               0.3 \nKS Naive p-value......    0.31127           0.37798 \nKS Statistic..........    0.20833           0.23333 \n\n\n***** (V7) `Religious Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.38572           0.38572 \nmean control..........     0.4431            0.4778 \nstd mean diff.........    -25.044           -40.192 \n\nmean raw eQQ diff.....   0.074969          0.098234 \nmed  raw eQQ diff.....   0.053677          0.065211 \nmax  raw eQQ diff.....    0.17146            0.2822 \n\nmean eCDF diff........   0.089106            0.1029 \nmed  eCDF diff........     0.0625               0.1 \nmax  eCDF diff........    0.27381               0.3 \n\nvar ratio (Tr/Co).....     1.0062           0.78179 \nT-test p-value........    0.27231          0.062384 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value......   0.085347           0.12971 \nKS Statistic..........    0.27381               0.3 \n\n\n***** (V8) `Life expectancy 1800` *****\n                       Before Matching       After Matching\nmean treatment........     31.531            31.531 \nmean control..........     33.279              32.3 \nstd mean diff.........    -48.136           -21.188 \n\nmean raw eQQ diff.....     1.6086            1.0179 \nmed  raw eQQ diff.....     1.6776              1.15 \nmax  raw eQQ diff.....        3.1                 2 \n\nmean eCDF diff........    0.13347          0.097436 \nmed  eCDF diff........    0.11071          0.066667 \nmax  eCDF diff........    0.33571           0.33333 \n\nvar ratio (Tr/Co).....    0.89741           0.98357 \nT-test p-value........   0.041052           0.15781 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value......   0.016592          0.055924 \nKS Statistic..........    0.33571           0.33333 \n\n\n***** (V9) `GDP per cap 1800` *****\n                       Before Matching       After Matching\nmean treatment........     884.67            884.67 \nmean control..........     1215.1            1141.5 \nstd mean diff.........    -103.91           -80.776 \n\nmean raw eQQ diff.....      327.8               287 \nmed  raw eQQ diff.....      269.5               272 \nmax  raw eQQ diff.....        796               622 \n\nmean eCDF diff........    0.17895           0.23188 \nmed  eCDF diff........    0.12738           0.23333 \nmax  eCDF diff........    0.45357           0.53333 \n\nvar ratio (Tr/Co).....    0.33049           0.73895 \nT-test p-value........ 0.00071184        0.00027375 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value...... 0.00035765        0.00027349 \nKS Statistic..........    0.45357           0.53333 \n\n\n***** (V10) `Country in America` *****\n                       Before Matching       After Matching\nmean treatment........        0.6               0.6 \nmean control..........   0.053571               0.3 \nstd mean diff.........     109.66            60.208 \n\nmean raw eQQ diff.....    0.53333               0.3 \nmed  raw eQQ diff.....          1                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.27321              0.15 \nmed  eCDF diff........    0.27321              0.15 \nmax  eCDF diff........    0.54643               0.3 \n\nvar ratio (Tr/Co).....     4.8094            1.1429 \nT-test p-value........ 1.8303e-06         0.0012161 \n\n\n***** (V11) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333           0.33333 \nmean control..........          0                 0 \nstd mean diff.........     69.522            69.522 \n\nmean raw eQQ diff.....    0.33333           0.33333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.16667           0.16667 \nmed  eCDF diff........    0.16667           0.16667 \nmax  eCDF diff........    0.33333           0.33333 \n\nvar ratio (Tr/Co).....        Inf               Inf \nT-test p-value........ 0.00067228         0.0005642 \n\n\n***** (V12) `Country in Europe` *****\n                       Before Matching       After Matching\nmean treatment........   0.033333          0.033333 \nmean control..........    0.51786           0.16667 \nstd mean diff.........    -265.38            -73.03 \n\nmean raw eQQ diff.....    0.46667           0.13333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.24226          0.066667 \nmed  eCDF diff........    0.24226          0.066667 \nmax  eCDF diff........    0.48452           0.13333 \n\nvar ratio (Tr/Co).....    0.13112             0.232 \nT-test p-value........ 9.3222e-09          0.040171 \n\n\n***** (V13) `Country in Africa` *****\n                       Before Matching       After Matching\nmean treatment........    0.23333           0.23333 \nmean control..........      0.125           0.23333 \nstd mean diff.........     25.183                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.054167                 0 \nmed  eCDF diff........   0.054167                 0 \nmax  eCDF diff........    0.10833                 0 \n\nvar ratio (Tr/Co).....     1.6617                 1 \nT-test p-value........    0.23622                 1 \n\n\n***** (V14) `Country in Asia` *****\n                       Before Matching       After Matching\nmean treatment........    0.13333           0.13333 \nmean control..........    0.23214               0.3 \nstd mean diff.........    -28.579           -48.205 \n\nmean raw eQQ diff.....        0.1           0.16667 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.049405          0.083333 \nmed  eCDF diff........   0.049405          0.083333 \nmax  eCDF diff........    0.09881           0.16667 \n\nvar ratio (Tr/Co).....    0.65865           0.55026 \nT-test p-value........    0.24898          0.020583 \n\n\n***** (V15) `Country in Oceania` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........   0.071429                 0 \nstd mean diff.........       -Inf                 0 \n\nmean raw eQQ diff.....   0.066667                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.035714                 0 \nmed  eCDF diff........   0.035714                 0 \nmax  eCDF diff........   0.071429                 0 \n\nvar ratio (Tr/Co).....          0               NaN \nT-test p-value........   0.044453                 1 \n\n\nBefore Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Ethic Fractionalization` `Religious Fractionalization` `Life expectancy 1800` `GDP per cap 1800`  Number(s): 5 7 8 9 \n\nAfter Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Year of Independence` `Religious Fractionalization` `Life expectancy 1800` `GDP per cap 1800`  Number(s): 2 7 8 9 \n\n\n\n\n\n\n\n\nIncorporating region wise exact matching\n\n## Now do exact matching\ncolnames(X2)\n\n [1] \"Violent Independenc\"            \"Year of Independence\"          \n [3] \"Independency by Decolonization\" \"Independence by Secession\"     \n [5] \"Ethic Fractionalization\"        \"Linguistic Fractionalization\"  \n [7] \"Religious Fractionalization\"    \"Life expectancy 1800\"          \n [9] \"GDP per cap 1800\"               \"Country in America\"            \n[11] \"Country in South America\"       \"Country in Europe\"             \n[13] \"Country in Africa\"              \"Country in Asia\"               \n[15] \"Country in Oceania\"            \n\nexactX = rep(FALSE, ncol(X2))\ncolnames(X2)[c(10:15)]\n\n[1] \"Country in America\"       \"Country in South America\"\n[3] \"Country in Europe\"        \"Country in Africa\"       \n[5] \"Country in Asia\"          \"Country in Oceania\"      \n\nexactX[c(10:15)] = TRUE\ncolnames(X2)[exactX]\n\n[1] \"Country in America\"       \"Country in South America\"\n[3] \"Country in Europe\"        \"Country in Africa\"       \n[5] \"Country in Asia\"          \"Country in Oceania\"      \n\n# match again with exact region pairing \nmout5 &lt;- Match(Tr=Tr[indx], X=X2[indx,], estimand=\"ATT\", exact=exactX)\nsummary(mout5)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  30 \nMatched number of observations...............  20 \nMatched number of observations  (unweighted).  20 \n\nNumber of obs dropped by 'exact' or 'caliper'  10 \n\nMatchBalance(Tr[indx] ~ . , data = X2[indx,], match.out=mout5, nboots=10)\n\n\n***** (V1) `Violent Independenc` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333               0.6 \nmean control..........        0.5               0.2 \nstd mean diff.........     27.204            79.582 \n\nmean raw eQQ diff.....    0.13333               0.4 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.066667               0.2 \nmed  eCDF diff........   0.066667               0.2 \nmax  eCDF diff........    0.13333               0.4 \n\nvar ratio (Tr/Co).....    0.94376               1.5 \nT-test p-value........    0.23865         0.0016974 \n\n\n***** (V2) `Year of Independence` *****\n                       Before Matching       After Matching\nmean treatment........     1882.7            1912.9 \nmean control..........     1826.1            1954.7 \nstd mean diff.........     81.576           -62.617 \n\nmean raw eQQ diff.....     134.53              43.5 \nmed  raw eQQ diff.....         53              14.5 \nmax  raw eQQ diff.....        840               127 \n\nmean eCDF diff........    0.13314           0.17381 \nmed  eCDF diff........    0.11667               0.2 \nmax  eCDF diff........    0.31667               0.4 \n\nvar ratio (Tr/Co).....   0.070412            9.6233 \nT-test p-value........    0.13242          0.018361 \nKS Bootstrap p-value..        0.1               0.1 \nKS Naive p-value......   0.023689          0.045888 \nKS Statistic..........    0.31667               0.4 \n\n\n***** (V3) `Independency by Decolonization` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333               0.6 \nmean control..........    0.46429               0.9 \nstd mean diff.........      34.49           -59.687 \n\nmean raw eQQ diff.....    0.16667               0.3 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.084524              0.15 \nmed  eCDF diff........   0.084524              0.15 \nmax  eCDF diff........    0.16905               0.3 \n\nvar ratio (Tr/Co).....     0.9486            2.6667 \nT-test p-value........    0.13617         0.0086361 \n\n\n***** (V4) `Independence by Secession` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333              0.35 \nmean control..........    0.23214               0.1 \nstd mean diff.........     21.105            51.087 \n\nmean raw eQQ diff.....        0.1              0.25 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.050595             0.125 \nmed  eCDF diff........   0.050595             0.125 \nmax  eCDF diff........    0.10119              0.25 \n\nvar ratio (Tr/Co).....     1.2666            2.5278 \nT-test p-value........    0.33686          0.050785 \n\n\n***** (V5) `Ethic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.48523           0.50032 \nmean control..........    0.35836           0.51093 \nstd mean diff.........     55.492            -4.463 \n\nmean raw eQQ diff.....     0.1415          0.072524 \nmed  raw eQQ diff.....    0.12276          0.046989 \nmax  raw eQQ diff.....     0.2612           0.24974 \n\nmean eCDF diff........    0.16375          0.098333 \nmed  eCDF diff........    0.15714               0.1 \nmax  eCDF diff........    0.37857              0.25 \n\nvar ratio (Tr/Co).....    0.99957            1.6511 \nT-test p-value........   0.017142           0.76341 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.2 \nKS Naive p-value......  0.0050673           0.51186 \nKS Statistic..........    0.37857              0.25 \n\n\n***** (V6) `Linguistic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.36042           0.45297 \nmean control..........    0.33199           0.44779 \nstd mean diff.........      9.407            1.6561 \n\nmean raw eQQ diff.....   0.052711          0.084695 \nmed  raw eQQ diff.....   0.023094           0.06169 \nmax  raw eQQ diff.....    0.18848           0.27104 \n\nmean eCDF diff........   0.055482             0.105 \nmed  eCDF diff........   0.039881               0.1 \nmax  eCDF diff........    0.20833              0.25 \n\nvar ratio (Tr/Co).....     1.4833           0.86807 \nT-test p-value........    0.66065           0.88091 \nKS Bootstrap p-value..        0.5               0.3 \nKS Naive p-value......    0.31127             0.502 \nKS Statistic..........    0.20833              0.25 \n\n\n***** (V7) `Religious Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.38572           0.44798 \nmean control..........     0.4431           0.56804 \nstd mean diff.........    -25.044           -50.042 \n\nmean raw eQQ diff.....   0.074969           0.12007 \nmed  raw eQQ diff.....   0.053677          0.088513 \nmax  raw eQQ diff.....    0.17146           0.30412 \n\nmean eCDF diff........   0.089106           0.13667 \nmed  eCDF diff........     0.0625               0.1 \nmax  eCDF diff........    0.27381              0.35 \n\nvar ratio (Tr/Co).....     1.0062            1.1251 \nT-test p-value........    0.27231          0.031915 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.3 \nKS Naive p-value......   0.085347           0.15913 \nKS Statistic..........    0.27381              0.35 \n\n\n***** (V8) `Life expectancy 1800` *****\n                       Before Matching       After Matching\nmean treatment........     31.531            30.726 \nmean control..........     33.279            31.717 \nstd mean diff.........    -48.136           -23.898 \n\nmean raw eQQ diff.....     1.6086            1.9916 \nmed  raw eQQ diff.....     1.6776             2.189 \nmax  raw eQQ diff.....        3.1               4.3 \n\nmean eCDF diff........    0.13347           0.15179 \nmed  eCDF diff........    0.11071             0.125 \nmax  eCDF diff........    0.33571              0.35 \n\nvar ratio (Tr/Co).....    0.89741            1.5235 \nT-test p-value........   0.041052           0.27411 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.1 \nKS Naive p-value......   0.016592           0.15476 \nKS Statistic..........    0.33571              0.35 \n\n\n***** (V9) `GDP per cap 1800` *****\n                       Before Matching       After Matching\nmean treatment........     884.67             869.9 \nmean control..........     1215.1            1129.4 \nstd mean diff.........    -103.91           -70.669 \n\nmean raw eQQ diff.....      327.8             304.7 \nmed  raw eQQ diff.....      269.5               212 \nmax  raw eQQ diff.....        796               741 \n\nmean eCDF diff........    0.17895           0.21167 \nmed  eCDF diff........    0.12738               0.2 \nmax  eCDF diff........    0.45357               0.5 \n\nvar ratio (Tr/Co).....    0.33049           0.83115 \nT-test p-value........ 0.00071184         0.0096143 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value...... 0.00035765          0.010558 \nKS Statistic..........    0.45357               0.5 \n\n\n***** (V10) `Country in America` *****\n                       Before Matching       After Matching\nmean treatment........        0.6               0.4 \nmean control..........   0.053571               0.4 \nstd mean diff.........     109.66                 0 \n\nmean raw eQQ diff.....    0.53333                 0 \nmed  raw eQQ diff.....          1                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.27321                 0 \nmed  eCDF diff........    0.27321                 0 \nmax  eCDF diff........    0.54643                 0 \n\nvar ratio (Tr/Co).....     4.8094                 1 \nT-test p-value........ 1.8303e-06                 1 \n\n\n***** (V11) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333                 0 \nmean control..........          0                 0 \nstd mean diff.........     69.522                 0 \n\nmean raw eQQ diff.....    0.33333                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.16667                 0 \nmed  eCDF diff........    0.16667                 0 \nmax  eCDF diff........    0.33333                 0 \n\nvar ratio (Tr/Co).....        Inf               NaN \nT-test p-value........ 0.00067228                 1 \n\n\n***** (V12) `Country in Europe` *****\n                       Before Matching       After Matching\nmean treatment........   0.033333              0.05 \nmean control..........    0.51786              0.05 \nstd mean diff.........    -265.38                 0 \n\nmean raw eQQ diff.....    0.46667                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.24226                 0 \nmed  eCDF diff........    0.24226                 0 \nmax  eCDF diff........    0.48452                 0 \n\nvar ratio (Tr/Co).....    0.13112                 1 \nT-test p-value........ 9.3222e-09                 1 \n\n\n***** (V13) `Country in Africa` *****\n                       Before Matching       After Matching\nmean treatment........    0.23333              0.35 \nmean control..........      0.125              0.35 \nstd mean diff.........     25.183                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.054167                 0 \nmed  eCDF diff........   0.054167                 0 \nmax  eCDF diff........    0.10833                 0 \n\nvar ratio (Tr/Co).....     1.6617                 1 \nT-test p-value........    0.23622                 1 \n\n\n***** (V14) `Country in Asia` *****\n                       Before Matching       After Matching\nmean treatment........    0.13333               0.2 \nmean control..........    0.23214               0.2 \nstd mean diff.........    -28.579                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.049405                 0 \nmed  eCDF diff........   0.049405                 0 \nmax  eCDF diff........    0.09881                 0 \n\nvar ratio (Tr/Co).....    0.65865                 1 \nT-test p-value........    0.24898                 1 \n\n\n***** (V15) `Country in Oceania` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........   0.071429                 0 \nstd mean diff.........       -Inf                 0 \n\nmean raw eQQ diff.....   0.066667                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.035714                 0 \nmed  eCDF diff........   0.035714                 0 \nmax  eCDF diff........   0.071429                 0 \n\nvar ratio (Tr/Co).....          0               NaN \nT-test p-value........   0.044453                 1 \n\n\nBefore Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Ethic Fractionalization` `Religious Fractionalization` `Life expectancy 1800` `GDP per cap 1800`  Number(s): 5 7 8 9 \n\nAfter Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `GDP per cap 1800`  Number(s): 9 \n\nmdataTr = data[indx,][mout5$index.treated,]\nmdataCo = data[indx,][mout5$index.control,]\n# Now look at Jamaica\ndata.frame(mdataTr$country, mdataCo$country)\n\n      mdataTr.country     mdataCo.country\n1               BENIN        SOUTH AFRICA\n2          COSTA RICA             JAMAICA\n3              CYPRUS              ISRAEL\n4  DOMINICAN REPUBLIC             JAMAICA\n5              GAMBIA                MALI\n6               GHANA           MAURITIUS\n7           GUATEMALA TRINIDAD AND TOBAGO\n8            HONDURAS             JAMAICA\n9           INDONESIA              ISRAEL\n10        KOREA SOUTH          BANGLADESH\n11             MALAWI        SOUTH AFRICA\n12             MEXICO             JAMAICA\n13            NAMIBIA          MOZAMBIQUE\n14          NICARAGUA TRINIDAD AND TOBAGO\n15             PANAMA             JAMAICA\n16        PHILIPPINES               INDIA\n17            SENEGAL                MALI\n18            UKRAINE           MACEDONIA\n19      UNITED STATES             JAMAICA\n20             ZAMBIA        SOUTH AFRICA\n\nii = (data$country == \"JAMAICA\" | data$country == \"COSTA RICA\" | data$country == \"MEXICO\")\ndata.frame(as.character(data$country[ii]),X2[ii,])\n\n  as.character.data.country.ii.. Violent.Independenc Year.of.Independence\n1                     COSTA RICA                   1                 1838\n2                        JAMAICA                   0                 1962\n3                         MEXICO                   1                 1821\n  Independency.by.Decolonization Independence.by.Secession\n1                              0                         1\n2                              1                         0\n3                              1                         0\n  Ethic.Fractionalization Linguistic.Fractionalization\n1                0.236800                    0.0489116\n2                0.412894                    0.1098046\n3                0.541800                    0.1511190\n  Religious.Fractionalization Life.expectancy.1800 GDP.per.cap.1800\n1                   0.2409582              30.2147              812\n2                   0.6159606              34.2000             1644\n3                   0.1795571              26.9000             1420\n  Country.in.America Country.in.South.America Country.in.Europe\n1                  1                        0                 0\n2                  1                        0                 0\n3                  1                        0                 0\n  Country.in.Africa Country.in.Asia Country.in.Oceania\n1                 0               0                  0\n2                 0               0                  0\n3                 0               0                  0\n\n\n\n\nComparing OLS with Matched Difference of Means\n\n# Outcome 1: President/PM held party position immediately prior\nindx2 = indx & !is.na(data$prexpty) # index for full observations from X2 and our variable of interest\n\n# matching ATT\nmout &lt;- Match(Y=data$prexpty[indx2], Tr=Tr[indx2], X=X2[indx2,], estimand=\"ATT\")\nsummary(mout)\n\n\nEstimate...  -0.42857 \nAI SE......  0.16342 \nT-stat.....  -2.6224 \np.val......  0.0087303 \n\nOriginal number of observations..............  82 \nOriginal number of treated obs...............  28 \nMatched number of observations...............  28 \nMatched number of observations  (unweighted).  28 \n\nt.test(data$prexpty[Tr==1],data$prexpty[Tr==0])\n\n\n    Welch Two Sample t-test\n\ndata:  data$prexpty[Tr == 1] and data$prexpty[Tr == 0]\nt = -0.731, df = 64.438, p-value = 0.4674\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2316728  0.1075349\nsample estimates:\nmean of x mean of y \n 0.137931  0.200000 \n\natt_df &lt;- cbind.data.frame( prexpty = data$prexpty[indx2],\n                 Tr = as.numeric(Tr[indx2])) %&gt;% tibble()\nstats::lm(prexpty ~ Tr, data = att_df)\n\n\nCall:\nstats::lm(formula = prexpty ~ Tr, data = att_df)\n\nCoefficients:\n(Intercept)           Tr  \n    0.20370     -0.06085  \n\nsummary(lm(data$prexpty[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,])))\n\n\nCall:\nlm(formula = data$prexpty[indx2] ~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2, \n    ]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5074 -0.2009 -0.1208 -0.0032  0.9050 \n\nCoefficients: (1 not defined because of singularities)\n                                                       Estimate Std. Error\n(Intercept)                                          -6.817e-01  6.974e-01\nas.numeric(Tr[indx2])                                -2.156e-01  1.484e-01\nas.matrix(X2[indx2, ])Violent Independenc             5.575e-02  1.056e-01\nas.matrix(X2[indx2, ])Year of Independence            3.643e-04  2.885e-04\nas.matrix(X2[indx2, ])Independency by Decolonization -1.751e-02  1.580e-01\nas.matrix(X2[indx2, ])Independence by Secession      -7.567e-02  1.710e-01\nas.matrix(X2[indx2, ])Ethic Fractionalization         6.112e-02  2.946e-01\nas.matrix(X2[indx2, ])Linguistic Fractionalization   -1.617e-01  2.532e-01\nas.matrix(X2[indx2, ])Religious Fractionalization     3.356e-01  2.381e-01\nas.matrix(X2[indx2, ])Life expectancy 1800            3.274e-04  1.493e-02\nas.matrix(X2[indx2, ])GDP per cap 1800                4.305e-05  1.393e-04\nas.matrix(X2[indx2, ])Country in America              2.315e-01  3.196e-01\nas.matrix(X2[indx2, ])Country in South America       -2.853e-02  1.957e-01\nas.matrix(X2[indx2, ])Country in Europe              -5.376e-02  2.695e-01\nas.matrix(X2[indx2, ])Country in Africa               2.320e-01  2.844e-01\nas.matrix(X2[indx2, ])Country in Asia                 1.141e-01  2.581e-01\nas.matrix(X2[indx2, ])Country in Oceania                     NA         NA\n                                                     t value Pr(&gt;|t|)\n(Intercept)                                           -0.978    0.332\nas.numeric(Tr[indx2])                                 -1.453    0.151\nas.matrix(X2[indx2, ])Violent Independenc              0.528    0.599\nas.matrix(X2[indx2, ])Year of Independence             1.263    0.211\nas.matrix(X2[indx2, ])Independency by Decolonization  -0.111    0.912\nas.matrix(X2[indx2, ])Independence by Secession       -0.442    0.660\nas.matrix(X2[indx2, ])Ethic Fractionalization          0.207    0.836\nas.matrix(X2[indx2, ])Linguistic Fractionalization    -0.639    0.525\nas.matrix(X2[indx2, ])Religious Fractionalization      1.410    0.163\nas.matrix(X2[indx2, ])Life expectancy 1800             0.022    0.983\nas.matrix(X2[indx2, ])GDP per cap 1800                 0.309    0.758\nas.matrix(X2[indx2, ])Country in America               0.724    0.471\nas.matrix(X2[indx2, ])Country in South America        -0.146    0.885\nas.matrix(X2[indx2, ])Country in Europe               -0.200    0.842\nas.matrix(X2[indx2, ])Country in Africa                0.816    0.418\nas.matrix(X2[indx2, ])Country in Asia                  0.442    0.660\nas.matrix(X2[indx2, ])Country in Oceania                  NA       NA\n\nResidual standard error: 0.3958 on 66 degrees of freedom\nMultiple R-squared:  0.1563,    Adjusted R-squared:  -0.03543 \nF-statistic: 0.8152 on 15 and 66 DF,  p-value: 0.6572\n\n\n\n## OLS versus Matching (paying attention to South America dummy)\nindx2 = indx & !is.na(data$prexpty)\nmout2 &lt;- Match(Y=data$prexpty[indx2], Tr=Tr[indx2], X=X2[indx2,11], estimand=\"ATT\")\nsummary(mout2)\n\n\nEstimate...  -0.060847 \nAI SE......  0.083155 \nT-stat.....  -0.73172 \np.val......  0.46434 \n\nOriginal number of observations..............  82 \nOriginal number of treated obs...............  28 \nMatched number of observations...............  28 \nMatched number of observations  (unweighted).  1512 \n\nMatchBalance(Tr[indx2] ~. , data =  X2[indx2,11], match.out=mout2, nboots=10)\n\n\n***** (V1) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.32143           0.32143 \nmean control..........          0                 0 \nstd mean diff.........     67.585            67.585 \n\nmean raw eQQ diff.....    0.32143           0.32143 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.16071           0.16071 \nmed  eCDF diff........    0.16071           0.16071 \nmax  eCDF diff........    0.32143           0.32143 \n\nvar ratio (Tr/Co).....        Inf               Inf \nT-test p-value........   0.001342         0.0011322 \n\n#South America does not has a control group\nsummary(lm(data$prexpty[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,11])))\n\n\nCall:\nlm(formula = data$prexpty[indx2] ~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2, \n    11]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2037 -0.2037 -0.2037 -0.1111  0.8889 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.20370    0.05342   3.813 0.000271 ***\nas.numeric(Tr[indx2])    -0.04581    0.10471  -0.437 0.662962    \nas.matrix(X2[indx2, 11]) -0.04678    0.15885  -0.295 0.769140    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3926 on 79 degrees of freedom\nMultiple R-squared:  0.006661,  Adjusted R-squared:  -0.01849 \nF-statistic: 0.2649 on 2 and 79 DF,  p-value: 0.768\n\nsummary(lm(data$prexpty[indx2]~ as.numeric(Tr[indx2])))\n\n\nCall:\nlm(formula = data$prexpty[indx2] ~ as.numeric(Tr[indx2]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2037 -0.2037 -0.2037 -0.1429  0.8571 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            0.20370    0.05312   3.835 0.000249 ***\nas.numeric(Tr[indx2]) -0.06085    0.09090  -0.669 0.505167    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3903 on 80 degrees of freedom\nMultiple R-squared:  0.00557,   Adjusted R-squared:  -0.00686 \nF-statistic: 0.4481 on 1 and 80 DF,  p-value: 0.5052\n\n\n\nlinreg &lt;- lm(data$prexpty[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,]))\ncat(\"Linear regression effect is\", summary(linreg)$coefficients[2,1], \" with p-value\", \n    summary(linreg)$coefficients[2,4] ,\"\\n\")\n\nLinear regression effect is -0.2155648  with p-value 0.1509716 \n\ncat(\"Matching effect is\", mout$est, \"with p-value\",  2 * (1-pnorm(abs(mout$est/mout$se))),\"\\n\")\n\nMatching effect is -0.4285714 with p-value 0.008730257 \n\n\n\n# Outcome 2: President/PM was member of parliament immediately prior to taking office \nindx2 = indx & !is.na(data$prexmp)\nmout &lt;- Match(Y=data$prexmp[indx2], Tr=Tr[indx2], X=X2[indx2,], estimand=\"ATT\")\nsummary(mout)\n\n\nEstimate...  -0.32143 \nAI SE......  0.18629 \nT-stat.....  -1.7255 \np.val......  0.084444 \n\nOriginal number of observations..............  82 \nOriginal number of treated obs...............  28 \nMatched number of observations...............  28 \nMatched number of observations  (unweighted).  28 \n\nt.test(data$prexmp[Tr==1],data$prexmp[Tr==0])\n\n\n    Welch Two Sample t-test\n\ndata:  data$prexmp[Tr == 1] and data$prexmp[Tr == 0]\nt = -1.8512, df = 64.388, p-value = 0.06873\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.40538134  0.01541269\nsample estimates:\nmean of x mean of y \n0.2413793 0.4363636 \n\nlinreg2 &lt;- summary(lm(data$prexmp[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,])))\n\ncat(\"Linear regression effect is\", linreg2$coefficients[2,1], \" with p-value\", \n    linreg2$coefficients[2,4],\"\\n\")\n\nLinear regression effect is -0.4289868  with p-value 0.01406203 \n\ncat(\"Matching effect is\", mout$est, \"with p-value\",  2 * (1-pnorm(abs(mout$est/mout$se))),\"\\n\")\n\nMatching effect is -0.3214286 with p-value 0.08444445",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#additional-links",
    "href": "matching.html#additional-links",
    "title": "Matching",
    "section": "Additional Links",
    "text": "Additional Links\n\nMatching in The Effect Book.\nMatching and Subclassification in Causal Inference: The Mixtape.\nMatchIt Package.\n\n\n\n\n\nCunningham, Scott. n.d. “Causal Inference the Mixtape - 5  Matching and Subclassification.” In. https://mixtape.scunning.com/05-matching_and_subclassification.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "equivalence.html",
    "href": "equivalence.html",
    "title": "Equivalence",
    "section": "",
    "text": "Equivalence Testing for Causal Inference in Social Science Research Design\nCausal Identification is critically dependent on the underlying assumptions about the data generating process. These assumptions are inherently untestable but have observable implications that are empirically validated. Conceptually, uncounfoundedness which is achieved by randomization in experiments and is conditionally achieved in analysis of observational and quasi-experimental data, is tested by balance tests. In simple terms, balance tests check that if indeed the treatment assignemnt was independent of potential outcomes on controlling for vector of confounders, the covariates do not significantly differ between the control and treatment groups.\nThis implies, to test unconfoundedness assumption or CIA, the null-hypothesis we should begin with is that of data being inconsistent with a research design valid for causal inference.\nThis can be framed in hypothesis testing terms as:\n\\(H_0:\\) Data Inconsistent with observable implications of uncounfounded Research Design.\n\\(\\implies\\) Covariates’ distribution not same across Treatment Groups.\n\\(H_A:\\) Data consistent with observable implications of uncounfounded Research Design.\n\\(\\implies\\) Covariates’ distribution same across Treatment Groups.\nHowever, as has traditionally been the practice, also seen in most of the approaches and papers in your coursework, especially those on natural experiments, the balance tests infer the non-significance of difference between distribution of covariates, as sameness of distribution between treatment groups. As Hartman and Hidalgo (2018) put it, this practice is akin to “incorrectly equating non-significant difference with significant homogeniety” (quoted from Wellek 2010).\nTraditional balance tests, which rely on null hypotheses of no difference, can be misleading due to issues of statistical power. Hartman and Hidalgo (2018) advocate for equivalence tests, where the null hypothesis assumes a meaningful difference exists, and researchers aim to find evidence for equivalence within a pre-defined range. The guide emphasizes the importance of selecting appropriate equivalence ranges and interpreting the results in the context of potential bias and causal identification.\nKey Concepts:",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "equivalence.html#mechanics",
    "href": "equivalence.html#mechanics",
    "title": "Equivalence",
    "section": "Mechanics",
    "text": "Mechanics\nEquivalence T test assumes the following Two One Sided Test (TOST) form: \\[\n\\begin{align*}\nH_0: \\frac{\\mu_T - \\mu_C}{\\sigma} &\\geq \\epsilon_U \\quad \\text{or} \\quad \\frac{\\mu_T - \\mu_C}{\\sigma} \\leq \\epsilon_L \\\\\n\\text{versus} \\\\\nH_1: \\epsilon_L &&lt; \\frac{\\mu_T - \\mu_C}{\\sigma} &lt; \\epsilon_U\n\\end{align*} \\\\\n\\] where [\u0005\\(\\epsilon_L\\) , \\(\\epsilon_U\\) ] is the equivalence range, \u0003\\(\\mu_T\\) and \\(\\mu_C\\)\u0003 are the means of the treated and control groups, respectively, for a given covariate, and \\(\\sigma\\)is the common standard deviation. The terms \\(\\epsilon_U\\) and \\(\\epsilon_L\\) are the upper and lower bounds for which two groups are considered equivalent.\nChoosing appropriate values for \\(\\epsilon_U\\) and \\(\\epsilon_U\\) is the most important aspect of equivalence testing, (Refer to Selecting an Equivalence Range section in H&H 2018 for more details).\nAs shown here, test is conducted using two one-sided t-tests, and the null of difference is rejected in favor of equivalence if the p-value for both one-sided tests is less than \\(\\alpha\\)\n\n\n\n\n\n\n\n\nFigure 1: Source- Hartman and Hidalgo, 2018, pp.5",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "equivalence.html#equivalence-testing-in-requivalence-1",
    "href": "equivalence.html#equivalence-testing-in-requivalence-1",
    "title": "Equivalence",
    "section": "Equivalence Testing in R1",
    "text": "Equivalence Testing in R1\nThe equivalence testing package developed by Hartman and Hidalgo (2018) is not yet available on CRAN. We install it from github by using the following code:\n\n#install.packages(\"devtools\")\nlibrary(devtools)\n\nLoading required package: usethis\n\n#install_github(\"ekhartman/equivtest\", force = TRUE)\nlibrary(equivtest)\n\nUsing the sample example from H&H 2018\nThe equivalence range for the t-test for equivalence is typically defined in standardized differences rather than the raw difference in means between the two groups, but researchers can easily map their substantive ranges to standardized differences by scaling by the standard deviation in the covariate. The standardized difference is a useful metric when testing for equivalence because, given some difference between the means of the two distributions, the two groups are increasingly indistinguishable as the variance of the distributions grows towards infinity, and increasingly disjoint as the variance of the distributions shrinks towards zero (Wellek 2010). We also recommend the t-test for equivalence because it is the uniformly most powerful invariant (UMPI) test for two normally distributed variables (Wellek 2010, pg. 120).\nFor the equivalence t-test, we are interested in \\(H_1: \\epsilon_L &lt; \\frac{\\mu_T-\\mu_C}{\\sigma} &lt; \\epsilon_U\\). For more information concerning acceptable \\(\\epsilon\\) inputs, refer to the equiv.t.test documentation or Hartman & Hidalgo (2018).\nWe now implement Example 6.1 from Wellek (2010). In summary, we wish to compare two treatments using a nonsymmetric equivalence range.\n\n# Wellek p 124\n\nx=c(10.3,11.3,2,-6.1,6.2,6.8,3.7,-3.3,-3.6,-3.5,13.7,12.6)\ny=c(3.3,17.7,6.7,11.1,-5.8,6.9,5.8,3,6,3.5,18.7,9.6)\n\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -1.0862, df = 21.915, p-value = 0.2892\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.826339  2.759672\nsample estimates:\nmean of x mean of y \n 4.175000  7.208333 \n\nres=equiv.t.test(x,y,eps_std=c(.5,1), alpha = .05)\nsummary(res)\n\nEquivalence t-test \nInput: eps_std,  SE = 2.793\nT-statistic critical interval: 0.28 to 0.931 \nSubstantive equivalence CI: NA to NA \nStandardized equivalence CI: NA to NA \nReject the null hypothesis? FALSE, p-value of NA\n\n# Compare with t.test\n\nThe data do not allow to reject the null hypothesis of nonequivalence of (treatment A) to (treatment B).\n\nHow does the equiv.t.test function work?\n\nCode from Hartman Github Page with NotesKey Terms in the Function\n\n\n\n\nCode\n```{r}\n#| code-fold: true\nequiv.t.test &lt;- function(x, y, alpha = .05, epsilon = .2, std.err = \"nominal\", cluster.x = NULL, cluster.y = NULL) {\n\n  # Remove NAs from the data\n  x = x[!is.na(x)]\n  y = y[!is.na(y)]\n\n  # Calculate the difference in means\n  dbar &lt;- mean(x) - mean(y)\n\n  # Get the sample sizes as doubles\n  m &lt;- as.double(length(x))\n  n &lt;- as.double(length(y))\n  N &lt;- m+n\n\n  # Calculate the variances of each group\n  x.var &lt;- var(x)\n  y.var &lt;- var(y)\n\n  # Calculate the non-centrality parameter for the power calculation\n  non.cent &lt;- (m*n*epsilon^2)/N\n\n  # Calculate the critical value for the t-statistic based on the non-centrality parameter\n  critical.const &lt;- sqrt(qf(alpha,1,N-2,non.cent))\n\n  # Calculate the standard error of the difference in means\n  se = sqrt((m-1)*x.var + (n-1)*y.var) / sqrt(m*n * (N-2)/N)\n\n  # Calculate the degrees of freedom\n  df = N - 2\n\n  # Calculate the t-statistic\n  t.stat &lt;- dbar / se\n\n  # Calculate the p-value\n  p = pf(abs(t.stat)^2, 1, df , non.cent)\n\n  # Calculate the observed standardized mean difference\n  obs_smd = (mean(x) - mean(y)) / sd(y)\n\n  # Calculate the inverted test statistic\n  inverted &lt;- try(uniroot(function(x) pf(abs(t.stat)^2, 1, N-2, ncp = (m*n*x^2)/N) - ifelse(pf(abs(t.stat)^2, 1, N-2, ncp = (m*n*0^2)/N) &lt; alpha, pf(abs(t.stat)^2, 1, N-2, ncp = (m*n*obs_smd^2)/N), alpha), c(0,10*abs(t.stat)), tol = 0.0001)$root, silent = TRUE)\n\n  # If the uniroot function throws an error, set the inverted test statistic to NA\n  if(class(inverted) == \"try-error\") {\n    inverted = NA\n  }\n\n  # Determine if the null hypothesis should be rejected\n  rej = abs(t.stat) &lt;= critical.const\n\n  # Return the test results\n  return(list(t.stat = t.stat, critical.const = critical.const, power = 2*pt(critical.const, N-2)-1, rej = rej, p = p, inverted = inverted))\n\n}\n```\n\n\nThe code defines a function equiv.t.test that performs an equivalence t-test.\n\nThe function takes two vectors of data (x and y) as input, along with several optional parameters.\nThe function first calculates the difference in means between the two groups, the standard error of the difference, the degrees of freedom, and the t-statistic.\nIt then calculates the p-value for the test.\nThe function also calculates the power of the test, which is the probability of rejecting the null hypothesis when it is false.\nFinally, the function returns a list of results, including the t-statistic, the critical value, the power, the rejection decision, the p-value, and the inverted test statistic.\n\n\n\n\nEquivalence Testing: A statistical test used to determine if two groups are similar within a predefined margin, rather than significantly different.\nt-test: A statistical test used to compare the means of two groups.\nAlpha (\\(\\alpha\\)): The significance level, typically set at 0.05, representing the probability of rejecting the null hypothesis when it is true (Type I error).\nEpsilon (\\(\\epsilon\\)): The equivalence margin, defining the maximum difference between two groups considered practically insignificant.\nDegrees of Freedom (df): The number of values in a statistical calculation free to vary.\np-value: The probability of obtaining the observed results (or more extreme) if the null hypothesis were true.\nStandard Error (SE): A measure of the variability of a sample mean.\nCritical Constant: The threshold value used to determine whether to reject the null hypothesis.\nPower: The probability of correctly rejecting the null hypothesis when it is false.\nUniroot Function: An R function used to find the root (solution) of an equation.\nNon-centrality Parameter (ncp): A parameter in non-central distributions, like the non-central t-distribution, that measures the departure from the null hypothesis.\nStandardized Mean Difference (SMD): A measure of effect size, calculated as the difference between two means divided by the standard deviation.\n\n\n\n\n\n\n\nIllustration used in Hartman and Hidalgo, 2018\nUsing data from Brady and Mcnulty (2011)\n\n\n\n\n\n\nAbstract:\nCould changing the locations of polling places affect the outcome of an election by increasing the costs of voting for some and decreasing them for others? The consolidation of voting precincts in Los Angeles County during California’s 2003 gubernatorial recall election provides a natural experiment for studying how changing polling places influences voter turnout. Overall turnout decreased by a substantial 1.85 percentage points: A drop in polling place turnout of 3.03 percentage points was partially offset by an increase in absentee voting of 1.18 percentage points. Both transportation and search costs caused these changes. Although there is no evidence that the Los Angeles Registrar of Voters changed more polling locations for those registered with one party than for those registered with another, the changing of polling places still had a small partisan effect because those registered as Democrats were more sensitive to changes in costs than those registered as Republicans. The effects were small enough to allay worries about significant electoral consequences in this instance (e.g., the partisan effect might be decisive in only about one in two hundred contested House elections), but large enough to make it possible for someone to affect outcomes by more extensive manipulation of polling place locations.\n\n\n\nFrom page 119,\n\n\n\n\n\n\nThose who had their polling place changed in 2003 had to go an average distance of 0.354 miles in 2002, # whereas those who did not have their polling place changed had to go only 0.320 miles—a difference of 0.034 miles”\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing code is from equivalence_replication_file.R from H&H 2018 replication docket available here\n\n## From figure 1 -- 3045206 voters, assuming roughly equal split between treatment and control\n# Difference of means between the two groups\ndbar &lt;- 0.034\n\n# Sample sizes for group 1 and group 2 (half of the total sample size)\nm &lt;- (3045206)/2\nn &lt;- (3045206)/2\n\n# Total sample size\nN &lt;- m + n\n\n# Variances for group 1 and group 2 (both equal here)\nx.var &lt;- (.2772)^2\ny.var &lt;- (.2772)^2\n\n# Tolerance level of 0.2 standard deviations\nepsilon &lt;- 0.2 # As per H&H, page 18\n\n# Significance level (alpha = 5%)\nalpha &lt;- 0.05\n\n# Non-centrality parameter (NCP)\nnon.cent &lt;- (m * n * epsilon^2) / N\n# This measures how far the true difference is from the null hypothesis under the alternative hypothesis\n\n# Critical constant for the F-distribution (inverse CDF)\ncritical.const &lt;- sqrt(qf(alpha, 1, N - 2, non.cent))\n# The critical constant determines the boundary value for hypothesis testing\n\n# T-statistic calculation\nt.stat &lt;- sqrt(m * n * (N - 2) / N) * dbar / sqrt((m - 1) * x.var + (n - 1) * y.var)\n# The t-statistic is used to test whether the observed difference in means is statistically significant\n\n# P-value calculation using the F-distribution CDF\np = pf(abs(t.stat)^2, 1, N - 2, non.cent)\n# The p-value indicates the probability of observing such a t-statistic under the null hypothesis\n\n# Finding the root for the equivalence confidence interval\ninverted &lt;- uniroot(function(x) pf(abs(t.stat)^2, 1, N - 2, ncp = (m * n * x^2) / N) - alpha, \n                    c(0, 2 * abs(t.stat)), tol = 0.0001)$root\n# 'uniroot()' is used to find the boundary where the p-value equals alpha (i.e., the confidence interval)\n\n# Output the p-value\np # Prints the p-value to assess significance\n\n[1] 0\n\n# Output the equivalence confidence interval in standardized terms\ninverted # Prints the confidence interval in terms of standardized differences\n\n[1] 0.1245523\n\n# Convert the standardized confidence interval to real terms (based on variance)\ninverted * sqrt(y.var) # Prints the confidence interval in real terms\n\n[1] 0.0345259\n\n# Calculate the inverted value in real terms (one side of the CI)\ninverted_real &lt;- inverted * sqrt(y.var)\n\n# Calculate the lower and upper bounds of the confidence interval\n(lower_bound &lt;- dbar - inverted_real)\n\n[1] -0.0005259025\n\n(upper_bound &lt;- dbar + inverted_real)\n\n[1] 0.0685259\n\n\nWe currently did the equivalence test for one covariate - distance - only.\nTo understand for to run this for multiple covariates at once:\n\nDownload the replcication docket for Hartman and Hidalgo (2018) from here.\nOpen and follow the file equivalence_replication_file.R.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "equivalence.html#how-to-run-equivalence-tests-on-your-own-data",
    "href": "equivalence.html#how-to-run-equivalence-tests-on-your-own-data",
    "title": "Equivalence",
    "section": "How to run equivalence tests on your own data?",
    "text": "How to run equivalence tests on your own data?\n\n\n\n\n\n\nSince, the equivtest package from Hartman and Hidalgo is not on CRAN, downloading and installing it on different systems can create some issues on some of them.\nFollowing are the steps to do the same test without loading the package.\n\nDownload the equiv-t-test.R script from here.\nRun the whole script after opening it in the same project window as your replication assignment. This would load three functions in your environment - equiv.t.test, generate_plot, and run_equiv.\n\nThese functions are from the replication packet of Hartman and Hidalgo (2018).\n\nOpen the replcication docket for Hartman and Hidalgo (2018) from here. Go through the file equivalence_replication_file.R from this docket to understand how the functions loaded in step 2 are used.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "equivalence.html#glossary-of-key-terms-in-hh-2018",
    "href": "equivalence.html#glossary-of-key-terms-in-hh-2018",
    "title": "Equivalence",
    "section": "Glossary of Key Terms in H&H 2018",
    "text": "Glossary of Key Terms in H&H 2018\n\nBalance: Refers to the similarity of the distributions of pretreatment covariates between the treatment and control groups in a study.\nBias: Systematic error in the estimation of a causal effect, often arising from confounding factors.\nCausal Empiricism: An approach to research that emphasizes the importance of testing the plausibility of causal assumptions using empirical data.\nConfounding: Occurs when a variable is associated with both the treatment and the outcome, making it difficult to isolate the treatment’s true effect.\nEquivalence: In the context of statistical testing, equivalence implies that the difference between two groups or parameters is within a predefined range considered substantively unimportant.\nExchangeability: The idea that the treatment and control groups are sufficiently similar that they could have been interchanged without affecting the outcome of interest.\nIdentification Assumption: An untestable assumption about the data-generating process that is necessary to estimate a causal effect.\nNatural Experiment: A study where the assignment of treatment is “as-if” random, occurring due to external factors or policy changes, rather than through researcher manipulation.\nNull Hypothesis: A statement of no effect or no difference, often used as the baseline for statistical testing. In equivalence testing, the null hypothesis typically posits a meaningful difference.\nObservational Study: Research where the researcher observes and measures variables without directly manipulating the treatment or exposure.\nPlacebo Effect: An observed effect on an outcome that is due to the act of receiving a treatment or intervention itself, rather than the treatment’s specific active ingredients.\nPower: The probability of correctly rejecting the null hypothesis when it is false. Higher power indicates a greater ability to detect a true effect or difference.\nRandomization: The process of randomly assigning units to treatment and control groups, which helps to ensure that the groups are similar on average.\nSensitivity Analysis: A method for examining how sensitive the results of an analysis are to changes in assumptions, such as the presence of unobserved confounding.\nStandardized Effect Size: A measure of the magnitude of an effect (e.g., difference between groups) that is standardized to a common scale, often using standard deviations, to allow for comparisons across studies or variables with different units.\nStatistical Significance: The likelihood of observing the data or more extreme results if the null hypothesis were true. Often determined by a p-value.\nSubstantive Significance: Whether a statistically significant finding is meaningful or important in the context of the research question and the real world.\nType I Error: Incorrectly rejecting a true null hypothesis. In equivalence testing, this would mean incorrectly concluding equivalence when a meaningful difference exists.\nType II Error: Incorrectly failing to reject a false null hypothesis. In equivalence testing, this would mean failing to conclude equivalence when the groups are actually equivalent within the defined range.\n\n\n\n\n\nBrady, Henry E., and John E. Mcnulty. 2011. “Turning Out to Vote: The Costs of Finding and Getting to the Polling Place.” The American Political Science Review 105 (1): 115–34. https://www.jstor.org/stable/41480830.\n\n\nHartman, Erin. 2021. “Equivalence Testing for Regression Discontinuity Designs.” Political Analysis 29 (4): 505–21. https://doi.org/10.1017/pan.2020.43.\n\n\nHartman, Erin, and F. Daniel Hidalgo. 2018. “An Equivalence Approach to Balance and Placebo Tests.” American Journal of Political Science 62 (4): 1000–1013. https://doi.org/10.1111/ajps.12387.\n\n\nWellek, Stefan. 2010. Testing statistical hypotheses of equivalence and noninferiority. Second edition. Boca Raton London New York: CRC Press, Chapman & Hall.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "equivalence.html#footnotes",
    "href": "equivalence.html#footnotes",
    "title": "Equivalence",
    "section": "",
    "text": "This section is adapted from the code and documentation given on https://github.com/ekhartman/equivtest↩︎",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "missing-data.html",
    "href": "missing-data.html",
    "title": "Session 3 - Missing Data and Multiple Imputation",
    "section": "",
    "text": "Today’s Lab",
    "crumbs": [
      "Session 3 - Missing Data and Multiple Imputation"
    ]
  },
  {
    "objectID": "missing-data.html#todays-lab",
    "href": "missing-data.html#todays-lab",
    "title": "Session 3 - Missing Data and Multiple Imputation",
    "section": "",
    "text": "Mechanisms of Missingnness\nDescription of Missingness\nRemoval of Missingness\n\n\nHot Decking\nMultiple Imputation\nAnalysis with Imputed Datasets",
    "crumbs": [
      "Session 3 - Missing Data and Multiple Imputation"
    ]
  },
  {
    "objectID": "missing-data.html#mechanisms-of-missingnness",
    "href": "missing-data.html#mechanisms-of-missingnness",
    "title": "Session 3 - Missing Data and Multiple Imputation",
    "section": "Mechanisms of Missingnness",
    "text": "Mechanisms of Missingnness\nIn data, missingness has been classified into three types based on the data generating mechanism leading to the occurrence of missingness (Rubin 1976, Little 2021). The following set-up explains them using DAGs.\nSuppose there is an outcome of interest - Vote for Far-Right Candidate (\\(Y\\)). An explanatory variable of interest is \\(Age\\). An indicator variable (\\(R_Y\\)) captures the reporting of \\(Y\\), and \\(Y_{Latent}\\) is a latent variable which captures the intention to vote for far right candidate.\n\nMCAR\n\n\nCode\ndag_nomiss &lt;- dagify(\n  Y ~ Y_l + R_Y,\n  Y_l ~ Age,\n  exposure = \"Age\",\n  outcome = \"Y\",\n  coords = list(\n    x = c(Age = 1, R_Y = 2, Y_l = 2, Y = 3),\n    y = c(Age = 0, R_Y = 1, Y_l = -1, Y = 0)\n  )\n)\n\n(mcar &lt;- ggdag(dag_nomiss) + theme_dag())\n\n\n\n\n\n\n\n\n\n\n\nMAR\n\n\nCode\n# MAR\ndag_nomiss_1 &lt;- dagify(\n  Y ~ Y_l + R_Y,\n  Y_l ~ Age,\n  R_Y ~ Age,\n  exposure = \"Age\",\n  outcome = \"Y\",\n  coords = list(\n    x = c(Age = 1, R_Y = 2, Y_l = 2, Y = 3),\n    y = c(Age = 0, R_Y = 1, Y_l = -1, Y = 0)\n  )\n)\n\n(mar &lt;- ggdag(dag_nomiss_1) + theme_dag())\n\n\n\n\n\n\n\n\n\n\n\nNMAR\n\n\nCode\n# NMAR\ndag_nomiss_2 &lt;- dagify(\n  Y ~ Y_l + R_Y,\n  Y_l ~ Age,\n  R_Y ~ Age,\n  R_Y ~ Y_l,\n  exposure = \"Age\",\n  outcome = \"Y\",\n  coords = list(\n    x = c(Age = 1, R_Y = 2, Y_l = 2, Y = 3),\n    y = c(Age = 0, R_Y = 1, Y_l = -1, Y = 0)\n  )\n)\n\n(nmar &lt;- ggdag(dag_nomiss_2) + theme_dag())",
    "crumbs": [
      "Session 3 - Missing Data and Multiple Imputation"
    ]
  },
  {
    "objectID": "missing-data.html#description-of-missingness",
    "href": "missing-data.html#description-of-missingness",
    "title": "Session 3 - Missing Data and Multiple Imputation",
    "section": "Description of Missingness",
    "text": "Description of Missingness\n\n\n\n\n\n\nNote\n\n\n\nFor this and the rest of the lab today we will use the paper and dataset used by Honaker, King, and Blackwell (2011) in their vignette for Amelia II\n\nPaper:\nMilner, Helen V., and Keiko Kubota. 2005. “Why the Move to Free Trade? Democracy and Trade Policy in the Developing Countries.” International Organization 59 (1): 107–43. https://www.jstor.org/stable/3877880.\n\n\nAbstract:\nRising international trade flows are a primary component of globalization. The liberalization of trade policy in many developing countries has helped foster the growth of these flows. Preceding and concurrent with this move to free trade, there has been a global movement toward democracy. We argue that these two trends are related: democratization of the political system reduces the ability of governments to use trade barriers as a strategy for building political support. Political leaders in labor-rich countries may prefer lower trade barriers as democracy increases. Empirical evidence supports our claim about the developing countries from 1970-99. Regime change toward democracy is associated with trade liberalization, controlling for many factors. Conventional explanations of economic reform, such as economic crises and external pressures, seem less salient. Democratization may have fostered globalization in this period.\n\n\n\nEstimation\n\nModel SpecificationLatex Code\n\n\n\\[\\begin{aligned}\ntradepolicy_{i,t} = \\beta_0 & + \\beta_1 REGIME_{i,t-1} + \\beta_3 IMF_{i,t-1} \\\\\n& + \\beta_4OFFICE_{i,t-1} + \\beta_5GDPPC_{i,t-1} \\\\\n& + \\beta_6LNPOP_{i,t-1} + \\beta_7ECCRISIS_{i,t-1} \\\\\n& + \\beta_8BPCRISIS_{i,t-1} + \\beta_9AVOPEN_{i,t-1} \\\\\n& + u_i + \\epsilon_{i,t}\n\\end{aligned}\\]\n\n\nThe following code is not needed for exercise today. I have written the Latex equation to given an example of how to write multi line equations in paper. Also note, there are dependencies on some Latex packages which some of which I add between lines 28-38 in the code above.\n$$\\begin{aligned}\ntradepolicy_{i,t} = \\beta_0 & + \\beta_1 REGIME_{i,t-1} + \\beta_3 IMF_{i,t-1} \\\\\n& + \\beta_4OFFICE_{i,t-1} + \\beta_5GDPPC_{i,t-1} \\\\\n& + \\beta_6LNPOP_{i,t-1} + \\beta_7ECCRISIS_{i,t-1} \\\\\n& + \\beta_8BPCRISIS_{i,t-1} + \\beta_9AVOPEN_{i,t-1} \\\\\n& + u_i + \\epsilon_{i,t}\n\\end{aligned}$$\n\n\n\n\nIn the example from Amelia page, a modified dataset is used\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nyear\n\n\ncountry\ncountry\n\n\ntariff\naverage tariff rates\n\n\npolity\nPolity IV Score[^polity]\n\n\npop\ntotal population\n\n\ngdp.pc\ngross domestic product per capita\n\n\nintresmi\ngross international reserves\n\n\nsigned\ndummy variable if signed an IMF agreement that year\n\n\nfivop\nmeasure of financial openness\n\n\nusheg\nmeasure of US hegemony[^hegemony]\n\n\n\n\n## library(Amelia) # Already loaded above, here just to remind\ndata(\"freetrade\")  # Modified dataset, included in the package\n\n\n# Seeing summary of all the variables\nsummary(freetrade)\n\n      year        country              tariff           polity      \n Min.   :1981   Length:171         Min.   :  7.10   Min.   :-8.000  \n 1st Qu.:1985   Class :character   1st Qu.: 16.30   1st Qu.:-2.000  \n Median :1990   Mode  :character   Median : 25.20   Median : 5.000  \n Mean   :1990                      Mean   : 31.65   Mean   : 2.905  \n 3rd Qu.:1995                      3rd Qu.: 40.80   3rd Qu.: 8.000  \n Max.   :1999                      Max.   :100.00   Max.   : 9.000  \n                                   NA's   :58       NA's   :2       \n      pop                gdp.pc           intresmi          signed      \n Min.   : 14105080   Min.   :  149.5   Min.   :0.9036   Min.   :0.0000  \n 1st Qu.: 19676715   1st Qu.:  420.1   1st Qu.:2.2231   1st Qu.:0.0000  \n Median : 52799040   Median :  814.3   Median :3.1815   Median :0.0000  \n Mean   :149904501   Mean   : 1867.3   Mean   :3.3752   Mean   :0.1548  \n 3rd Qu.:120888400   3rd Qu.: 2462.9   3rd Qu.:4.4063   3rd Qu.:0.0000  \n Max.   :997515200   Max.   :12086.2   Max.   :7.9346   Max.   :1.0000  \n                                       NA's   :13       NA's   :3       \n     fiveop          usheg       \n Min.   :12.30   Min.   :0.2558  \n 1st Qu.:12.50   1st Qu.:0.2623  \n Median :12.60   Median :0.2756  \n Mean   :12.74   Mean   :0.2764  \n 3rd Qu.:13.20   3rd Qu.:0.2887  \n Max.   :13.20   Max.   :0.3083  \n NA's   :18                      \n\n\n\nUsing Custom Function\n\n# Function to calculate missingness in a vector or data frame\nmdesc &lt;- function(df) {\n  \n  # Check if the input is a vector\n  if (is.vector(df)) {\n    # Calculate the number of missing values in the vector\n    missing_col &lt;- sum(is.na(df))\n    \n    # Get the total number of elements in the vector\n    total_col &lt;- length(df)\n    \n    # Calculate the percentage of missing values\n    perc_missing_col &lt;- missing_col / total_col\n    \n    # Create a data frame summarizing the missingness statistics for the vector\n    out &lt;- data.frame(\n      'missing' = missing_col,           # Number of missing values\n      'total' = total_col,               # Total number of elements\n      'percent_missing' = perc_missing_col # Percentage of missing values\n    )\n    \n    # Return the result\n    return(out)\n  \n  # Check if the input is a data frame\n  } else if (is.data.frame(df)) {\n    # Get the column names of the data frame (variables)\n    var_col &lt;- colnames(df)\n    \n    # Calculate the number of missing values for each column\n    missing_col &lt;- unlist(lapply(df, function(x) sum(is.na(x))))\n    \n    # Get the total number of rows in the data frame (same for all variables)\n    total_col &lt;- nrow(df)\n    \n    # Calculate the percentage of missing values for each column\n    perc_missing_col &lt;- missing_col / total_col\n    \n    # Create a data frame summarizing the missingness statistics for each variable\n    out &lt;- data.frame(\n      'variable' = var_col,              # Name of the variable (column)\n      'missing' = missing_col,           # Number of missing values in the column\n      'total' = total_col,               # Total number of rows in the data frame\n      'percent_missing' = perc_missing_col # Percentage of missing values\n    )\n    \n    # Remove row names for cleaner output\n    rownames(out) &lt;- NULL\n    \n    # Return the result\n    return(out)\n  \n  # If the input is neither a vector nor a data frame, return an error\n  } else {\n    stop('Please enter a vector or data.frame!')\n  }\n}\n\n\n# Missingness on DV\nmdesc(freetrade$tariff)\n\n  missing total percent_missing\n1      58   171       0.3391813\n\n# Missingness in whole dataset\nmdesc(freetrade)\n\n   variable missing total percent_missing\n1      year       0   171      0.00000000\n2   country       0   171      0.00000000\n3    tariff      58   171      0.33918129\n4    polity       2   171      0.01169591\n5       pop       0   171      0.00000000\n6    gdp.pc       0   171      0.00000000\n7  intresmi      13   171      0.07602339\n8    signed       3   171      0.01754386\n9    fiveop      18   171      0.10526316\n10    usheg       0   171      0.00000000\n\n\n\n\nUsing VIM package\nAggregation Plots\nAggregation plots provide a high-level overview of missingness int he dataset. Together the two plots below show missingness in the individual variables as well as joint-missingness in the data.\nThis code is using the aggr function to perform exploratory data analysis on the freetrade dataset. The numbers argument is set to TRUE, which means that the function will display the number of missing values for each variable in the dataset.\n\n# install.packages(\"VIM\")\n# library(VIM)\n\nmice_plot &lt;- aggr(freetrade, col=c('skyblue','yellow'),\n                    numbers=TRUE, sortVars=TRUE,prop = c(TRUE, FALSE),\n                    labels=names(freetrade), cex.axis=.7,interactive = FALSE,\n                    gap=3, ylab=c(\"Missing data\",\"Pattern\"))\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n Variable      Count\n   tariff 0.33918129\n   fiveop 0.10526316\n intresmi 0.07602339\n   signed 0.01754386\n   polity 0.01169591\n     year 0.00000000\n  country 0.00000000\n      pop 0.00000000\n   gdp.pc 0.00000000\n    usheg 0.00000000\n\n\nSpinogram and Spineplot\nSpine plots from the VIM packages let’s us go deeper into invetigating missingness in the data between combination of any two variables. The utilities in ggmice package (covered in next section) performs similar task with different visual tools.\n\nspineMiss(freetrade[, c(\"polity\", \"tariff\")],interactive = FALSE)\n\n\n\n\n\n\n\n\nThe relative width of the bars for different polity scores the frequency of this category in the dataset. Within each bar, the missing proportion of earnings is shown, while the shadowed bar on the right-hand side presents this proportion for the whole dataset.\nSimilarly, let’s try with variable fiveop.\n\nspineMiss(freetrade[, c(\"fiveop\", \"tariff\")],interactive = FALSE)\n\n\n\n\n\n\n\n\nThe variable fiveop is a numeric variable, The function automatically creates bins in this plot.\nParallel Boxplot\nParallel boxplots subset the data into two. One for missing values and another for observed values of the supplied variables. In these two subsets the distribution of the second variable is displayed via boxplots. It is particularly useful to understand the pattern of missingness and whether it could possibly be leading to biasing of results if we choose to perform listwise deletion.\n\npbox(freetrade[, c(\"tariff\", \"fiveop\")],interactive = FALSE)\n\nWarning in createPlot(main, sub, xlab, ylab, labels, ca$at): not enough space\nto display frequencies\n\n\n\n\n\n\n\n\n\nThe white box is the overall distribution of tariff in the complete dataset.The blue box represents distribution of tariff for observed values of fiveop and the red box for missing values of fiveop. The relative width of the boxes reflects the sizes of the subsets on which they are based: the wider blue box means there are more observed than missing values in fiveop. Other than that, the two boxes look similar to each other and also the overall white box. This suggests that missing race information has no impact on the distribution of earnings.\nWhile the blue box appears similar to overall plot, the red box shows a distribution different from them. This gives us an indication that the data is not MCAR. The missingness in fiveop is possibly systematically associated to other variables. In the world of MAR and MNAR we would need to be cautious of just dropping the rows with missingness in fiveop.\n\n\n\n\n\n\nTip\n\n\n\nMore functions from VIM graphic utilities here.\nHow to read the output\n\n\n\n\nUsing ggmicepackage\n\n## library(ggmice)\nsum(is.na(freetrade$fiveop))\n\n[1] 18\n\nggmice(freetrade, aes(gdp.pc,tariff )) + geom_point()\n\n\n\n\n\n\n\n\nThis function can again be useful in seeing patterns in missingness in the data.\nThe plot above shoes red marks on the x-axis for those values of gdp.pc for which the tariff value on y-axis is missing.\nIn this case, gdp.pc has no missing values. What if both axes have variables that have missing value. Let’s look at the plot created by VIM to look for two such variables that have jointly missing values.\nLet’s use tariff with variable fiveop.\n\nsum(is.na(freetrade$fiveop))\n\n[1] 18\n\nggmice(freetrade, aes(fiveop,tariff )) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nMore functionalities in the ggmice package",
    "crumbs": [
      "Session 3 - Missing Data and Multiple Imputation"
    ]
  },
  {
    "objectID": "missing-data.html#removal-of-missingness",
    "href": "missing-data.html#removal-of-missingness",
    "title": "Session 3 - Missing Data and Multiple Imputation",
    "section": "Removal of Missingness",
    "text": "Removal of Missingness\nMost packages perform listwise/case-wise deletion. This is the strategy of complete-case analysis. Revisit the slides from class and Lall (2017) to undertsand why this is pernicious - causes bias and reduces efficiency - in short.\n\nsummary(model.lm &lt;- lm(tariff ~ polity + pop + gdp.pc + year + country,\n          data = freetrade))\n\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country, \n    data = freetrade)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.7640  -3.2595   0.0868   2.5983  18.3097 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.973e+03  4.016e+02   4.912 3.61e-06 ***\npolity             -1.373e-01  1.821e-01  -0.754    0.453    \npop                -2.021e-07  2.542e-08  -7.951 3.23e-12 ***\ngdp.pc              6.096e-04  7.442e-04   0.819    0.415    \nyear               -8.705e-01  2.084e-01  -4.176 6.43e-05 ***\ncountryIndonesia   -1.823e+02  1.857e+01  -9.819 2.98e-16 ***\ncountryKorea       -2.204e+02  2.078e+01 -10.608  &lt; 2e-16 ***\ncountryMalaysia    -2.245e+02  2.171e+01 -10.343  &lt; 2e-16 ***\ncountryNepal       -2.163e+02  2.247e+01  -9.629 7.74e-16 ***\ncountryPakistan    -1.554e+02  1.982e+01  -7.838 5.63e-12 ***\ncountryPhilippines -2.040e+02  2.088e+01  -9.774 3.75e-16 ***\ncountrySriLanka    -2.091e+02  2.210e+01  -9.460 1.80e-15 ***\ncountryThailand    -1.961e+02  2.095e+01  -9.358 2.99e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 98 degrees of freedom\n  (60 observations deleted due to missingness)\nMultiple R-squared:  0.9247,    Adjusted R-squared:  0.9155 \nF-statistic: 100.3 on 12 and 98 DF,  p-value: &lt; 2.2e-16\n\n#library(estimatr)\nsummary(model.lm.r &lt;- lm_robust(tariff ~ polity + pop + gdp.pc,\n          data = freetrade,\n           fixed_effects = year,\n            clusters = country))\n\n\nCall:\nlm_robust(formula = tariff ~ polity + pop + gdp.pc, data = freetrade, \n    clusters = country, fixed_effects = year)\n\nStandard error type:  CR2 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)   CI Lower  CI Upper    DF\npolity  9.933e-01  5.954e-01   1.668   0.1963 -9.303e-01 2.917e+00 2.923\npop     3.075e-08  1.850e-08   1.662   0.2449 -5.304e-08 1.145e-07 1.899\ngdp.pc -2.168e-03  1.560e-03  -1.390   0.3072 -9.369e-03 5.032e-03 1.865\n\nMultiple R-squared:  0.5238 ,   Adjusted R-squared:  0.4114\nMultiple R-squared (proj. model):  0.415 ,  Adjusted R-squared (proj. model):  0.277 \nF-statistic (proj. model): 4.069 on 3 and 8 DF,  p-value: 0.04992\n\n\n\n# Let's see how many observations final models have \nnrow(model.lm$model)\n\n[1] 111\n\n(model.lm.r$nobs)\n\n[1] 111\n\n\n\nHot-Decking\nA hot-deck imputation is a method for handling missing data where missing values in a dataset are imputed by values from similar records (donors) within the same dataset. The idea is to “borrow” values from observed (non-missing) data. Two common variations are the sequential hot-deck and random hot-deck algorithms.\nIn the sequential hot-deck method, the missing value is filled with the most recent observed value from a similar case, which is often nearby in the data (usually within sorted rows or groups). In the random hot-deck method, the missing value is imputed by randomly selecting a value from a set of similar cases (donors) that have valid (non-missing) values.\nHot-deck imputation could generally introduce bias is the missing values are too frequent, akin to mean imputation but now with the same value. Sequential imputation is more prone to increased bias as compared to random imputation. The latter generally has more variability in the imputed vector. However, random imputation is computationally more complex. Also, random selection from observed value could lead to varying results depending on the random choice in each new instanc eof imputation on the same dataset.\nFor more details, read Andridge and Little (2010).\nVIM has an inbuilt function for hot-decking called hotdeck().\n\nimputed.freetrade &lt;- hotdeck(freetrade)\n\nsummary(model.lm.hotdeck &lt;- lm(tariff ~ polity + pop + gdp.pc + year + country,\n          data = imputed.freetrade))\n\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country, \n    data = imputed.freetrade)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.037  -7.889  -2.935   4.908  54.440 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)         1.462e+03  7.064e+02   2.070   0.0401 *\npolity             -1.149e-01  3.843e-01  -0.299   0.7654  \npop                -4.477e-08  4.965e-08  -0.902   0.3686  \ngdp.pc              6.664e-04  1.583e-03   0.421   0.6744  \nyear               -6.896e-01  3.672e-01  -1.878   0.0622 .\ncountryIndonesia   -5.443e+01  3.486e+01  -1.562   0.1204  \ncountryKorea       -7.787e+01  3.980e+01  -1.957   0.0521 .\ncountryMalaysia    -6.976e+01  4.106e+01  -1.699   0.0913 .\ncountryNepal       -5.939e+01  4.207e+01  -1.412   0.1600  \ncountryPakistan    -2.751e+01  3.753e+01  -0.733   0.4647  \ncountryPhilippines -6.343e+01  3.948e+01  -1.606   0.1102  \ncountrySriLanka    -5.764e+01  4.175e+01  -1.381   0.1693  \ncountryThailand    -5.842e+01  3.947e+01  -1.480   0.1409  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.54 on 158 degrees of freedom\nMultiple R-squared:  0.4284,    Adjusted R-squared:  0.3849 \nF-statistic: 9.866 on 12 and 158 DF,  p-value: 3.15e-14\n\n# Check the number of rows\nnrow(model.lm.hotdeck$model)\n\n[1] 171\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe imputation can be improved with more substantive knowledge. Read the documentation of function with ?hotdeck.\nThe paper with detailed discussion of various imoutation methods including K-Nearest Neighbor and Regression Imputation (Kowarik and Templ 2016) has detailed information of hot-decking as well as other imputation methods.\n\n\n\n\nMultiple Imputation\n\n“Lights will guide you home,\nAnd ignite your bones\nAnd I will try to fix you”\n\n\n- Honaker, King, and Blackwell (2011) responding to Chris Martin while introducing Amelia package\n\nMultiple imputation, is a statistical technique for handling missing data by creating several (multiple) different plausible datasets (imputations), analyzing each of them, and then combining the results to produce estimates that account for the uncertainty introduced by the missing data (Honaker, King, and Blackwell 2011).\nAmelia II performs multiple imputations and allows for comuting estimates across multiple imputed datasets while capturing uncertainties at both imputed value as well as multiple datasets.\nWhich Vairables to choose for Imputation Model\n\n## How many variables? Do we need to go through reduction? \ndim(freetrade)\n\n[1] 171  10\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom Amelia Webpage:\nWhen performing multiple imputation, the first step is to identify the variables to include in the imputation model. It is crucial to include at least as much information as will be used in the analysis model. That is, any variable that will be in the analysis model should also be in the imputation model. This includes any transformations or interactions of variables that will appear in the analysis model.\nIn fact, it is often useful to add more information to the imputation model than will be present when the analysis is run. Since imputation is predictive, any variables that would increase predictive power should be included in the model, even if including them in the analysis model would produce bias in estimating a causal effect (such as for post-treatment variables) or collinearity would preclude determining which variable had a relationship with the dependent variable (such as including multiple alternate measures of GDP). In our case, we include all the variables in freetrade in the imputation model, even though our analysis model focuses on polity, pop and gdp.pc.\n\n\nHow many imputations?\nAmelia II creators recommend that unless the rate of missingness is very high in the dataset, m = 5 where m represents number of imputed datsets should be enough (Honaker, King, and Blackwell 2011, pp2). However, in practical applications Lall (2017) suggets to be more cautious of this approach as it could produce some divergence in the results (pp. 426) and suggest calculating \\(\\gamma\\) - the average missignness rate in the dataset - and then chose an \\(m\\) roughly equal to this rate.\n\n## What is average percentage of missing data?\n#m is equal to the average missing-data rate of all variables in the imputation model\nNAs &lt;- function(x) {\n    as.vector(apply(x, 2, function(x) length(which(is.na(x)))))\n    }\nNAs(freetrade)\n\n [1]  0  0 58  2  0  0 13  3 18  0\n\nmean(NAs(freetrade)/nrow(freetrade))*100\n\n[1] 5.497076\n\n\nSo, in this case we can safely take \\(m=5\\).\nCreating Imputed Datasets\nTo create multiple imputations in Amelia, we can simply run\n\nset.seed(20230918)\na.out &lt;- amelia(freetrade,\n                m = 5,\n                ts = \"year\",\n                cs = \"country\")\n\n-- Imputation 1 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12\n\n-- Imputation 2 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16\n\n-- Imputation 3 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16\n\n-- Imputation 4 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n-- Imputation 5 --\n\n  1  2  3  4  5  6  7  8  9 10\n\na.out\n\n\nAmelia output with 5 imputed datasets.\nReturn code:  1 \nMessage:  Normal EM convergence. \n\nChain Lengths:\n--------------\nImputation 1:  12\nImputation 2:  16\nImputation 3:  16\nImputation 4:  15\nImputation 5:  10\n\n\nThe output gives some information about how the algorithm ran. Each of the imputed datasets is now in the list a.out$imputations.\na.out is a list object with 5 imputed datsets stored in it. We can access any of those imputations by using the synatx a.out$imputations[[&lt;number&gt;]] or a.out$imputations$imp&lt;number&gt;.\n\n# See the number of rows\nnrow(a.out$imputations$imp1)\n\n[1] 171\n\nnrow(a.out$imputations[[2]])\n\n[1] 171\n\n\nWe can access variables in different imputations to coompare the distribution with the original variable’s distribution.\nLet’s plot a histogram of the tariff variable from the 3rd imputation,\n\nOriginalImputed \\(3^{rd}\\) DatasetCombined\n\n\n\n\nCode\nggplot(freetrade, aes(tariff)) +geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 58 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(a.out$imputations[[3]], aes(tariff)) +geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Compare with original data\ncomb.tariff &lt;- rbind.data.frame(\n  cbind(tariff = a.out$imputations[[3]]$tariff, df = \"impute.3\"),\n  cbind(tariff = freetrade$tariff, df = \"original\")) %&gt;% \n  mutate( tariff = as.numeric(tariff))\n\nggplot(comb.tariff, aes(tariff, fill = df)) +geom_histogram(position = 'identity', alpha = 0.6, color = \"black\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 58 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Why are red bars higher than green bars in the comibined plot?\n\n\n\nSaving the imputed datasets\n```{r}\n\nsave(a.out, file = \"imputations.RData\") # .RData file\nwrite.amelia(obj = a.out, file.stem = \"imputation-freetrade\") # As five separate csvs\n```\nExpansive Function\n\n# More expansive model\n# Amelia model\n\n# as2012.out &lt;- amelia(freetrade, # data \n#                      m = 5, # number of imputations \n#                      ts = \"year\",  # time series index\n#                      cs = \"country\", # cross section index\n#                      polytime = 3, # third-order time polynomial\n#                      lags = c(\"polity\", \"gdp.pc\"), # lag variable\n#                      ords =  \"polity\" # Ordinal Variable\n#                      noms =  \"polity\" # If polity was a nominal variable\n#                      empri = 0.01*nrow(as)) # priors\n# Not running, just for explanation\n\nAlso, read sections on transformations,bounds, time-series-cross-section data (TSCS) and id-variables from the Amelia Website\n\n\nAnalysis with Imputed Datasets\n\n# Recall the Linear Models we ran above\nsummary(model.lm)\n\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country, \n    data = freetrade)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.7640  -3.2595   0.0868   2.5983  18.3097 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.973e+03  4.016e+02   4.912 3.61e-06 ***\npolity             -1.373e-01  1.821e-01  -0.754    0.453    \npop                -2.021e-07  2.542e-08  -7.951 3.23e-12 ***\ngdp.pc              6.096e-04  7.442e-04   0.819    0.415    \nyear               -8.705e-01  2.084e-01  -4.176 6.43e-05 ***\ncountryIndonesia   -1.823e+02  1.857e+01  -9.819 2.98e-16 ***\ncountryKorea       -2.204e+02  2.078e+01 -10.608  &lt; 2e-16 ***\ncountryMalaysia    -2.245e+02  2.171e+01 -10.343  &lt; 2e-16 ***\ncountryNepal       -2.163e+02  2.247e+01  -9.629 7.74e-16 ***\ncountryPakistan    -1.554e+02  1.982e+01  -7.838 5.63e-12 ***\ncountryPhilippines -2.040e+02  2.088e+01  -9.774 3.75e-16 ***\ncountrySriLanka    -2.091e+02  2.210e+01  -9.460 1.80e-15 ***\ncountryThailand    -1.961e+02  2.095e+01  -9.358 2.99e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 98 degrees of freedom\n  (60 observations deleted due to missingness)\nMultiple R-squared:  0.9247,    Adjusted R-squared:  0.9155 \nF-statistic: 100.3 on 12 and 98 DF,  p-value: &lt; 2.2e-16\n\nsummary(model.lm.r)\n\n\nCall:\nlm_robust(formula = tariff ~ polity + pop + gdp.pc, data = freetrade, \n    clusters = country, fixed_effects = year)\n\nStandard error type:  CR2 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)   CI Lower  CI Upper    DF\npolity  9.933e-01  5.954e-01   1.668   0.1963 -9.303e-01 2.917e+00 2.923\npop     3.075e-08  1.850e-08   1.662   0.2449 -5.304e-08 1.145e-07 1.899\ngdp.pc -2.168e-03  1.560e-03  -1.390   0.3072 -9.369e-03 5.032e-03 1.865\n\nMultiple R-squared:  0.5238 ,   Adjusted R-squared:  0.4114\nMultiple R-squared (proj. model):  0.415 ,  Adjusted R-squared (proj. model):  0.277 \nF-statistic (proj. model): 4.069 on 3 and 8 DF,  p-value: 0.04992\n\n# Now running with imputed datasets\nimp.models &lt;- with( # Note this command\n  a.out,\n  lm(tariff ~ polity + pop + gdp.pc + year + country)\n)\n\n# Analysis results with five imputed datsets\nimp.models[1:5]\n\n[[1]]\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country)\n\nCoefficients:\n       (Intercept)              polity                 pop              gdp.pc  \n         3.402e+03           1.314e-01          -1.253e-07           5.026e-04  \n              year    countryIndonesia        countryKorea     countryMalaysia  \n        -1.624e+00          -1.138e+02          -1.522e+02          -1.517e+02  \n      countryNepal     countryPakistan  countryPhilippines     countrySriLanka  \n        -1.384e+02          -9.973e+01          -1.398e+02          -1.370e+02  \n   countryThailand  \n        -1.311e+02  \n\n\n[[2]]\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country)\n\nCoefficients:\n       (Intercept)              polity                 pop              gdp.pc  \n         3.080e+03           6.137e-02          -4.630e-08          -2.889e-04  \n              year    countryIndonesia        countryKorea     countryMalaysia  \n        -1.500e+00          -5.777e+01          -7.636e+01          -7.796e+01  \n      countryNepal     countryPakistan  countryPhilippines     countrySriLanka  \n        -6.808e+01          -3.119e+01          -6.796e+01          -6.195e+01  \n   countryThailand  \n        -6.287e+01  \n\n\n[[3]]\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country)\n\nCoefficients:\n       (Intercept)              polity                 pop              gdp.pc  \n         2.769e+03           1.266e-01          -1.298e-07          -6.143e-04  \n              year    countryIndonesia        countryKorea     countryMalaysia  \n        -1.304e+00          -1.204e+02          -1.455e+02          -1.502e+02  \n      countryNepal     countryPakistan  countryPhilippines     countrySriLanka  \n        -1.459e+02          -1.010e+02          -1.419e+02          -1.353e+02  \n   countryThailand  \n        -1.341e+02  \n\n\n[[4]]\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country)\n\nCoefficients:\n       (Intercept)              polity                 pop              gdp.pc  \n         2.517e+03           6.279e-02          -1.128e-07           3.814e-04  \n              year    countryIndonesia        countryKorea     countryMalaysia  \n        -1.186e+00          -1.081e+02          -1.403e+02          -1.358e+02  \n      countryNepal     countryPakistan  countryPhilippines     countrySriLanka  \n        -1.304e+02          -8.434e+01          -1.260e+02          -1.245e+02  \n   countryThailand  \n        -1.192e+02  \n\n\n[[5]]\n\nCall:\nlm(formula = tariff ~ polity + pop + gdp.pc + year + country)\n\nCoefficients:\n       (Intercept)              polity                 pop              gdp.pc  \n         2.498e+03           9.533e-02          -2.128e-08          -4.360e-04  \n              year    countryIndonesia        countryKorea     countryMalaysia  \n        -1.219e+00          -3.874e+01          -5.313e+01          -5.325e+01  \n      countryNepal     countryPakistan  countryPhilippines     countrySriLanka  \n        -5.009e+01          -1.167e+01          -4.665e+01          -3.946e+01  \n   countryThailand  \n        -3.571e+01  \n\n# Similarly, lm_robust can be run with approprite modifications\n\n# BTW, see the class of created object\nclass(imp.models)\n\n[1] \"amest\"\n\n\nWe can combine the imputed estimates using Rubin Rules (Lall 2017; Honaker, King, and Blackwell 2011).\n\nout &lt;- mi.combine(imp.models, conf.int = TRUE)\n\nout\n\n# A tibble: 13 × 10\n   term    estimate std.error statistic p.value     df      r miss.info conf.low\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Inter…  2.85e+3   6.87e+2    4.15   2.86e-4 2.76e1 0.615     0.421   4.26e+3\n 2 polity   9.55e-2   2.98e-1    0.321  7.48e-1 1.73e4 0.0155    0.0153  6.79e-1\n 3 pop     -8.71e-8   6.65e-8   -1.31   1.78e+0 8.78e0 2.08      0.730   6.38e-8\n 4 gdp.pc  -9.10e-5   1.33e-3   -0.0685 1.05e+0 1.37e2 0.206     0.183   2.54e-3\n 5 year    -1.37e+0   3.49e-1   -3.92   2.00e+0 3.25e1 0.541     0.387  -6.57e-1\n 6 countr… -8.78e+1   4.84e+1   -1.81   1.89e+0 8.21e0 2.31      0.752   2.35e+1\n 7 countr… -1.13e+2   5.84e+1   -1.94   1.91e+0 7.56e0 2.67      0.779   2.25e+1\n 8 countr… -1.14e+2   5.87e+1   -1.94   1.91e+0 7.84e0 2.50      0.767   2.20e+1\n 9 countr… -1.07e+2   5.81e+1   -1.84   1.90e+0 8.30e0 2.27      0.748   2.65e+1\n10 countr… -6.56e+1   5.37e+1   -1.22   1.74e+0 7.83e0 2.51      0.768   5.86e+1\n11 countr… -1.04e+2   5.70e+1   -1.83   1.89e+0 7.72e0 2.57      0.772   2.78e+1\n12 countr… -9.97e+1   5.93e+1   -1.68   1.87e+0 7.91e0 2.46      0.764   3.73e+1\n13 countr… -9.66e+1   5.74e+1   -1.68   1.87e+0 7.62e0 2.63      0.776   3.69e+1\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n# Compare with models with missing data\ntidy(model.lm)\n\n# A tibble: 13 × 5\n   term               estimate std.error statistic  p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)         1.97e+3   4.02e+2     4.91  3.61e- 6\n 2 polity             -1.37e-1   1.82e-1    -0.754 4.53e- 1\n 3 pop                -2.02e-7   2.54e-8    -7.95  3.23e-12\n 4 gdp.pc              6.10e-4   7.44e-4     0.819 4.15e- 1\n 5 year               -8.71e-1   2.08e-1    -4.18  6.43e- 5\n 6 countryIndonesia   -1.82e+2   1.86e+1    -9.82  2.98e-16\n 7 countryKorea       -2.20e+2   2.08e+1   -10.6   5.82e-18\n 8 countryMalaysia    -2.25e+2   2.17e+1   -10.3   2.19e-17\n 9 countryNepal       -2.16e+2   2.25e+1    -9.63  7.74e-16\n10 countryPakistan    -1.55e+2   1.98e+1    -7.84  5.63e-12\n11 countryPhilippines -2.04e+2   2.09e+1    -9.77  3.75e-16\n12 countrySriLanka    -2.09e+2   2.21e+1    -9.46  1.80e-15\n13 countryThailand    -1.96e+2   2.10e+1    -9.36  2.99e-15\n\ntidy(model.lm.r)\n\n    term      estimate    std.error statistic   p.value      conf.low\n1 polity  9.932974e-01 5.954310e-01  1.668199 0.1962662 -9.302831e-01\n2    pop  3.074611e-08 1.849543e-08  1.662363 0.2448695 -5.304187e-08\n3 gdp.pc -2.168425e-03 1.559823e-03 -1.390174 0.3071929 -9.368722e-03\n     conf.high       df outcome\n1 2.916878e+00 2.922744  tariff\n2 1.145341e-07 1.898740  tariff\n3 5.031871e-03 1.864708  tariff\n\ntidy(model.lm.hotdeck)\n\n# A tibble: 13 × 5\n   term               estimate std.error statistic p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)         1.46e+3   7.06e+2     2.07   0.0401\n 2 polity             -1.15e-1   3.84e-1    -0.299  0.765 \n 3 pop                -4.48e-8   4.97e-8    -0.902  0.369 \n 4 gdp.pc              6.66e-4   1.58e-3     0.421  0.674 \n 5 year               -6.90e-1   3.67e-1    -1.88   0.0622\n 6 countryIndonesia   -5.44e+1   3.49e+1    -1.56   0.120 \n 7 countryKorea       -7.79e+1   3.98e+1    -1.96   0.0521\n 8 countryMalaysia    -6.98e+1   4.11e+1    -1.70   0.0913\n 9 countryNepal       -5.94e+1   4.21e+1    -1.41   0.160 \n10 countryPakistan    -2.75e+1   3.75e+1    -0.733  0.465 \n11 countryPhilippines -6.34e+1   3.95e+1    -1.61   0.110 \n12 countrySriLanka    -5.76e+1   4.17e+1    -1.38   0.169 \n13 countryThailand    -5.84e+1   3.95e+1    -1.48   0.141 \n\n\n\n# Install required packages if you haven't already\n# install.packages(c(\"dplyr\", \"kableExtra\", \"tidyr\"))\n\nlibrary(broom)\n\n# Assuming tidy outputs from the three models\ntidy_lm &lt;- tidy(model.lm)\ntidy_lm_hotdeck &lt;- tidy(model.lm.hotdeck)\ntidy_amelia &lt;- out\n\n# Combine the tidy outputs into one data frame with model names\ncombined_tidy &lt;- bind_rows(\n  tidy_lm %&gt;% mutate(Model = \"Complete-Case Model\"),\n  tidy_lm_hotdeck %&gt;% mutate(Model = \"Hot-Deck Model\"),\n  tidy_amelia %&gt;% mutate(Model = \"Amelia MI Model\")\n)\n\n# Reshape the data to wide format\nwide_tidy &lt;- combined_tidy %&gt;%\n  select(Model, term, estimate, std.error, statistic, p.value) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(estimate, std.error, statistic, p.value),\n              names_sep = \"_\") # This will create column names like estimate_Complete-Case Model\n\n# Create LaTeX table using kableExtra\nwide_tidy %&gt;%\n  kbl( booktabs = TRUE, caption = \"Combined Regression Models (Transposed)\") %&gt;%\n  kable_styling(latex_options = c(\"hold_position\", \"striped\"))\n\n\nCombined Regression Models (Transposed)\n\n\nterm\nestimate_Complete-Case Model\nestimate_Hot-Deck Model\nestimate_Amelia MI Model\nstd.error_Complete-Case Model\nstd.error_Hot-Deck Model\nstd.error_Amelia MI Model\nstatistic_Complete-Case Model\nstatistic_Hot-Deck Model\nstatistic_Amelia MI Model\np.value_Complete-Case Model\np.value_Hot-Deck Model\np.value_Amelia MI Model\n\n\n\n\n(Intercept)\n1972.6328639\n1462.1977381\n2853.2141618\n401.6146966\n706.4218427\n687.1615407\n4.9117547\n2.0698648\n4.1521738\n0.0000036\n0.0400927\n0.0002857\n\n\npolity\n-0.1372722\n-0.1148692\n0.0955053\n0.1821269\n0.3842814\n0.2975730\n-0.7537175\n-0.2989195\n0.3209475\n0.4528260\n0.7653942\n0.7482541\n\n\npop\n-0.0000002\n0.0000000\n-0.0000001\n0.0000000\n0.0000000\n0.0000001\n-7.9508396\n-0.9015938\n-1.3103516\n0.0000000\n0.3686450\n1.7766806\n\n\ngdp.pc\n0.0006096\n0.0006664\n-0.0000910\n0.0007442\n0.0015831\n0.0013298\n0.8191485\n0.4209571\n-0.0684692\n0.4146893\n0.6743585\n1.0544879\n\n\nyear\n-0.8705461\n-0.6896356\n-1.3667123\n0.2084439\n0.3671533\n0.3487968\n-4.1764050\n-1.8783313\n-3.9183625\n0.0000643\n0.0621782\n1.9995679\n\n\ncountryIndonesia\n-182.3267131\n-54.4297016\n-87.7626156\n18.5678981\n34.8557982\n48.4445877\n-9.8194590\n-1.5615681\n-1.8116083\n0.0000000\n0.1203905\n1.8933361\n\n\ncountryKorea\n-220.4438668\n-77.8728598\n-113.4919900\n20.7803472\n39.7959606\n58.3744290\n-10.6082860\n-1.9568031\n-1.9442073\n0.0000000\n0.0521333\n1.9101103\n\n\ncountryMalaysia\n-224.5464768\n-69.7642156\n-113.7811609\n21.7107818\n41.0574759\n58.6693456\n-10.3426251\n-1.6991842\n-1.9393630\n0.0000000\n0.0912525\n1.9108244\n\n\ncountryNepal\n-216.3338310\n-59.3897376\n-106.5699358\n22.4672149\n42.0660816\n58.0575005\n-9.6288673\n-1.4118201\n-1.8355929\n0.0000000\n0.1599693\n1.8976104\n\n\ncountryPakistan\n-155.3509868\n-27.5125863\n-65.5886664\n19.8214313\n37.5349785\n53.6629118\n-7.8375262\n-0.7329853\n-1.2222346\n0.0000000\n0.4646535\n1.7428515\n\n\ncountryPhilippines\n-204.0378373\n-63.4309827\n-104.4628202\n20.8758293\n39.4849492\n56.9810714\n-9.7738794\n-1.6064598\n-1.8332899\n0.0000000\n0.1101696\n1.8945536\n\n\ncountrySriLanka\n-209.0534653\n-57.6395202\n-99.6524394\n22.0994094\n41.7481912\n59.2852871\n-9.4596856\n-1.3806471\n-1.6808966\n0.0000000\n0.1693375\n1.8682816\n\n\ncountryThailand\n-196.0930793\n-58.4150854\n-96.6075914\n20.9539028\n39.4691702\n57.3985862\n-9.3583082\n-1.4800181\n-1.6831005\n0.0000000\n0.1408596\n1.8672833\n\n\n\n\n\n\n\n\n# Filter out the intercept, as it's typically not included in coefficient plots\ncombined_tidy_filtered &lt;- combined_tidy %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  filter(!str_detect(term,\"^country\"))\n\n# Create the coefficient plot\nggplot(combined_tidy_filtered, aes(x = term, y = estimate, color = Model)) +\n  geom_point(position = position_dodge(width = 0.5)) +  # Points for estimates\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), \n                width = 0.2, position = position_dodge(width = 0.5)) +  # Error bars for std errors\n  labs(title = \"Coefficient Plot of Combined Models\",\n       x = \"Term\",\n       y = \"Estimate\",\n       color = \"Model\") +\n  theme_minimal() +\n  coord_flip()  # Flip coordinates for better readability (optional)\n\n\n\n\n\n\n\n\n\n# Just country fixed effects\ncombined_tidy_filtered_fe &lt;- combined_tidy %&gt;% \n  filter(str_detect(term,\"^country\"))\n\n# Create the coefficient plot\nggplot(combined_tidy_filtered_fe, aes(x = term, y = estimate, color = Model)) +\n  geom_point(position = position_dodge(width = 0.5)) +  # Points for estimates\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), \n                width = 0.2, position = position_dodge(width = 0.5)) +  # Error bars for std errors\n  labs(title = \"Coefficient Plot of Combined Models\",\n       x = \"Term\",\n       y = \"Estimate\",\n       color = \"Model\") +\n  theme_minimal() +\n  coord_flip()  # Flip coordinates for better readability (optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia\n\n\n\nAny idea why the name is AMELIA?\n\n\nAdditional Resources\n\nUnderstanding Assumptions - Little (2021)\nUnderstanding Analysis - Little (2024)\nExpectation Maximization Algorithm - King et al. (2001)\nMore tools to describe patterns of missingness",
    "crumbs": [
      "Session 3 - Missing Data and Multiple Imputation"
    ]
  },
  {
    "objectID": "missing-data.html#extra-stuff",
    "href": "missing-data.html#extra-stuff",
    "title": "Session 3 - Missing Data and Multiple Imputation",
    "section": "Extra Stuff",
    "text": "Extra Stuff\nNote on Expectation Utilization Algorithm (King et al. 2001) which is employed in Amelia II.\nEM Algorithm for Multiple Imputation\nThe Expectation-Maximization (EM) algorithm is a method for finding the maximum likelihood estimate of parameters in a statistical model when there is missing data. It’s an iterative process that involves two steps:\n\nExpectation Step (E-step): In this step, the expected value of the complete-data log-likelihood function is calculated, conditional on the observed data and the current parameter estimates. This step imputes the missing data using predicted values based on the current parameter estimates.\nMaximization Step (M-step): This step involves maximizing the expected complete-data log-likelihood function with respect to the parameters. The updated parameter estimates are then used in the next E-step.\n\nIn the context of multiple imputation, the EM algorithm is used to estimate the parameters of the multivariate normal distribution assumed to underlie the data. These estimated parameters are then used to generate imputations for the missing data.\nThe EM algorithm has several advantages, including:\n\nSpeed: EM is faster than other multiple imputation algorithms like the Imputation-Posterior (IP) algorithm.\nDeterministic Convergence: EM converges deterministically, meaning that the objective function increases with every iteration and it avoids the convergence issues associated with stochastic algorithms like IP.\n\nHowever, EM also has some drawbacks:\n\nLimited Output: EM only yields maximum posterior estimates of the parameters, rather than the entire posterior distribution.\nPotential Bias: Using EM for multiple imputation can lead to biased estimates and downwardly biased standard errors because it ignores estimation uncertainty in the parameters.\n\nThe modifications to original EM algorithm include:\n\nEMs (EM with sampling): This method uses the asymptotic approximation of the posterior distribution of the parameters to generate multiple imputations. It is fast and produces independent imputations but can be inaccurate in small samples or with a large number of variables.\nEMis (EM with importance resampling): This method uses importance resampling to improve the small sample performance of EMs by approximating the true (finite sample) posterior distribution of the parameters. EMis retains the advantages of IP and addresses the limitations of EM and EMs.\n\n\n\n\n\nAndridge, Rebecca R., and Roderick J. A. Little. 2010. “A Review of Hot Deck Imputation for Survey Non-Response.” International Statistical Review = Revue Internationale de Statistique 78 (1): 40. https://doi.org/10.1111/j.1751-5823.2010.00103.x.\n\n\nHonaker, James, Gary King, and Matthew Blackwell. 2011. “Amelia II: A Program for Missing Data.” Journal of Statistical Software 45 (7). https://doi.org/10.18637/jss.v045.i07.\n\n\nKing, Gary, James Honaker, Anne Joseph, and Kenneth Scheve. 2001. “Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation.” American Political Science Review 95 (1): 49–69. https://doi.org/10.1017/S0003055401000235.\n\n\nKowarik, Alexander, and Matthias Templ. 2016. “Imputation with the R Package VIM.” Journal of Statistical Software 74 (7). https://doi.org/10.18637/jss.v074.i07.\n\n\nLall, Ranjit. 2017. “How Multiple Imputation Makes a Difference.” Political Analysis 24 (4): 414–33. https://doi.org/10.1093/pan/mpw020.\n\n\nLittle, Roderick J. 2021. “Missing Data Assumptions.” Annual Review of Statistics and Its Application 8 (Volume 8, 2021): 89–107. https://doi.org/10.1146/annurev-statistics-040720-031104.\n\n\n———. 2024. “Missing Data Analysis.” Annual Review of Clinical Psychology 20 (1): 149–73. https://doi.org/10.1146/annurev-clinpsy-080822-051727.",
    "crumbs": [
      "Session 3 - Missing Data and Multiple Imputation"
    ]
  },
  {
    "objectID": "sensitivity-mediation.html",
    "href": "sensitivity-mediation.html",
    "title": "Session 4 - Sensitivity and Mediation",
    "section": "",
    "text": "Today’s Lab\nThe links above redirect to online vignettes or paper that have the expanded codes and writeup used in today’s lab.",
    "crumbs": [
      "Session 4 - Sensitivity and Mediation"
    ]
  },
  {
    "objectID": "sensitivity-mediation.html#todays-lab",
    "href": "sensitivity-mediation.html#todays-lab",
    "title": "Session 4 - Sensitivity and Mediation",
    "section": "",
    "text": "Sensitivity Analysis using sensemaker\nMediation Analysis using mediation",
    "crumbs": [
      "Session 4 - Sensitivity and Mediation"
    ]
  },
  {
    "objectID": "sensitivity-mediation.html#sensitivity-analysis",
    "href": "sensitivity-mediation.html#sensitivity-analysis",
    "title": "Session 4 - Sensitivity and Mediation",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nA technique for assessing the robustness of research findings to the potential influence of unobserved confounders.\n\nUnderstanding from an Omitted Variable Bias Approach1\n\n\n\n\n\n\nIdea: How strong unobserved factor/s have to be to question our research conclusions (or at least provoke a rethinking)?\nOr Put Differently:\nHow far along our results would hold before they are threatened by possible unobserved factors?\nBut..\nUnobserved are well unobserved, so how do we quantify them?\nAns: In relative terms. That is, as some number of times of the observed controls.\n\n\n\nAs discussed in class, this has clear resemblance to discussion on Omitted Variable Bias. For a thorough conceptual reading, refer to Cinelli, Ferwerda, and Hazlett (n.d.) and Cinelli and Hazlett (2020).\n\n\nCode\n# Load required packages\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create the updated DAG using dagitty\ndag &lt;- dagitty(\"dag {\n  D -&gt; Y\n  Z -&gt; D\n  Z -&gt; Y\n  X -&gt; D\n  X -&gt; Y\n}\")\n\n# Set layout coordinates for nodes\ncoordinates(dag) &lt;- list(\n  x = c(D = 1, Y = 3, Z = 2, X = 2),\n  y = c(D = 2, Y = 2, Z = 3, X = 1)\n)\n\n# Convert the DAG to ggdag format\ntidy_dag &lt;- tidy_dagitty(dag)\n\n# Add node type and colors\ntidy_dag &lt;- tidy_dag %&gt;%\n  mutate(node_type = case_when(\n    name == \"Z\" ~ \"Unobserved\",\n    TRUE ~ \"Observed\"\n  ),\n  node_color = case_when(\n    name == \"Z\" ~ \"red\",       # Unobserved\n    name == \"X\" ~ \"green\",     # Observed confounder\n    TRUE ~ \"#3498db\"           # Other observed variables\n  ))\n\n# Plot the DAG with colors and a legend\nggdag(tidy_dag, text = TRUE) +\n  geom_dag_edges(size = 1.2) +\n  geom_dag_node(aes(fill = node_color), color = \"black\", size = 12, shape = 21) +\n  geom_dag_text(color = \"white\", size = 5, fontface = \"bold\") +\n  theme_dag() +\n  labs(\n    subtitle = \"Highlighting Observed (Blue & Green) and Unobserved (Red) Variables\",\n    fill = \"Node Type\"\n  ) +\n  scale_fill_identity() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nIf this is the Data Generating Mechanism or the true state of the world,\n\\[\nY = \\hat\\tau D +\\boldsymbol{X}\\hat\\beta + \\hat\\gamma Z + \\epsilon ....(1)\n\\] where Y is an n × 1 vector containing the outcome of interest for each of the n observations and D is an n × 1 treatment variable (which may be continuous or binary); X is an n × p matrix of observed (pretreatment) covariates including the constant; and Z is a single n × 1 unobserved covariate. However, since Z is unobserved, we are forced instead to estimate a restricted model:\n\\[\nY = \\hat\\tau_{res} D +\\boldsymbol{X}\\hat\\beta_{res} + \\hat\\epsilon_{res}\n\\] where \\(\\tau_{res}\\) and \\(\\beta_{res}\\) are the coefficient estimates of the restricted ordinary least squares with only D and X, omitting Z, and \\(\\epsilon_{res}\\) its corresponding residual.\nRecall from Govt 8001 and Govt 80022 :\n\\[\n\\hat\\tau_{res} = \\hat\\tau + \\hat\\gamma\\hat\\delta\n\\] where \\(\\hat\\delta\\) is the coefficient on observed and included covariate \\(X\\) in auxillary regression.\nThus, bias is.\n\\[\n\\hat{Bias} = \\hat\\gamma\\hat\\delta\n\\] This recall of OVB provides intuition for sensitivity analysis. Omission of \\(Z\\) affects determination of causality i.e. equation (1) has causal meaning only when \\(Z\\) is included. As we have gone through lectures, if \\(Z\\) is a confoudner, the supposedly causal effect of D is biased, unless it is included.\nHowever, as true in observational studies, while theoretically \\(Z\\) si needed, in any instance of data collection it might be unobserved. Yet this does not preclude it from true scientific inquiry and consequentially, nor from peer/reviewer comments.\nSensitivity analysis uses the idea of qunatifying the magnitude of bias in terms of statistics obtained in the restricted model for observed covrariates (\\(X\\) in above model). In case of a single binary \\(X\\) it could be said that a possible unobserved confounder \\(Z\\), if has an effect size of \\(k\\) times of \\(\\hat\\beta_{res}\\), would soak away all the effect. However, in case of multiple \\(Xs\\) and an unknown number of \\(Zs\\), both of which could be continuous, the OVB characterization of effect size comparisions could fail to provide interpretatble answers.\nCinelli and Hazlett (2020) reparameterize the OVB formula, to partial \\(R^2\\) values and other derivative sensitivity statistics, instead of of observed effect sizes in restricted model.\n\n\n\n\n\n\nSensitivity Statistics.\n\n\n\nPartial \\(R^2\\) The proportion of variation in a dependent variable explained by an independent variable after controlling for other independent variables.\nRobustness Value (RV) The minimum strength of association, measured in partial R2, that an unobserved confounder would need to have with both the treatment and the outcome to change the research conclusions. In essence, it tells you how strong the unobserved confounder needs to be to nullify the observed effect.\nBounding: A method for limiting the plausible range of values for the strength of association between an unobserved confounder and the treatment or outcome\n\n\n\n\nHow to perform sensitivity analysis\nWe will use Hazlett (2020) as an illustrative example. The code and text below is adapted from the sensemakr vignette.\n\n\n\n\n\n\nPaper:\n\n\n\nHazlett, C. (2020). Angry or Weary? How Violence Impacts Attitudes toward Peace among Darfurian Refugees. The Journal of Conflict Resolution, 64(5), 844–870. https://www.jstor.org/stable/48596931\n\nAbstract:\nDoes exposure to violence motivate individuals to support further violence or to seek peace? Such questions are central to our understanding of how conflicts evolve, terminate, and recur. Yet, convincing empirical evidence as to which response dominates—even in a specific case—has been elusive, owing to the inability to rule out confounding biases. This article employs a natural experiment based on the indiscriminacy of violence within villages in Darfur to examine how refugees’ experiences of violence affect their attitudes toward peace. The results are consistent with a pro-peace or “weary” response: individuals directly harmed by violence were more likely to report that peace is possible and less likely to demand execution of their enemies. This provides microlevel evidence supporting earlier country-level work on “war-weariness” and extends the growing literature on the effects of violence on individuals by including attitudes toward peace as an important outcome. These findings suggest that victims harmed by violence during war can play a positive role in settlement and reconciliation processes.\n\n\n\nResearch Question\nWhether, on average, being directly injured or maimed in this episode made individuals more likely to feel “vengeful” and unwilling to make peace with those who perpetrated this violence. Or, were those who directly suffered from such violence most motivated to see it end by making peace?\nModel Specification\n\\[\n\\begin{aligned}\npeacefactor = \\beta_0 & + \\beta_1  directlyharmed\n+ \\beta_2  female + \\beta_3  village \\\\\n& + \\beta_4  age + \\beta_5  farmerdar + \\beta_6  herderdar \\\\\n& + \\beta_7  pastvoted + \\beta_8  hhsizedarfur + \\epsilon\n\\end{aligned}\n\\] where, \\(peacefactor\\) is the outcome, \\(directlyharmed\\) is the treatment of interest, and rest are observed controls.\n\n#install.packages(\"sensemakr\")\nlibrary(sensemakr)\n# load data from packages\ndata(\"darfur\")\n\n# runs regression model\ndarfur.model &lt;- lm(peacefactor ~ directlyharmed  + # DV and Treatment\n                      female + village + # Confounders Observed\n                     age + farmer_dar + herder_dar + pastvoted + \n                     hhsize_darfur, # Other Controls\n                   data = darfur)\n\n#summary(darfur.model) # Shows whole result with Village FE\ndarfur.model$coefficients[1:3] # Just the intercept, treatment, one observed covariate\n\n   (Intercept) directlyharmed         female \n    1.08189175     0.09731582    -0.23205144 \n\n\nTo make causal claim, we have to assume that above model completely identifies the process. That is, there is no unobserved confounding causing any bias. But as most cases that is a big sell, particularly in observational studies.\nThree possible confounders unobserved are location of bombing (center), wealth of individuals (wealth) and political ideology (political_attitudes).\n\n\n\n\n\n\nThink\n\n\n\nHow are they possible confounders?\n\n\nAdditonally, all these factors could interact with each other or otherwise have non-linear effects.\nSo, in an ideal world, we would want to run the following model:\n\\[\n\\begin{aligned}\npeacefactor = \\beta_0 & + \\beta_1 directlyharmed + \\beta_2 village + \\beta_3 female \\\\\n& + \\beta_4 age + \\beta_5 farmerDar + \\beta_6 herderDar + \\beta_7 pastvoted \\\\\n& + \\beta_8 hhsizeDarfur + \\beta_9 center + \\beta_{10} wealth + \\beta_{11} politicalAttitudes \\\\\n& + \\beta_{12} (center * wealth) + \\beta_{13} (center * politicalAttitudes) \\\\\n& + \\beta_{14} (wealth * politicalAttitudes) + \\beta_{15} (center * wealth * politicalAttitudes) + \\epsilon\n\\end{aligned}\n\\]\nEquivalent R code:\n\ndarfur.complete.model &lt;- lm(peacefactor ~ directlyharmed  + village +  \n                              female + age + farmer_dar + herder_dar + \n                              pastvoted +hhsize_darfur +\n                              center*wealth*political_attitudes, \n                            data = darfur)\n\n# Just BTW, why is eval = FALSE in the code chunk header?\n\nBut we will get an error, as those measures are not there in data - because they were never measured.\nOmitted Variable Bias is lurking!\nHowever, recalling the idea from start of the class, we can estimate how much these or all unobserved confounders can affect our result/conclusions.\n“Or, more precisely, given an assumption on how strongly these and other omitted variables relate to the treatment and the outcome, how would including them have changed our inferences regarding the coefficient of directlyharmed?” - Cinelli and Hazlett (2020)\n\n\nUsing sensemakr package\nThe following text is from online vignette of sensemaker package as they explain it in very clear terms.\n\n# runs sensemakr for sensitivity analysis\n# in the darfur example\ndarfur.sensitivity &lt;- sensemakr(model = darfur.model, \n                                treatment = \"directlyharmed\",\n                                benchmark_covariates = \"female\",\n                                kd = 1:3,\n                                ky = 1:3, \n                                q = 1,\n                                alpha = 0.05, \n                                reduce = TRUE)\n\n\n\n\n\n\n\nThe arguments are:\n\nmodel: the lm object with the outcome regression. In our case, darfur.model.\ntreatment: the name of the treatment variable. In our case, \"directlyharmed\".\nbenchmark_covariates: the names of covariates that will be used to bound the plausible strength of the unobserved confounders. Here, we put \"female\", which we argue to be among the main determinants of exposure to violence and find to be a strong determinant of attitudes towards peace.\nkd and ky: these arguments parameterize how many times stronger the confounder is related to the treatment (kd) and to the outcome (ky) in comparison to the observed benchmark covariate (female). In our example, setting kd = 1:3 and ky = 1:3 means we want to investigate the maximum strength of a confounder once, twice, or three times as strong as female (in explaining treatment and outcome variation). If only kd is given, ky will be set equal to it by default.\nq: this allows the user to specify what fraction of the effect estimate would have to be explained away to be problematic. Setting q = 1, as we do here, means that a reduction of 100% of the current effect estimate, that is, a true effect of zero, would be deemed problematic. The default is 1.\nalpha: significance level of interest for making statistical inferences. The default is 0.05.\nreduce: should we consider confounders acting towards increasing or reducing the absolute value of the estimate? The default is reduce = TRUE, which means we are considering confounders that pull the estimate towards (or through) zero.\n\n\n\n\n\n# Simpler run of the code\ndarfur.sensitivity &lt;- sensemakr(model = darfur.model, \n                                treatment = \"directlyharmed\",\n                                benchmark_covariates = \"female\",\n                                kd = 1:3)\n\n# printing the result in console\ndarfur.sensitivity\n\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: peacefactor ~ directlyharmed + female + village + age + farmer_dar + \n    herder_dar + pastvoted + hhsize_darfur\n\nNull hypothesis: q = 1 and reduce = TRUE \n\nUnadjusted Estimates of ' directlyharmed ':\n  Coef. estimate: 0.09732 \n  Standard Error: 0.02326 \n  t-value: 4.18445 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.02187 \n  Robustness Value, q = 1 : 0.13878 \n  Robustness Value, q = 1 alpha = 0.05 : 0.07626 \n\nFor more information, check summary.\n\n\nThe print method of sensemakr provides a quick review of the original (unadjusted) estimate along with three summary sensitivity statistics suited for routine reporting: the partial \\(R^2\\) of the treatment with the outcome, the robustness value (RV) required to reduce the estimate entirely to zero (i.e. \\(q=1\\)), and the RV beyond which the estimate would no longer be statistically distinguishable from zero at the 0.05 level (\\(q=1\\), \\(\\alpha=0.05\\)).\n# Latex and HTML results\n\novb_minimal_reporting(darfur.sensitivity, format = \"html\")\n\n\n\n\n\n\nOutcome: peacefactor\n\n\n\n\nTreatment\n\n\nEst.\n\n\nS.E.\n\n\nt-value\n\n\n\\(R^2_{Y \\sim D |{\\bf X}}\\)\n\n\n\\(RV_{q = 1}\\)\n\n\n\\(RV_{q = 1, \\alpha = 0.05}\\)\n\n\n\n\n\n\ndirectlyharmed\n\n\n0.097\n\n\n0.023\n\n\n4.184\n\n\n2.2%\n\n\n13.9%\n\n\n7.6%\n\n\n\n\n\nNote: df = 783; Bound ( 1x female ): \\(R^2_{Y\\sim Z| {\\bf X}, D}\\) = 12.5%, \\(R^2_{D\\sim Z| {\\bf X} }\\) = 0.9%\n\n\n\n#ovb_minimal_reporting(darfur.sensitivity, format = \"html\")\n# summary(darfur.sensitivity) #Verbose summary\nThese three sensitivity statistics provide a minimal reporting for sensitivity analysis. More precisely:\n\nThe robustness value for bringing the point estimate of directlyharmed exactly to zero (\\(RV_{q=1}\\)) is 13.9% . This means that unobserved confounders that explain 13.9% of the residual variance both of the treatment and of the outcome are sufficiently strong to explain away all the observed effect. On the other hand, unobserved confounders that do not explain at least 13.9% of the residual variance both of the treatment and of the outcome are not sufficiently strong to do so.\nThe robustness value for testing the null hypothesis that the coefficient of directlyharmed is zero \\((RV_{q =1, \\alpha = 0.05})\\) falls to 7.6%. This means that unobserved confounders that explain 7.6% of the residual variance both of the treatment and of the outcome are sufficiently strong to bring the lower bound of the confidence interval to zero (at the chosen significance level of 5%). On the other hand, unobserved confounders that do not explain at least 7.6% of the residual variance both of the treatment and of the outcome are not sufficiently strong to do so.\nFinally, the partial \\(R^2\\) of directlyharmed with peacefactor means that, in an extreme scenario, in which we assume that unobserved confounders explain all of the left out variance of the outcome, these unobserved confounders would need to explain at least 2.2% of the residual variance of the treatment to fully explain away the observed effect.\n\nThese are useful quantities that summarize what we need to know in order to safely rule out confounders that are deemed to be problematic. Interpreting these values requires domain knowledge about the data generating process. Therefore, we encourage researchers to argue about what are plausible bounds on the maximum explanatory power that unobserved confounders could have in a given application.\nSometimes researchers may have a hard time making judgments regarding the absolute strength of a confounder, but may have grounds to make relative claims, for instance, by arguing that unobserved confounders are likely not multiple times stronger than a certain observed covariate. In our application, this is indeed the case. One could argue that, given the nature of the attacks, it is hard to imagine that unobserved confounding could explain much more of targeting than what was explained by the observed variable female.\nThe lower corner of the table, thus, provides bounds on confounding as strong as female, \\(R^2_{Y\\sim Z| {\\bf X}, D}\\) = 12.5%, and \\(R^2_{D\\sim Z| {\\bf X} }\\) = 0.9%. Since both of those are below the RV, the table reveals that confounders as strong as female are not sufficient to explain away the observed estimate.\nMoreover, the bound on \\(R^2_{D\\sim Z| {\\bf X} }\\) is below the partial \\(R^2\\) of the treatment with the outcome, \\(R^2_{Y \\sim D |{\\bf X}}\\). This means that even an extreme confounder explaining all residual variation of the outcome and as strongly associated with the treatment as female would not be able to overturn the research conclusions.\nThese results are exact for a single unobserved confounder, and conservative for multiple confounders, possibly acting non-linearly. Finally, the summary method for sensemakr provides an extensive report with verbal descriptions of all these analyses. Here, for instance, entering summary(darfur.sensitivity) produces verbose output similar to the text explanations in the last several paragraphs, so that researchers can directly cite or include such text in their reports.\n\n\n\nSensitivity contour plots of point estimates and t-values\nThe previous sensitivity table provides a good summary of how robust the current estimate is to unobserved confounding. However, researchers may be willing to refine their analysis by visually exploring the whole range of possible estimates that confounders with different strengths could cause. For these, one can use the plot method for sensemakr.\n\n\n\n\n\n\nSensitivity contour plots?\n\n\n\nSensitivity contour plots are graphical tools that visually represent the sensitivity of point estimates and t-values to varying levels of unobserved confounding. They help to visualize the potential impact of different types of confounders on the study results.\n\n\nWe begin by examining the default plot type, contour plots for the point estimate.\n\nplot(darfur.sensitivity)\n\n\n\n\n\n\n\n\nThe horizontal axis shows the hypothetical residual share of variation of the treatment that unobserved confounding explains, \\(R^2_{D\\sim Z| {\\bf X} }\\). The vertical axis shows the hypothetical partial \\(R^2\\) of unobserved confounding with the outcome, \\(R^2_{Y\\sim Z| {\\bf X}, D}\\). The contours show what would be the estimate for directlyharmed that one would have obtained in the full regression model including unobserved confounders with such hypothetical strengths. Note the plot is parameterized in way that hurts our preferred hypothesis, by pulling the estimate towards zero—the direction of the bias was set in the argument reduce = TRUE of sensemakr().\nThe bounds on the strength of confounding, determined by the parameter kd = 1:3 in the call for sensemakr(), are also shown in the plot. Note that the plot reveals that the direction of the effect (positive) is robust to confounding once, twice or even three times as strong as the observed covariate female, although in this last case the magnitude of the effect is reduced to a third of the original estimate.\nWe now examine the sensitivity of the t-value for testing the null hypothesis of zero effect. For this, it suffices to change the option sensitivity.of = \"t-value\".\n\nplot(darfur.sensitivity, sensitivity.of = \"t-value\")\n\n\n\n\n\n\n\n\nThe plot reveals that, at the 5% significance level, the null hypothesis of zero effect would still be rejected given confounders once or twice as strong as female. However, by contrast to the point-estimate, accounting for sampling uncertainty now means that the null hypothesis of zero effect would not be rejected with the inclusion of a confounder three times as strong as female.\n\n\nSensitivity plots of extreme scenarios\nSometimes researchers may be better equipped to make plausibility judgments about the strength of determinants of the treatment assignment mechanism, and have less knowledge about the determinants of the outcome. In those cases, sensitivity plots using extreme scenarios are a useful option. These are produced with the option type = extreme. Here one assumes confounding explains all or some large fraction of the residual variance of the outcome, then vary how strongly such confounding is hypothetically related to the treatment, to see how this affects the resulting point estimate. \n\n\n\n\n\n\nExtreme scenario sensitivity plots\n\n\n\nExtreme scenario sensitivity plots examine the sensitivity of the estimated effect under the conservative assumption that all or a large portion of the unexplained variance in the outcome is due to confounding. These plots help to explore the worst-case scenarios in terms of unobserved confounding.\n\n\n\nplot(darfur.sensitivity, type = \"extreme\")\n\n\n\n\n\n\n\n\nThe default option for the extreme scenarios is r2yz.dx = c(1, 0.75, 0.5), which sets the association of confounders with the outcome to \\(R^2_{Y \\sim Z | X, D} = 100\\%\\), \\(R^2_{Y \\sim Z | X, D} = 75\\%\\), and \\(R^2_{Y \\sim Z | X, D} = 50\\%\\) (producing three separate curves). The bounds on the strength of association of a confounder being once, twice, or three times as strongly associated with the treatment as the effect of being female are shown as red ticks on the horizontal axis.\nAs the plot shows, even in the most extreme case of \\(R^2_{Y \\sim Z | X, D} = 100\\%\\), confounders would need to be more than twice as strongly associated with the treatment to fully explain away the point estimate. Moving to the scenarios \\(R^2_{Y \\sim Z | X, D} = 75\\%\\) and \\(R^2_{Y \\sim Z | X, D} = 50\\%\\), confounders would need to be more than three times as strongly associated with the treatment as the effect of being female in order to fully explain away the point estimate.\n\n\n\n\n\n\nExplanation by Taylor Swift\n\n\n\n\n\n\n\n#Throwback to the good old days when #TaylorSwift explained Omitted Variable Bias to an audience of non-economists 🙃#Swifties #EconSwifties pic.twitter.com/Ygz6uyqSYu\n\n— Alexis H. Villacis ((AHVillacis?)) November 10, 2024",
    "crumbs": [
      "Session 4 - Sensitivity and Mediation"
    ]
  },
  {
    "objectID": "sensitivity-mediation.html#mediation-analysis",
    "href": "sensitivity-mediation.html#mediation-analysis",
    "title": "Session 4 - Sensitivity and Mediation",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\nMediation analysis aims to explain the causal relationship between a treatment variable and an outcome variable by examining the intermediating mediatior variable. That is, it is a way to understand possible causal mechanism. It’s the “how” and “why” behind a cause-and-effect relationship.\nMediation analysis uses Sequential Ignorability which is a crucial assumption for identifying causal mechanisms. Formally,\n\\[\n\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}\n\\begin{align}\n    \\{Y_i(t', m), M_i(t)\\} &\\indep T_i \\mid X_i = x, \\tag{1} \\\\\n    Y_i(t', m) &\\indep M_i(t) \\mid T_i = t, X_i = x, \\tag{2}\n\\end{align}\n\\]\n\\(Eq.1\\) Treatment assignment is ignorable: Given pre-treatment confounders, the treatment is statistically independent of potential outcomes and mediators. This is similar to the “no omitted variable bias” assumption in causal effect estimation.\n\\(Eq.2\\)Mediator is ignorable: Given treatment status and pre-treatment confounders, the mediator is statistically independent of potential outcomes.\nThe second assumption is a particularly strict assumption as it seeks to assume that there is a complete absence of any post as well and pre-treatment confounders (both measured and unmeasured).\nThis assumption allows us to estimate the counterfactual outcomes we need to calculate the effects of the mediator, even though we cannot observe them directly.\nConceptually, mediation is understood by decomposing the total causal effect of a treatment on an outcome into the indirect effect (through the mediator) and the direct effect (all other causal pathways). The indirect effect is also known as the causal mediation effect.\n\n\n\n\n\n\nDecomposed effects\n\n\n\nAverage Causal Mediation Effect (ACME): The average change in the outcome variable that would occur if the mediator were changed from the value it would take under the control condition to the value it would take under the treatment condition, while holding the treatment variable constant.\nAverage Direct Effect (ADE): The average change in the outcome variable that would occur due to the treatment, independent of any effect mediated through the mediator variable.\n\n\n\nHow to do mediation analysis\n\nModel mediator as function of \\(Tr\\) and \\(X\\)s.\nModel outcome as function of \\(Mediator\\), \\(Tr\\), and \\(Xs\\).\nBased on (1) produce two set of prediction values of mediators for \\(Tr =1\\) and \\(Tr = 0\\).\nMake outcome predictions from (2).\n\nOutcome predicted under treatment using predicted mediator values under \\(Tr=1\\).\nOutcome predicted under treatment, now using predicted mediator values under \\(Tr=0\\).\nACME is calculated using average difference between the above two.\n\n\nRefer to Tingley et al. (2014) for thorough details of the code below. The details of the mediation analysis on framing experiment used as an example here is explained in Imai et al. (2011) and the original framing experiment is from the paper by Brader, Valentino, and Suhay (2008) .\n\n\n\n\n\n\nPaper:\n\n\n\nBrader, T., Valentino, N. A., & Suhay, E. (2008). What Triggers Public Opposition to Immigration? Anxiety, Group Cues, and Immigration Threat. American Journal of Political Science, 52(4), 959–978. https://www.jstor.org/stable/25193860\n\nAbstract:\nWe examine whether and how elite discourse shapes mass opinion and action on immigration policy. One popular but untested suspicion is that reactions to news about the costs of immigration depend upon who the immigrants are. We confirm this suspicion in a nationally representative experiment: news about the costs of immigration boosts white opposition far more when Latino immigrants, rather than European immigrants, are featured. We find these group cues influence opinion and political action by triggering emotions-in particular, anxiety-not simply by changing beliefs about the severity of the immigration problem. A second experiment replicates these findings but also confirms their sensitivity to the stereotypic consistency of group cues and their context. While these results echo recent insights about the power of anxiety, they also suggest the public is susceptible to error and manipulation when group cues trigger anxiety independently of the actual threat posed by the group.\n\n\n\n\n\nExploratory Data Analysis\n\nlibrary(mediation)\n# data \ndata(\"framing\", package = \"mediation\")\n\n# codebook for dataset \n# ?framing\n\n# treat - product of two treatment variables. Equal to one when the story focuses on costs and features Latino immigrants\n\nggplot(framing, aes(x = as.factor(treat))) +\n  geom_bar(fill = \"steelblue\", color = \"black\") +\n  labs(x = \"Treatment\", y = \"Count\", title = \"Histogram of Treatment Groups\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# mediator - lower scores equal more negative emotion\n\nggplot(framing, aes(x =emo)) +\n  geom_bar(fill = \"steelblue\", color = \"black\") +\n  labs(x = \"Mediator\", y = \"Count\", title = \"Mediator Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# outcome - whether the subjects requested anti-immigration message to Congress on their behalf\n\nggplot(framing, aes(x =as.factor(cong_mesg))) +\n  geom_bar(fill = \"steelblue\", color = \"black\") +\n  labs(x = \"Outcome\", y = \"Count\", title = \"Outcome Distribution\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# table of anxiousness about increased immigration vs. emotional scale mediator during experiment | See codebook\nkable(table(framing$anx, framing$emo))\n\n\n\n\n\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nnot asked\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nrefused\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nvery anxious\n35\n15\n7\n1\n1\n0\n1\n0\n0\n0\n\n\nsomewhat anxious\n0\n8\n27\n26\n15\n6\n3\n1\n0\n0\n\n\na little anxious\n0\n0\n2\n2\n20\n19\n14\n12\n5\n0\n\n\nnot anxious at all\n0\n0\n0\n0\n0\n1\n4\n8\n10\n22\n\n\n\n\n\n\n\nUsing mediation package\n\n# naive model - using linear probability model\nbase.fit &lt;- lm(cong_mesg ~ treat + age + educ + gender + income, data = framing)\nmodelsummary(base.fit)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)                    \n                  0.295   \n                \n                \n                                                 \n                  (0.160) \n                \n                \n                  treat                          \n                  0.105   \n                \n                \n                                                 \n                  (0.065) \n                \n                \n                  age                            \n                  0.002   \n                \n                \n                                                 \n                  (0.002) \n                \n                \n                  educhigh school                \n                  -0.132  \n                \n                \n                                                 \n                  (0.115) \n                \n                \n                  educsome college               \n                  -0.293  \n                \n                \n                                                 \n                  (0.120) \n                \n                \n                  educbachelor's degree or higher\n                  -0.299  \n                \n                \n                                                 \n                  (0.119) \n                \n                \n                  genderfemale                   \n                  -0.123  \n                \n                \n                                                 \n                  (0.057) \n                \n                \n                  income                         \n                  0.020   \n                \n                \n                                                 \n                  (0.008) \n                \n                \n                  Num.Obs.                       \n                  265     \n                \n                \n                  R2                             \n                  0.081   \n                \n                \n                  R2 Adj.                        \n                  0.056   \n                \n                \n                  AIC                            \n                  348.5   \n                \n                \n                  BIC                            \n                  380.7   \n                \n                \n                  Log.Lik.                       \n                  -165.262\n                \n                \n                  F                              \n                  3.243   \n                \n                \n                  RMSE                           \n                  0.45    \n                \n        \n      \n    \n\n\n#using linear fit to estimate the effect of treatment on the mediator | Mediator\nmed.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing)\nmodelsummary(med.fit)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)                    \n                  8.672   \n                \n                \n                                                 \n                  (0.885) \n                \n                \n                  treat                          \n                  1.339   \n                \n                \n                                                 \n                  (0.360) \n                \n                \n                  age                            \n                  0.002   \n                \n                \n                                                 \n                  (0.010) \n                \n                \n                  educhigh school                \n                  -1.041  \n                \n                \n                                                 \n                  (0.634) \n                \n                \n                  educsome college               \n                  -1.813  \n                \n                \n                                                 \n                  (0.664) \n                \n                \n                  educbachelor's degree or higher\n                  -3.047  \n                \n                \n                                                 \n                  (0.660) \n                \n                \n                  genderfemale                   \n                  0.070   \n                \n                \n                                                 \n                  (0.314) \n                \n                \n                  income                         \n                  -0.035  \n                \n                \n                                                 \n                  (0.042) \n                \n                \n                  Num.Obs.                       \n                  265     \n                \n                \n                  R2                             \n                  0.190   \n                \n                \n                  R2 Adj.                        \n                  0.168   \n                \n                \n                  AIC                            \n                  1254.2  \n                \n                \n                  BIC                            \n                  1286.5  \n                \n                \n                  Log.Lik.                       \n                  -618.123\n                \n                \n                  F                              \n                  8.590   \n                \n                \n                  RMSE                           \n                  2.49    \n                \n        \n      \n    \n\n\n#using probit to estimate the effect of the mediator on the outcome | Outcome\nout.fit &lt;- glm(cong_mesg ~ emo + treat + age + educ + gender + income, data = framing, family = binomial(\"probit\"))\nmodelsummary(out.fit)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)                    \n                  -2.608  \n                \n                \n                                                 \n                  (0.609) \n                \n                \n                  emo                            \n                  0.206   \n                \n                \n                                                 \n                  (0.036) \n                \n                \n                  treat                          \n                  0.040   \n                \n                \n                                                 \n                  (0.202) \n                \n                \n                  age                            \n                  0.006   \n                \n                \n                                                 \n                  (0.006) \n                \n                \n                  educhigh school                \n                  -0.197  \n                \n                \n                                                 \n                  (0.337) \n                \n                \n                  educsome college               \n                  -0.590  \n                \n                \n                                                 \n                  (0.363) \n                \n                \n                  educbachelor's degree or higher\n                  -0.367  \n                \n                \n                                                 \n                  (0.370) \n                \n                \n                  genderfemale                   \n                  -0.396  \n                \n                \n                                                 \n                  (0.175) \n                \n                \n                  income                         \n                  0.084   \n                \n                \n                                                 \n                  (0.026) \n                \n                \n                  Num.Obs.                       \n                  265     \n                \n                \n                  AIC                            \n                  297.3   \n                \n                \n                  BIC                            \n                  329.5   \n                \n                \n                  Log.Lik.                       \n                  -139.652\n                \n                \n                  F                              \n                  6.069   \n                \n                \n                  RMSE                           \n                  0.42    \n                \n        \n      \n    \n\n\n#we estimate ACME and ADE using the mediate function\nmed.out &lt;- mediate(med.fit, out.fit, treat = \"treat\", mediator = \"emo\", robustSE = TRUE, sims = 1000)\nsummary(med.out)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)             0.0816       0.0299         0.14  &lt;2e-16 ***\nACME (treated)             0.0822       0.0303         0.14  &lt;2e-16 ***\nADE (control)              0.0159      -0.0947         0.13    0.80    \nADE (treated)              0.0164      -0.1046         0.14    0.80    \nTotal Effect               0.0981      -0.0266         0.23    0.15    \nProp. Mediated (control)   0.7461      -6.0873         5.81    0.15    \nProp. Mediated (treated)   0.7647      -5.4960         5.35    0.15    \nACME (average)             0.0819       0.0291         0.14  &lt;2e-16 ***\nADE (average)              0.0162      -0.0998         0.14    0.80    \nProp. Mediated (average)   0.7554      -5.7918         5.49    0.15    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 265 \n\n\nSimulations: 1000 \n\n# See the details of function\n#?mediate\n\n# plot \nplot(med.out)\n\n\n\n\n\n\n\n\nIn this example, the estimated ACMEs are statistically significantly different from zero but the estimated average direct and total effects are not. The results suggest that the treatment in the framing experiment may have increased emotional response, which in turn made subjects more likely to send a message to his or her member of Congress. Here, since the outcome is binary all estimated effects are expressed as the increase in probability that the subject sent a message to his or her Congress person.\n\n\nInteraction between the treatment and the mediator\nIt is possible that the ACME takes different values depending on the baseline treatment status. In such a situation, we can add an interaction term between the treatment and mediator to the outcome model.\n\n#treatment and mediator interaction\nset.seed(2014)\n#the estimated ACME varies with treatment status\nmed.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit &lt;- glm(cong_mesg ~ emo * treat + age + educ + gender + income, data = framing, family = binomial(\"probit\")) # adding interaction here\nmed.out &lt;- mediate(med.fit, out.fit, treat = \"treat\", mediator = \"emo\", robustSE = TRUE, sims = 1000)\n\n# summary with interaction \nsummary(med.out)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n                         Estimate 95% CI Lower 95% CI Upper p-value    \nACME (control)            0.07512      0.02808         0.13  &lt;2e-16 ***\nACME (treated)            0.09570      0.03502         0.17  &lt;2e-16 ***\nADE (control)            -0.00761     -0.11916         0.11    0.86    \nADE (treated)             0.01297     -0.11991         0.14    0.82    \nTotal Effect              0.08809     -0.04365         0.22    0.18    \nProp. Mediated (control)  0.72901     -5.06182         6.84    0.18    \nProp. Mediated (treated)  0.97503     -5.33374         7.24    0.18    \nACME (average)            0.08541      0.03482         0.14  &lt;2e-16 ***\nADE (average)             0.00268     -0.11395         0.12    0.98    \nProp. Mediated (average)  0.85202     -5.53193         6.94    0.18    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 265 \n\n\nSimulations: 1000 \n\nplot(med.out)\n\n\n\n\n\n\n\n#testing for the significance of the treatment-mediator interaction\ntest.TMint(med.out, conf.level = .95)\n\n\n    Test of ACME(1) - ACME(0) = 0\n\ndata:  estimates from med.out\nACME(1) - ACME(0) = 0.020577, p-value = 0.358\nalternative hypothesis: true ACME(1) - ACME(0) is not equal to 0\n95 percent confidence interval:\n -0.03019934  0.07454068\n\n\n\n\nSensitivity analysis for sequential ignorability.\nRecall \\(\\rho\\) i.e. the covariance between \\(\\epsilon_{i2}\\) and \\(\\epsilon_{i3}\\) from class slides. It forms a strong assumption in the traditonally followed Linear Structural Equation Modelling Method (LSEM) of determining mediation.\nMore detailed discussion in the section Sensitivity Analysis in Imai et al. (2011, pp 774):\n\n“Imai, Keele, and Tingley (2010) and Imai, Keele, and Yamamoto (2010) propose a sensitivity analysis based on the correlation between \\(\\epsilon_{i2}\\), the error for the mediation model, and \\(\\epsilon_{i3}\\) , the error for the outcome model, under a standard LSEM setting and several commonly used nonlinear models. They use ρ to denote the correlation across the two error terms. If sequential ignorability holds, all relevant pretreatment confounders have been conditioned on, and thus ρ equals zero. However, nonzero values of ρ imply departures from the sequential ignorability assumption and that some hidden confounder is biasing the ACME estimate.”.\n\n\n#sensitivity analysis for sequential ignorability\nmed.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit &lt;- glm(cong_mesg ~ emo + treat + age + educ + gender + income, data = framing, family = binomial(\"probit\"))\nmed.out &lt;- mediate(med.fit, out.fit, treat = \"treat\", mediator = \"emo\", robustSE = TRUE, sims = 1000)\nsens.out &lt;- medsens(med.out, rho.by = 0.1, effect.type = \"indirect\", sims = 1000)\n\nWarning in rho^2 * (1 - r.sq.m) * (1 - r.sq.y): Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\nWarning in err.cr.d^2 * (1 - r.sq.m) * (1 - r.sq.y): Recycling array of length 1 in vector-array arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\nsummary(sens.out)\n\n\nMediation Sensitivity Analysis: Average Mediation Effect\n\nSensitivity Region: ACME for Control Group\n\n     Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n[1,] 0.3        0.0055      -0.0076       0.0172         0.09       0.0493\n[2,] 0.4       -0.0094      -0.0265       0.0020         0.16       0.0877\n\nRho at which ACME for Control Group = 0: 0.3\nR^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09\nR^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0493 \n\n\nSensitivity Region: ACME for Treatment Group\n\n     Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n[1,] 0.3        0.0064      -0.0086        0.020         0.09       0.0493\n[2,] 0.4       -0.0113      -0.0319        0.002         0.16       0.0877\n\nRho at which ACME for Treatment Group = 0: 0.3\nR^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.09\nR^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0493 \n\n\nhere rho.by = 0.1 specifies that ρ will vary from −0.9 to 0.9 by 0.1 increments, and effect.type = “indirect” means that sensitivity analysis is conducted for the ACME. Alternatively, specifying effect.type = “direct” performs sensitivity analysis for the ADE and “both” returns sensitivity analysis for the ACME and ADE.\nThe tabular output from the summary function displays the values of ρ at which the confidence intervals contain zero for the ACME. For both the control and treatment conditions, the confidence intervals for the ACME contain zero when ρ equals 0.3 and 0.4.\nIn the last section of tabular output, the first row captures the point at which the ACME is 0 as a function of the proportions of residual variance in the mediator and outcome explained by the hypothesized unobserved confounder. The second line uses the total variance instead of residual variance.\n\n#plotting sensitivity\n\n#as a function of rho\nplot(sens.out, sens.par = \"rho\", main = \"Anxiety\", ylim = c(-0.2, 0.2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#as a function of R2\nplot(sens.out, sens.par = \"R2\", r.type = \"total\", sign.prod = \"positive\")\n\nWarning in (1 - x$r.square.y) * seq(0, 1 - x$rho.by, 0.01): Recycling array of length 1 in array-vector arithmetic is deprecated.\n  Use c() or as.vector() instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen using the R2 statistic version of sensitivity analysis the user must specify whether the hypothesized confounder affects the mediator and outcome variables in the same direction or in different directions. This matters because the sensitivity analysis is in terms of the product of R2 statistics. In the current example, it is assumed that the confounder influences both variables in the same direction by setting sign.prod = “positive” (rather than sign.prod = “negative”).\nSubstantive interpretation of the plots in Imai et al. (2011, pp 776).",
    "crumbs": [
      "Session 4 - Sensitivity and Mediation"
    ]
  },
  {
    "objectID": "sensitivity-mediation.html#glossary-and-additional-notes",
    "href": "sensitivity-mediation.html#glossary-and-additional-notes",
    "title": "Session 4 - Sensitivity and Mediation",
    "section": "Glossary and Additional Notes",
    "text": "Glossary and Additional Notes\n\n\n\n\n\n\nSensitivity Analysis\n\n\n\nSensitivity by OVB perspective \nSensitivity for Instrumental Variable Regressions \nOmitted Variable Bias (OVB): The bias in the estimate of a treatment effect that arises when a variable that is correlated with both the treatment and the outcome is not included in the regression model.\nSensitivity Analysis: A technique used to assess the robustness of research findings to the potential influence of unobserved confounders. It involves examining how the results would change under different assumptions about the strength and nature of these confounders.\nPartial R2: A statistical measure indicating the proportion of variance in the dependent variable uniquely explained by a specific independent variable, after controlling for other variables in the model.\nRobustness Value (RV): A sensitivity measure developed by Cinelli and Hazlett (2020) that represents the minimum strength of association an unobserved confounder needs to have with both the treatment and outcome to change the study conclusions.\nR2Y∼D|X: The partial R2 of the treatment with the outcome, representing the proportion of outcome variation uniquely explained by the treatment assignment.\nBenchmarking: A method used in sensitivity analysis to gauge the plausible strength of unobserved confounders by comparing them to observed covariates.\nInformal Benchmarking: A problematic approach to benchmarking where researchers use the observed statistics of an included covariate to speculate about the characteristics of the omitted variable. This can lead to inaccurate assessments of sensitivity due to the potential influence of the omitted variable on the observed covariate.\nBounding the Strength of Confounders: A technique proposed by Cinelli and Hazlett (2020) where researchers formally compare the explanatory power of an unobserved confounder to that of an observed covariate. This allows for establishing plausible limits on the potential influence of the unobserved confounder.\nExtreme Scenario Sensitivity Analysis: An analysis where researchers consider the maximum possible strength of association an unobserved confounder could have with the outcome. This usually involves assuming the unobserved confounder explains all residual variation in the outcome.\nSensitivity Contour Plots: Graphical tools that visually display how point estimates, t-values, or other statistics change under different assumptions about the strength of association between the unobserved confounder and the treatment and outcome. These plots help visualize the sensitivity of the results to various confounding scenarios.\nBias Factor (BF): In the partial R2 parameterization, the BF quantifies the degree to which the treatment effect estimate is biased due to an unobserved confounder. It is calculated using the partial R2 values representing the associations between the confounder and the treatment and the confounder and the outcome.\n\n\n\n\n\n\n\n\nMediation Analysis\n\n\n\nAverage Causal Mediation Effect (ACME): The average change in the outcome variable that would occur if the mediator were changed from the value it would take under the control condition to the value it would take under the treatment condition, while holding the treatment variable constant.\nAverage Direct Effect (ADE): The average change in the outcome variable that would occur due to the treatment, independent of any effect mediated through the mediator variable.\nCausal Mechanism: The process by which a treatment variable influences an outcome variable, often involving intermediate variables or mediators.\nComplier Average Mediation Effect (CACME): The ACME for the subset of individuals who comply with the encouragement in an encouragement design.\nEncouragement Design: A research design where an encouragement variable is randomized to influence the mediator, allowing researchers to estimate the causal effect of the mediator on the outcome.\nMediator Variable: A variable that lies on the causal pathway between the treatment and outcome variables, representing the mechanism through which the treatment affects the outcome.\nPotential Outcomes: The hypothetical outcomes that would occur for each individual under different treatment and mediator conditions.\nSequential Ignorability: The assumption that both the treatment and the mediator are effectively randomized, conditional on observed pre-treatment covariates, meaning there are no unobserved confounders.\nSensitivity Analysis: A technique used to assess the robustness of findings to potential violations of key assumptions, such as sequential ignorability, by examining how estimates change as the sensitivity parameters vary.\nSingle-Experiment Design: A research design where only the treatment variable is randomized, while the mediator is not manipulated.\nTreatment Variable: The variable that is manipulated or changed in a study to observe its effect on the outcome variable.\n\n\n\n\n\n\nBailey, Michael A. 2021. Real Stats: Using Econometrics for Political Science and Public Policy. Second edition. New York: Oxford University Press.\n\n\nBrader, Ted, Nicholas A. Valentino, and Elizabeth Suhay. 2008. “What Triggers Public Opposition to Immigration? Anxiety, Group Cues, and Immigration Threat.” American Journal of Political Science 52 (4): 959–78. https://www.jstor.org/stable/25193860.\n\n\nCinelli, Carlos, Jeremy Ferwerda, and Chad Hazlett. n.d. “Sensemakr: Sensitivity Analysis Tools for OLS in R and Stata.” https://doi.org/10.2139/ssrn.3588978.\n\n\nCinelli, Carlos, and Chad Hazlett. 2020. “Making Sense of Sensitivity: Extending Omitted Variable Bias.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (1): 39–67. https://doi.org/10.1111/rssb.12348.\n\n\nHazlett, Chad. 2020. “Angry or Weary? How Violence Impacts Attitudes Toward Peace Among Darfurian Refugees.” Journal of Conflict Resolution 64 (5): 844–70. https://doi.org/10.1177/0022002719879217.\n\n\nImai, Kosuke, Luke Keele, Dustin Tingley, and Teppei Yamamoto. 2011. “Unpacking the Black Box of Causality: Learning about Causal Mechanisms from Experimental and Observational Studies.” American Political Science Review 105 (4): 765–89. https://doi.org/10.1017/S0003055411000414.\n\n\nTingley, Dustin, Teppei Yamamoto, Kentaro Hirose, Luke Keele, and Kosuke Imai. 2014. “Mediation : R Package for Causal Mediation Analysis.” Journal of Statistical Software 59 (5). https://doi.org/10.18637/jss.v059.i05.",
    "crumbs": [
      "Session 4 - Sensitivity and Mediation"
    ]
  },
  {
    "objectID": "sensitivity-mediation.html#footnotes",
    "href": "sensitivity-mediation.html#footnotes",
    "title": "Session 4 - Sensitivity and Mediation",
    "section": "",
    "text": "For detailed explanation see the Video by Carlos Cinelli here and read the paper Cinelli and Hazlett (2020).↩︎\nAlso Page 141 in Bailey (2021)↩︎",
    "crumbs": [
      "Session 4 - Sensitivity and Mediation"
    ]
  },
  {
    "objectID": "moderation.html",
    "href": "moderation.html",
    "title": "Session 5 - Moderation",
    "section": "",
    "text": "Today’s Lab",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "moderation.html#todays-lab",
    "href": "moderation.html#todays-lab",
    "title": "Session 5 - Moderation",
    "section": "",
    "text": "Heterogeneous Effects of Same Treatment\nHow to analyse moderation?\nModeration using interflex package\nIn-class Exercise",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "moderation.html#heterogeneous-effects-of-same-treatment",
    "href": "moderation.html#heterogeneous-effects-of-same-treatment",
    "title": "Session 5 - Moderation",
    "section": "Heterogeneous Effects of Same Treatment",
    "text": "Heterogeneous Effects of Same Treatment\nWe have studied a a range of concepts (in this class and before) that look at how an expnlanatory variable (or \\(treatment\\)) can have its relationship with dependent variable (or \\(outcome\\)) vary according other another variable (moderator or mediator). A year and a half ago, it was covered as \\(interactions\\) in Govt 8001, as \\(moderator\\) in Govt 8002, and in Govt 8003 we have talked about \\(treatment-effect-heterogeneity\\). Additionally, we have also talked about \\(mediation\\) which conceptually has a similar skeleton, with the major change being that of context i.e identifying whether and how the above relationship varies through a particular path (causal mechanisms, that it).\nArguably, they have similarities in mechanics, both mathematically and statistically. The table below looks at how and when all these terms are used.\n\nConcepts and Usage\n\n\n\n\n\n\n\n\nConcept\nQuestion It Answers\nExample\n\n\n\n\nTreatment Effect Heterogeneity\nDoes the treatment effect vary across groups?\nA get-out-the-vote campaign is more effective in swing districts than in safe districts.\n\n\nInteraction\nDoes (X)’s effect on (Y) depend on (Z)?\nThe effect of campaign spending ((X)) on voter turnout ((Y)) depends on voter education levels (\\(Z\\))).\n\n\nModeration\nWhen or for whom does (X \\(\\to\\) Y)?\nThe relationship between political advertisements ((X)) and voter persuasion ((Y)) is stronger for undecided voters ((Z)).\n\n\nMediation\nHow or why does (X \\(\\to\\) Y)?\nEconomic inequality ((X)) increases political polarization ((Y)) by reducing trust in institutions ((M)).\n\n\n\nBelow, we see how despite having same mechanistic underpinnings, these terms have different usage and critically - different aims in knowledge production.\n\n\nKey Differences and Overlap\n\n\n\n\n\n\n\n\n\n\nAspect\nTreatment Effect Heterogeneity\nInteraction\nModeration\nMediation\n\n\n\n\nFocus\nStudies variability in treatment effects across groups.\nExamines joint effects: Does (X)’s effect on (Y) depend on another variable?\nExamines when, where, or for whom (X \\(\\to\\) Y) occurs.\nExplains how or why (X \\(\\to\\) Y) happens.\n\n\nKey Question\nDoes the effect of (X) vary across subgroups?\nDo (X) and another variable combine to influence (Y)?\nDoes a third variable influence the strength/direction of (X \\(\\to\\) Y)?\nWhat is the pathway from (X) to (Y)?\n\n\nCausal or Statistical\nCausal (subgroup differences).\nStatistical (interaction terms).\nStatistical (often causally interpreted).\nCausal (pathways/mechanisms).\n\n\nMethodology\nInteraction terms, subgroup analysis.\nInteraction terms in regression.\nInteraction terms in regression.\nPath analysis, structural equation modeling.\n\n\nExample\nGet-out-the-vote campaigns are more effective in swing districts than safe ones.\nCampaign spending ((X)) affects voter turnout ((Y)) differently based on education ((Z)).\nSocial media use ((X)) affects political knowledge ((Y)) more for younger voters ((Z)).\nEconomic inequality ((X)) increases polarization ((Y)) via reduced institutional trust ((M)).\n\n\nOverlap\nCan be explained by mediation (pathways) or tested via interactions.\nOften used to model both treatment heterogeneity and moderation.\nModeration can influence mediation pathways (moderated mediation).\nMechanisms can interact with moderators or vary across groups.\n\n\n\nTreatment Effect Heterogeneity can arise due to both mediation and moderation.\nModeration occurs when the relationship between a treatment (or independent variable X) and an outcome (or dependent variable Y) changes depending on a third variable (the moderator Z). Moderation explains heterogeneity when the effect of the treatment on the outcome differs for different levels or categories of the moderator variable.\nMediation involves understanding how or why a treatment has an effect on the outcome through an intervening variable (the mediator M). Treatment effect heterogeneity can arise if the mediating pathway varies across different subgroups or conditions. Mediation explains heterogeneity when the process or pathway through which the treatment influences the outcome differs between subgroups.\n\n\n\n\n\n\nExample\n\n\n\nSuppose we are studying a voter turnout campaign (treatment X) and its impact on voter turnout (outcome Y).\n\nModeration Example: The effect of the campaign on voter turnout might be stronger for young voters than for older voters. Age (Z) moderates the effect of the treatment on the outcome.\nMediation Example: The campaign increases trust in the political system (mediator M), and trust is what drives higher voter turnout. However, the mediating effect of trust might be stronger for younger voters than for older voters, creating heterogeneity in the treatment effect due to the mediation pathway.\n\n\n\nLastly, interaction is the statistical concept that underpins the investigation of either of these.",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "moderation.html#how-to-analyse-moderation",
    "href": "moderation.html#how-to-analyse-moderation",
    "title": "Session 5 - Moderation",
    "section": "How to analyse Moderation?",
    "text": "How to analyse Moderation?\nWell, by including interaction term/s in our model.\n\\[\nY = \\beta_0 + \\beta_1 D + \\beta_2 X + \\beta_3 D \\times X + \\gamma \\mathbf{Z} + \\epsilon\n\\]\nwhere, D is our binary treatment and X is a moderator (could be continuous or discrete), and \\(\\mathbf{Z}\\) are other control variables.\nThis model allows for testing of conditional hypothesis of the form:\n\\(H_1\\): The increase in \\(Y\\) is associated with change in D when condition X is present, but not when X is absent.\nIn a paper highly cited for formalizing the practice around usage and reporting of multiplicative-interaction models of the above form, Brambor, Clark, and Golder (2006), list out three instructions which have become a norm in analysis since then (Hainmueller, Mummolo, and Xu 2019). These are:\n\nInclude in the model all constitutive terms (D and X ) alongside the interaction term (D \\(\\times\\) X ).\n\nNot interpret the coefficients on the constitutive terms (\\(\\beta_1\\) and \\(\\beta_2\\)) as unconditional marginal effects.\n\nCompute substantively meaningful marginal effects and confidence intervals, ideally with a plot that shows how the conditional marginal effect of D on Y changes across levels of the moderator X.\n\nHainmueller, Mummolo, and Xu (2019) contend that despite these norms, the practice and reporting of multiplicative-interaction has flaws. They distill them into two overlooked assumptions that the estimation of such models depend on:\n\nLinear Interaction Assumption (LIE): The assumption that the effect of the treatment variable on the outcome variable changes linearly with the moderator variable. That is, interaction effect is linear and follows the functional form:\n\n\\[\\frac{\\partial Y}{\\partial D} = \\beta_1 + \\beta_3 X\\] The LIE assumption suggests that the heterogeneity in effects is such that for every one-unit increase in X, the effect of D on Y changes by \\(\\beta_3\\), and this change in the effect remains constant across the entire range of X.\n\nCommon support: The range of values for the moderator variable where there are observations for both the treatment and control groups. To compute the marginal effect of \\(D\\) at a given value of the moderator, \\(x_0\\)​, two conditions must ideally be met:\n\n\n\n\nthere should be enough observations with X values close to \\(x_0\\), and\n\nthere must be variation in the treatment D at \\(x_0\\).\n\nIf either condition is not met, the estimates of the conditional marginal effect rely on extrapolation or interpolation of the functional form into regions with little or no data. As a result, these effect estimates become fragile and highly dependent on the model used (King and Zeng 2006).\nSolution\nHainmueller, Mummolo, and Xu (2019) propose few remedies to the issues.\n\nDiagnostic Tool To asses the validity or invalidity of LIE and common support assumptions, they suggest usage of Linear-Interaction-Diagnostic (LID plots).\n\n\n\n\n\n\nChecking, if the linear regression lines (blue) and the LOESS fits (red) diverge considerably across the range of X values, and compare the distribution of X in both groups and examine the range of X values for which there are a sufficient number of data points for the estimation of marginal effects. Taken from Page 169 in Hainmueller, J., Mummolo, J., & Xu, Y. (2019). How Much Should We Trust Estimates from Multiplicative Interaction Models? Simple Tools to Improve Empirical Practice. Political Analysis, 27(2), 163–192. https://doi.org/10.1017/pan.2018.46\n\n\n\n\nEstimation- They propose two alternative estimation strategies:\n\nBinning estimator: This approach discretizes the moderator into bins and allows for different treatment effects in each bin, capturing non-linearity. As the authors state, “the binning estimator is much more flexible as it jointly fits the interaction components of the standard model to each bin separately, thereby relaxing the LIE assumption” (p. 177).\n\n\\[\nY = \\sum_{j=1}^{3} \\big\\{\\mu_j + \\alpha_j D + \\eta_j (X - x_j) + \\beta_j (X - x_j)D\\big\\} G_j + Z\\gamma + \\epsilon\n\\]\nwhere, \\(G_js\\) are dummy variables for bins. When entire range is split into three bins according to treciles. They are defined as shown below,\n\\[\nG_1 =\n\\begin{cases}\n1 & X &lt; \\delta_{1/3}, \\\\\n0 & \\text{otherwise},\n\\end{cases}\n\\quad\nG_2 =\n\\begin{cases}\n1 & X \\in [\\delta_{1/3}, \\delta_{2/3}], \\\\\n0 & \\text{otherwise},\n\\end{cases}\n\\quad\nG_3 =\n\\begin{cases}\n1 & X \\geq \\delta_{2/3}, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]\n\n\nKernel estimator: This method employs non-parametric smoothing to estimate the conditional marginal effect of the treatment across the range of the moderator, again relaxing the LIE assumption. The output is a smooth function rather than a set of discrete point estimates, making it potentially less straightforward to summarize and interpret compared to the binning estimator 1.",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "moderation.html#moderation-using-interflex-package",
    "href": "moderation.html#moderation-using-interflex-package",
    "title": "Session 5 - Moderation",
    "section": "Moderation using interflex package",
    "text": "Moderation using interflex package\nThe solutions discussed above are incorporated by Jens Hainmueller, Jiehan Liu, Licheng Liu, Ziyi Liu, Jonathan Mummolo, Tianzhu Qin, and Yiqing Xu in interflex package. The functions in it estimates, interprets, and visualizes marginal effects and performs diagnostics, as proposed by Hainmueller, Mummolo, and Xu (2019) 2.\n\n# Load package\n# install.packages(\"interflex\")\nlibrary(interflex)\n\n# Comes with simulated datasets\n# Code on construction of simulation datasets is given below\n# No need to run. It is just for reference\ndata(interflex)\nls()\n\n[1] \"s1\" \"s2\" \"s3\" \"s4\" \"s5\" \"s6\" \"s7\" \"s8\" \"s9\"\n\n\n\n\nCode\nset.seed(1234)\nn&lt;-200\nd1&lt;-sample(c(0,1),n,replace=TRUE) # dichotomous treatment\nd2&lt;-rnorm(n,3,1) # continuous treatment\nx&lt;-rnorm(n,3,1) # moderator\nz&lt;-rnorm(n,3,1) # covariate\ne&lt;-rnorm(n,0,1) # error term\n\n## linear marginal effect\ny1&lt;-5 - 4 * x - 9 * d1 + 3 * x * d1 + 1 * z + 2 * e\ny2&lt;-5 - 4 * x - 9 * d2 + 3 * x * d2 + 1 * z + 2 * e\ns1&lt;-cbind.data.frame(Y = y1, D = d1, X = x, Z1 = z)\ns2&lt;-cbind.data.frame(Y = y2, D = d2, X = x, Z1 = z)\n\n## quadratic marginal effect\nx3 &lt;- runif(n, -3,3) # uniformly distributed moderator\ny3 &lt;- d1*(x3^2-2.5) + (1-d1)*(-1*x3^2+2.5) + 1 * z + 2 * e\ns3 &lt;- cbind.data.frame(D=d1, X=x3, Y=y3, Z1 = z)\n\n## adding two-way fixed effects\nn  &lt;- 500\nd4 &lt;-sample(c(0,1),n,replace=TRUE) # dichotomous treatment\nx4 &lt;- runif(n, -3,3) # uniformly distributed moderator\nz4 &lt;- rnorm(n, 3,1) # covariate\nalpha &lt;- 20 * rep(rnorm(n/10), each = 10)\nxi &lt;- rep(rnorm(10), n/10)\ny4 &lt;- d4*(x4^2-2.5) + (1-d4)*(-1*x4^2+2.5) + 1 * z4 + alpha + xi + \n  2 * rnorm(n,0,1)\ns4 &lt;- cbind.data.frame(D=d4, X=x4, Y=y4, Z1 = z4, \n                       unit = rep(1:(n/10), each = 10),\n                       year = rep(1:10, (n/10)))\n\n## Multiple treatment arms\nn &lt;- 600\n# treatment 1\nd1 &lt;- sample(c('A','B','C'),n,replace=T)\n# moderator\nx &lt;- runif(n,min=-3, max = 3)\n# covriates\nz1 &lt;- rnorm(n,0,3)\nz2 &lt;- rnorm(n,0,2)\n# error\ne &lt;- rnorm(n,0,1)\ny1 &lt;- rep(NA,n)\ny1[which(d1=='A')] &lt;- -x[which(d1=='A')]\ny1[which(d1=='B')] &lt;- (1+x)[which(d1=='B')]\ny1[which(d1=='C')] &lt;- (4-x*x-x)[which(d1=='C')]\ny1 &lt;- y1 + e + z1 + z2\ns5 &lt;- cbind.data.frame(D=d1, X=x, Y=y1, Z1 = z1,Z2 = z2)\n\n\ns1 is a case of a dichotomous treatment indicator with linear marginal effects;\ns2 is a case of a continuous treatment indicator with linear marginal effects;\ns3 is a case of a dichotomous treatment indicator with nonlinear marginal effects;\ns4 is a case of a dichotomous treatment indicator, nonlinear marginal effects, with additive two-way fixed effects; and\ns5 is a case of a discrete treatment indicator, nonlinear marginal effects, with additive two-way fixed effects.\n\nRaw LID Plots\nLet’s check different types of datsets and outputs\n\nDichotomous treatment indicator with linear marginal effects.\n\n\ninterflex(estimator = \"raw\",\n          Y = \"Y\", \n          D = \"D\", \n          X = \"X\", \n          data = s1, \n          weights = NULL,\n          Ylabel = \"Outcome\", Dlabel = \"Treatment\", Xlabel=\"Moderator\", \n          main = \"Raw Plot\", cex.main = 1.2, ncols=2\n          )\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\n\n\n\n\n\n\n\n\n\nContinuous treatment indicator with linear marginal effects.\n\n\n\ninterflex(estimator = \"raw\", Y = \"Y\", D = \"D\", X = \"X\", data = s2,\n          Ylabel = \"Outcome\", Dlabel = \"Treatment\", Xlabel=\"Moderator\", \n          theme.bw = TRUE, show.grid = FALSE, ncols=3)\n\n\n\n\n\n\n\n\n\nDichotomous treatment indicator with nonlinear marginal effects.\n\n\ninterflex(estimator = \"raw\", Y = \"Y\", D = \"D\", X = \"X\", data = s3,\n          Ylabel = \"Outcome\", Dlabel = \"Treatment\", \n          Xlabel=\"Moderator\",ncols=3)\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation Exercise\n\n\n\n\nInterpret the three plots. What do the red and blue lines signify?\nIn which of them LIE seems to be violated?\nWhich seems to be suffering from lack of common support?\n\nRead the documentation with ?interflex and 3. Diagnostics section in Hainmueller, Mummolo, and Xu (2019).\n\n\n\n\nBinning Estimator\nWe will use s1 dataset. The data generating process underlying it is\n\\[\nY = -5 - 4X - 9D + 3X \\times D + Z + 2\\epsilon\n\\]\n\nout &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", data = s1, \n                 estimator = \"binning\", vcov.type = \"robust\", \n                 main = \"Marginal Effects\", ylim = c(-15, 15))\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\nplot(out)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow are the bins created in this plot?\nAlso, interpret the histogram at the bottom.\n\n\n\nNote that interflex will also automatically report a set of statistics when estimator = “binning”, including: (1) the binning estimates and their standard errors and 95% confidence intervals, (2) the percentage of observations within each bin, (3) the L-kurtosis of the moderator, and (4) a Wald test to formally test if we can reject the linear multiplicative interaction model by comparing it with a more flexible model of multiple bins\n\nprint(out$tests$p.wald)\n\n[1] \"0.526\"\n\n\nInterpreting the wald test p-vlue We see that the Wald test cannot reject the NULL hypothesis that the linear interaction model and the three-bin model are statistically equivalent.\n\nWhen LIE is not valid\nLet’s look at dataset s3, which is simulated from the following data generating process,\n\\[\nY = 2.5 - 2.5D - X^2 + 2(D \\times X^2) + Z + \\varepsilon\n\\] The marginal effect of D on Y is\n\\[\n\\frac{\\partial Y}{\\partial D} = -2.5 + 2X^2\n\\] which is non-linear interaction. However, by estimating with linear interaction assumption, we would have missed out this facet and get biased results.\n\nout_li &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", data = s3, \n                    estimator = \"linear\") \n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\nplot(out_li)\n\n\n\n\n\n\n\n\nBinning estimator gives a better insight into the non-linear interactive relationship.\n\nout_nli &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", data = s3, \n                     estimator = \"binning\")\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\nplot(out_nli)\n\n\n\n\n\n\n\n\nLet’s increase the number of bins.\n\nout_nli_5 &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", \n                       data = s3, nbins = 5,\n                     estimator = \"binning\", main = \"With 5 bins\")\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\nplot(out_nli_5)\n\n\n\n\n\n\n\n\nConducting wald tests again, for the new models with 3 and 5 bins.\n\nprint(out_nli$tests$p.wald)\n\n[1] \"0.000\"\n\nprint(out_nli_5$tests$p.wald)\n\n[1] \"0.000\"\n\n\nThis time the NULL hypothesis that the linear interaction model and the three-bin model are statistically equivalent is safely rejected (p.wald = 0.00).\n\n\n\nKernel Estimator\nImagine, increasing the number of bins infinitely. We would be somewhat capturing the underlying relationship, in a smoothed curve 3.\nThe kernel estimator is a method used to flexibly estimate the functional form of a marginal effect across the values of a moderator variable, relaxing the linear interaction effect (LIE) assumption and offering protection against the problem of lack of common support.\n\nout_ke &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", data = s3, \n                 estimator = \"kernel\", theme.bw = TRUE)\n\nBaseline group not specified; choose treat = 0 as the baseline group. \nCross-validating bandwidth ... \nParallel computing with 4 cores...\nOptimal bw=0.4582.\nNumber of evaluation points:50\nParallel computing with 4 cores...\n\n\nout_ke$figure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can change the plotting attributes, without running the interflex function again. See code below how it uses same out_ke output from interflex function inside a plotting command\n\nplot(out_ke, main = \"Nonlinear Marginal Effects\",\n     ylab = \"Coefficients\",\n     Xdistr = \"density\", # Instead of histogram, density curve\n     xlim = c(-3,3), ylim = c(-10,12),\n     CI = FALSE, cex.main = 0.8, cex.lab = 0.7, cex.axis = 0.7) \n\n\n\n\n\n\n\n\n\n\nThe semi-parametric marginal effect estimates are stored in out$est.kernel.\nNote that we can use the file option to save a plot to a file in interflex (e.g. by setting file = \"myplot.pdf\" or file = \"myplot.png\").\n\n\n\n\n\n\nFeatures of Kernel Estimator\n\n\n\nAdvantages:\nFlexibility: Allows the conditional effect of the treatment variable to vary freely across the range of the moderator, capturing nonlinear and nonmonotonic relationships4.\nProtection against extrapolation: Estimates the marginal effect across the full range of the moderator, highlighting areas with limited common support through wider confidence intervals57.\nAutomated bandwidth selection: The bandwidth parameter, h, which determines the smoothness of the estimated function, is automatically selected through cross-validation, minimizing subjectivity8.\nLimitation\nThe output is a smooth function rather than a set of discrete point estimates, making it potentially less straightforward to summarize and interpret compared to the binning estimator\n\n\n\n\nEstimation with Fixed effects\nFocusing on dataset s4, which has the following DGP\n\\[\nY = \\beta_0 + \\beta_1 D + \\beta_2 X^2 + \\beta_3 (D \\times X^2) + \\beta_4 Z + \\alpha + \\xi + \\varepsilon\n\\]\nThe regression includes both unit fixed effects (\\(\\alpha\\)) and year fixed effects (\\(\\xi\\)), which control for unobserved heterogeneity across units (such as individuals or firms) and years (such as time effects).\nRemember in s4, a large chunk of the variation in the outcome variable is driven by group fixed effects (see the code where simulated datasets are created above). Below is a scatterplot of the raw data (group index vs. outcome). Red and green dots represent treatment and control units, respectively. We can see that outcomes are highly correlated within a group.\n\nggplot(s4, aes(x=group, y = Y, colour = as.factor(D))) + geom_point() + guides(colour=FALSE) \n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\nWhen fixed effects are present, it is possible that we cannot observe a clear pattern of marginal effects in the raw plot as before, while binning estimates have wide confidence intervals.\n\ninterflex(estimator = \"raw\", Y = \"Y\", D = \"D\", X = \"X\", \n          data = s4, weights = NULL,ncols=2)\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\n\n\n\n\n\n\n\n\n\n# binning\ns4.binning &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", \n                        data = s4, estimator = \"binning\",\n                        FE = NULL, cl = \"group\")\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\nplot(s4.binning)\n\n\n\n\n\n\n\n\nSimilarly, kernel estimates have wide cross-validated bandwidths.\n\n# Kernel\ns4$wgt &lt;- 1\ns4.kernel &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\",\n                       data = s4, estimator = \"kernel\",\n                       FE = NULL, cl = \"group\", weights = \"wgt\")\n\nBaseline group not specified; choose treat = 0 as the baseline group. \nCross-validating bandwidth ... \nParallel computing with 4 cores...\nOptimal bw=1.7732.\nNumber of evaluation points:50\nParallel computing with 4 cores...\n\n\nplot(s4.kernel)\n\n\n\n\n\n\n\n\nThe issues are caused by in-correct identification and specification of the model.\nThe binning and kernel estimates are much more informative when fixed effects are included, by using the FE option. Note that the number of group indicators can exceed 2. The algorithm is optimized for a large number of fixed effects or many group indicators. The cl option controls the level at which standard errors are clustered.\n\nBinning with FE\n\ns4.binning &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", \n                        data = s4, estimator = \"binning\", \n                        FE = c(\"group\", \"year\"), cl = \"group\", \n                        weights = \"wgt\")\n\nBaseline group not specified; choose treat = 0 as the baseline group. \n\nplot(s4.binning)\n\n\n\n\n\n\n\n\n\n\nKernel with FE\n\ns4.kernel &lt;- interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = \"Z1\", \n                       data = s4, estimator = \"kernel\",\n                       FE = c(\"group\",\"year\"), cl = \"group\")\n\nBaseline group not specified; choose treat = 0 as the baseline group. \nCross-validating bandwidth ... \nParallel computing with 4 cores...\nOptimal bw=0.3514.\nNumber of evaluation points:50\nParallel computing with 4 cores...\n\n\nplot(s4.kernel)\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Heterogeneity in Effect\nWe can use interflex to visualize predicted outcomes based on non-linear interaction that we just analysed.\n\ns5.kernel &lt;-interflex(Y = \"Y\", D = \"D\", X = \"X\", Z = c(\"Z1\", \"Z2\"),\n                      data = s5, estimator = \"kernel\")\n\nBaseline group not specified; choose treat = A as the baseline group. \nCross-validating bandwidth ... \nParallel computing with 4 cores...\nOptimal bw=0.3522.\nNumber of evaluation points:50\nParallel computing with 4 cores...\n\n\npredict(s5.kernel)\n\n\n\n\n\n\n\n\nOr, by setting pool = TRUE,\n\npredict(s5.kernel,order = c('A','B','C'),\n        subtitle = c(\"Group A\", \"Group B\", \"Group C\"), pool = T,\n        legend.title = \"Three Different Groups\")\n\n\n\n\n\n\n\n\n\n\nAdditional Notes on Advanced Issues\nThe documentation of the interflex package also has examples of application to cases of Multiple (&gt;2) Treatment Arms, as well as analysing Difference in Treatment Effects at Different Values of the Moderator.\nThe interflex function also incorporate the interaction terms between the moderator X and all covariates Z in models to reduce biases induced by their correlations (Blackwell and Olson 2022). Though not recommended, users can turn off this option by setting full.moderate = FALSE.\nWhile Hainmueller, Mummolo, and Xu (2019) provide much needed disgnostics and estimation tools, Beiser-McGrath and Beiser-McGrath (2023) argue that by allowing simply for non-linear interaction between treatment and moderator, “researchers can infer nonlinear interaction effects, even though the true interaction effect is linear, when variables used for covariate adjustment that are correlated with the moderator have a nonlinear effect upon the outcome of interest”. If the research includes moderated effects, the latter paper is recommended to be read.",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "moderation.html#in-class-exercise",
    "href": "moderation.html#in-class-exercise",
    "title": "Session 5 - Moderation",
    "section": "In-class Exercise",
    "text": "In-class Exercise\n\n\n\n\n\n\nUse the replication dataset that you have been using till now to perform the following:\n\nIdentify a covariate that could plausibly be driving heterogeneity in treatment effect. You could use theoretical priors or your experience of working with this dataset to inform your choice.\nWrite down two versions of possible moderation in the data generating process - linear and non-linear interaction ones.\nMake LID plots to assess your hypothesised interactions in 2.\nUse interflex function to assess moderation with binning as well as kernel estimators.\nUse wald-test p-value from interflex output to comment on your inference.\n\n\n\n\n\n\n\n\nBeiser-McGrath, Janina, and Liam F. Beiser-McGrath. 2023. “The Consequences of Model Misspecification for the Estimation of Nonlinear Interaction Effects.” Political Analysis 31 (2): 278–87. https://doi.org/10.1017/pan.2022.25.\n\n\nBlackwell, Matthew, and Michael P. Olson. 2022. “Reducing Model Misspecification and Bias in the Estimation of Interactions.” Political Analysis 30 (4): 495–514. https://doi.org/10.1017/pan.2021.19.\n\n\nBrambor, Thomas, William Roberts Clark, and Matt Golder. 2006. “Understanding Interaction Models: Improving Empirical Analyses.” Political Analysis 14 (1): 63–82. https://doi.org/10.1093/pan/mpi014.\n\n\nHainmueller, Jens, Jonathan Mummolo, and Yiqing Xu. 2019. “How Much Should We Trust Estimates from Multiplicative Interaction Models? Simple Tools to Improve Empirical Practice.” Political Analysis 27 (2): 163–92. https://doi.org/10.1017/pan.2018.46.\n\n\nKing, Gary, and Langche Zeng. 2006. “The Dangers of Extreme Counterfactuals.” Political Analysis 14 (2): 131–59. https://doi.org/10.1093/pan/mpj004.",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "moderation.html#footnotes",
    "href": "moderation.html#footnotes",
    "title": "Session 5 - Moderation",
    "section": "",
    "text": "The kernel estimator discussed in the Hainmueller paper is considered semi-parametric because it uses:\nA non-parametric approach: to estimate the potentially nonlinear relationship between the outcome, the treatment, and the moderator.\nA parametric approach: for the relationship between the outcome and the control variables.\nAlso see, https://en.wikipedia.org/wiki/Kernel_density_estimation.↩︎\nThe code and discussion below has been adapted from the vignette of the package available at https://yiqingxu.org/packages/interflex/RGuide.html.↩︎\nThis is not fully what happens in kernel smoothening but is nevertheless a good learning/heuristic tool.↩︎",
    "crumbs": [
      "Session 5 - Moderation"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Andridge, Rebecca R., and Roderick J. A. Little. 2010. “A Review\nof Hot Deck Imputation for Survey Non-Response.”\nInternational Statistical Review = Revue Internationale de\nStatistique 78 (1): 40. https://doi.org/10.1111/j.1751-5823.2010.00103.x.\n\n\nBailey, Michael A. 2021. Real Stats: Using Econometrics for\nPolitical Science and Public Policy. Second edition. New York:\nOxford University Press.\n\n\nBeiser-McGrath, Janina, and Liam F. Beiser-McGrath. 2023. “The\nConsequences of Model Misspecification for the Estimation of Nonlinear\nInteraction Effects.” Political Analysis 31 (2): 278–87.\nhttps://doi.org/10.1017/pan.2022.25.\n\n\nBlackwell, Matthew, and Michael P. Olson. 2022. “Reducing Model\nMisspecification and Bias in the Estimation of Interactions.”\nPolitical Analysis 30 (4): 495–514. https://doi.org/10.1017/pan.2021.19.\n\n\nBrader, Ted, Nicholas A. Valentino, and Elizabeth Suhay. 2008.\n“What Triggers Public Opposition to Immigration? Anxiety, Group\nCues, and Immigration Threat.” American Journal of Political\nScience 52 (4): 959–78. https://www.jstor.org/stable/25193860.\n\n\nBrady, Henry E., and John E. Mcnulty. 2011. “Turning Out to Vote:\nThe Costs of Finding and Getting to the Polling Place.” The\nAmerican Political Science Review 105 (1): 115–34. https://www.jstor.org/stable/41480830.\n\n\nBrambor, Thomas, William Roberts Clark, and Matt Golder. 2006.\n“Understanding Interaction Models: Improving Empirical\nAnalyses.” Political Analysis 14 (1): 63–82. https://doi.org/10.1093/pan/mpi014.\n\n\nCinelli, Carlos, Jeremy Ferwerda, and Chad Hazlett. n.d.\n“Sensemakr: Sensitivity Analysis Tools for OLS in R and\nStata.” https://doi.org/10.2139/ssrn.3588978.\n\n\nCinelli, Carlos, and Chad Hazlett. 2020. “Making Sense of\nSensitivity: Extending Omitted Variable Bias.” Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology) 82\n(1): 39–67. https://doi.org/10.1111/rssb.12348.\n\n\nCunningham, Scott. n.d. “Causal Inference the Mixtape - 5 \nMatching and Subclassification.” In. https://mixtape.scunning.com/05-matching_and_subclassification.\n\n\nHainmueller, Jens, Jonathan Mummolo, and Yiqing Xu. 2019. “How\nMuch Should We Trust Estimates from Multiplicative Interaction Models?\nSimple Tools to Improve Empirical Practice.” Political\nAnalysis 27 (2): 163–92. https://doi.org/10.1017/pan.2018.46.\n\n\nHartman, Erin. 2021. “Equivalence Testing for Regression\nDiscontinuity Designs.” Political Analysis 29 (4):\n505–21. https://doi.org/10.1017/pan.2020.43.\n\n\nHartman, Erin, and F. Daniel Hidalgo. 2018. “An Equivalence\nApproach to Balance and Placebo Tests.” American Journal of\nPolitical Science 62 (4): 1000–1013. https://doi.org/10.1111/ajps.12387.\n\n\nHazlett, Chad. 2020. “Angry or Weary? How Violence Impacts\nAttitudes Toward Peace Among Darfurian Refugees.” Journal of\nConflict Resolution 64 (5): 844–70. https://doi.org/10.1177/0022002719879217.\n\n\nHonaker, James, Gary King, and Matthew Blackwell. 2011.\n“Amelia II: A Program for Missing\nData.” Journal of Statistical Software 45 (7). https://doi.org/10.18637/jss.v045.i07.\n\n\nHuntington-Klein, Nick. n.d. Chapter 14 - Matching | the\nEffect. https://theeffectbook.net/ch-Matching.html.\n\n\nImai, Kosuke, Luke Keele, Dustin Tingley, and Teppei Yamamoto. 2011.\n“Unpacking the Black Box of Causality: Learning about Causal\nMechanisms from Experimental and Observational Studies.”\nAmerican Political Science Review 105 (4): 765–89. https://doi.org/10.1017/S0003055411000414.\n\n\nKing, Gary, James Honaker, Anne Joseph, and Kenneth Scheve. 2001.\n“Analyzing Incomplete Political Science Data: An Alternative\nAlgorithm for Multiple Imputation.” American Political\nScience Review 95 (1): 49–69. https://doi.org/10.1017/S0003055401000235.\n\n\nKing, Gary, and Langche Zeng. 2006. “The Dangers of Extreme\nCounterfactuals.” Political Analysis 14 (2): 131–59. https://doi.org/10.1093/pan/mpj004.\n\n\nKowarik, Alexander, and Matthias Templ. 2016. “Imputation with the\nR Package\nVIM.” Journal of Statistical\nSoftware 74 (7). https://doi.org/10.18637/jss.v074.i07.\n\n\nLall, Ranjit. 2017. “How Multiple Imputation Makes a\nDifference.” Political Analysis 24 (4): 414–33. https://doi.org/10.1093/pan/mpw020.\n\n\nLittle, Roderick J. 2021. “Missing Data Assumptions.”\nAnnual Review of Statistics and Its Application 8 (Volume 8,\n2021): 89–107. https://doi.org/10.1146/annurev-statistics-040720-031104.\n\n\n———. 2024. “Missing Data Analysis.” Annual Review of\nClinical Psychology 20 (1): 149–73. https://doi.org/10.1146/annurev-clinpsy-080822-051727.\n\n\nTingley, Dustin, Teppei Yamamoto, Kentaro Hirose, Luke Keele, and Kosuke\nImai. 2014. “Mediation :\nR Package for Causal Mediation Analysis.”\nJournal of Statistical Software 59 (5). https://doi.org/10.18637/jss.v059.i05.\n\n\nWellek, Stefan. 2010. Testing statistical hypotheses of equivalence\nand noninferiority. Second edition. Boca Raton London New York: CRC\nPress, Chapman & Hall.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "matching-equivalence.html",
    "href": "matching-equivalence.html",
    "title": "Session 2 - Matching and Equivalence",
    "section": "",
    "text": "Ackowledgements\nThe lab builds on the resources:",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  },
  {
    "objectID": "matching-equivalence.html#ackowledgements",
    "href": "matching-equivalence.html#ackowledgements",
    "title": "Session 2 - Matching and Equivalence",
    "section": "",
    "text": "Chapter 14 - Matching (Huntington-Klein, n.d.)\nChapter 5 - Matching and Subclassification (Cunningham, n.d.)\nLab Material for Govt 8003 created by Nicolo Bonifai and Ben Burnley",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  },
  {
    "objectID": "matching-equivalence.html#todays-lab",
    "href": "matching-equivalence.html#todays-lab",
    "title": "Session 2 - Matching and Equivalence",
    "section": "Today’s Lab",
    "text": "Today’s Lab\n\nMatching\n\n\nStratification\nExact Matching\nPropensity Score Matching\nChecking the Match\n- On Covariates\n- On Propensity Scores - Comparing Results\n\n\nEquivalence Testing",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  },
  {
    "objectID": "matching-equivalence.html#material",
    "href": "matching-equivalence.html#material",
    "title": "Session 2 - Matching and Equivalence",
    "section": "Material",
    "text": "Material\n\nDownload the 8003 Matching and Equivalence Test Lab Zipped Folder.\nRun the .RProj file.\nOpen matching-10012024.qmd in Rstudio.\n\n\n\n\n\nCunningham, Scott. n.d. “Causal Inference the Mixtape - 5  Matching and Subclassification.” In. https://mixtape.scunning.com/05-matching_and_subclassification.\n\n\nHuntington-Klein, Nick. n.d. Chapter 14 - Matching | the Effect. https://theeffectbook.net/ch-Matching.html.",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab for Quantitative Methods - 3",
    "section": "",
    "text": "Why are we here?\n\nThis web book is meant to be one stop-resource for course related labs and some interesting stuff. The detailed course material is available on the Canvas Page.\nYou can find here all the code, links to replication data, and notes from the lab session. We will be using R for all the analysis work in this course.\nI will update the session wise pages as we go through them.\nEveryone should have the following on their systems:\n\nR - Statistical Programming Language.\nRStudio - Interactive Development Environment for R.\n\\(\\LaTeX\\) (pronounced â€œLAY-tekâ€ or â€œLAH-Tek) Typesetting tool for preparing high-quality professional documents.\n\nYou can check the installation of \\(\\LaTeX\\) for R by following the instructions here.\nIt would be useful if you are familiar with tidyverse framework. If not, or if you need a refresher, the online book here by Hadley Wickham is a great resource.\nIf you are fairly confident, the amazing song below is nevertheless a good recall heuristic!",
    "crumbs": [
      "Why are we here?"
    ]
  },
  {
    "objectID": "latex-quarto.html",
    "href": "latex-quarto.html",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "",
    "text": "Todayâ€™s Lab",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#todays-lab",
    "href": "latex-quarto.html#todays-lab",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "",
    "text": "Good Coding\n\\(\\LaTeX\\)\nQuarto\n\n\nYAML\nCode Chunks\nMarkdown text\n\n\nR Projects\n\n\nhere package\n\n\nFolder Structure",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#good-coding",
    "href": "latex-quarto.html#good-coding",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Good Coding",
    "text": "Good Coding\nGood programming or coding is closely related to the idea of Literate Statistical Programming. As Donald Knuth (1984) defines, it is a way to write programs that focuses on explaining to human readers what we want the computers to do, rather than just instructing the computers to do so.\nStatistical Programming , hence, is about formalizing your thinking about how you treat the data and using functional programming to automate such formalized tasks to be done repetitively. It improves efficiency, enhances reproducibility, and boosts creativity when it comes to finding new patterns in your data.\nGuidelines for data and statistical analyses:1\n\nAccuracy: Write a code that reduces the chances of making an error and lets you catch one if it occurs.\nEfficiency: If you are doing it twice, see the pattern of your decision-making and formalize it in your code. Difference between Excel and coding\nReplicate-and-Reproduce: Ability to repeat the computational process which reflects your thinking and decisions that you took along the way. Improves transparency and forces one to be deliberate and responsible about choices during analyses.\n\nHuman Interpretability: Writing code is not just about analyzing but allowing yourself and then others to be able to understand your analytic choices.\n\nPublic Good: Research is a public good. And the code allows your research to be truly accessible. This means you write a code that anyone else who understands the language can read, reuse, and recreate without you being present. We essentially ensure that by writing a readable and ideally publicly accessible code.\n\nFurther, writing good code could also benefit from some common guidelines used across coders. A good starting point is the tidyverse style guide.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#latex",
    "href": "latex-quarto.html#latex",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "\\(\\LaTeX\\)",
    "text": "\\(\\LaTeX\\)\n\\(\\LaTeX\\) (pronounced â€œLAY-tekâ€ or â€œLAH-Tek) is a typesetting tool for preparing high-quality professional documents. It is the preferred typesetting tool used in high-end scientific documentation task.It is not a word-processing tool. It is a simple tool without too many priors about how the document should look like.\n\\(\\LaTeX\\) gives us superior control over how your document look like, has enhanced capabilities to write technical specifications (Maths, stats, proofs, etc.), include code, and produces readily editable back-end documents.\nThere are many interfaces that allow you to work with \\(\\LaTeX\\). Overleaf is a widely used online platform and Texmaker is a popular offline application.\nHowever, RStudio has in-built capability to double as a \\(\\LaTeX\\) editor. Previously RMarkdown and now Quarto have capabilities that you can harness to achieve professional and beautifully typeset documents.\nThink of writing an equation like:\n\\[\nViolence_{i,j} = \\beta_0 + \\beta_1EthnicFractionalization_i + \\gamma_j + \\epsilon_i\n\\] In Latex, using quarto, you have to write something like the following:\n$Violence_{i,j} = \\\\beta_0 + \\\\beta_1EthnicFractionalization_i + \\\\gamma_j + \\\\epsilon_i$\nFor a single line of text we encapsulate code by $ sign.\nFor multi-line code we use $$.\nRead more about \\(\\LaTeX\\) here\nThe box folder has some detailed resources for helping with typesetting in \\(\\LaTeX\\).\n\n\n\n\n\n\nTo Do\nFollow these instructions to install library(tinytex).\n\n\n\nThis can also happen, btw!",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#quarto",
    "href": "latex-quarto.html#quarto",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a literate statistical programming tool.\nQuarto can include code from not just R, but also Python, Julia, Stata and many other languages/tools.\nQuarto allows you to include the good coding guidelines that we discussed above. It provides you with capability to write code and perform data analysis using R, write text that is part of any professional communication, and include mathematical symbols and equations in a well typeset format. Essentially, it allows you to work on a manuscript with data analysis at one place.\nHere is some cool stuff that you can do with quarto.\n\n\n\n\n\n\nExercise 1\n\nOpen a new quarto document by File &gt; New File &gt; Quarto Document.\nUse Render button on top on scripts panel to save and get a .pdf output.\n\n\n\n\nA Quarto document is saved as a .qmd file. You can edit this file in two ways: Programmatically by being in source button and visually by choosing the Visual button, both button on top left corner of the .qmd window. More details about workign with Quarto can be found on the quarto website here.\nThere are three building blocks in a .qmd file:\n\n\nYAML\nShort for Yet-Another-Markup-Languge\nThis is the part we see sandwiched between two --- at the strat of .qmd file. Here we define different global settings for the particular document.\nCurrently, we see\n---\ntitle: \"Untitled\"\nformat: html\n---\nWe can add many more options here to modify the details to appear at the start of the document. Hereâ€™s an example from quarto reference site\n\n\n\n\n\n\n---\ntitle: \"Toward a Unified Theory of High-Energy Metaphysics\"\ndate: 2008-02-29\nauthor:\n  - name: Josiah Carberry\n    id: jc\n    orcid: 0000-0002-1825-0097\n    email: josiah@psychoceramics.org\n    affiliation: \n      - name: Brown University\n        city: Providence\n        state: RI\n        url: www.brown.edu\nabstract: &gt; \n  The characteristic theme of the works of Stone is \n  the bridge between culture and society. ...\nkeywords:\n  - Metaphysics\n  - String Theory\nlicense: \"CC BY\"\ncopyright: \n  holder: Josiah Carberry\n  year: 2008\ncitation: \n  container-title: Journal of Psychoceramics\n  volume: 1\n  issue: 1\n  doi: 10.5555/12345678\nfunding: \"The author received no specific funding for this work.\"\n---\n\n\n\nOr, global settings for different formats of outputs like html or pdf, as follows\n\n\n\n\n\n\n---\ntitle: \"My Document\"\nformat: \n  html:\n    fig-width: 8\n    fig-height: 6\n  pdf:\n    fig-width: 7\n    fig-height: 5\n---\n\n\n\n\n\nCode Chunks\nYou can start a new R code chunk by pressing cmd + option + I or ctrl + alt + I.\nYou can also do this with the Insert button icon in the editor toolbar or by manually typing the chunk delimiters ```{r} and ```.\nTry to use the keyboard shortcut more often as it will save you a ton of time later.\nR code chunks are surrounded by ```{r} and ```.\nYou can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter.\n#| eval: true # Do evaluate this chunk\n#| echo: true # Do show this chunk in the final rendered document\n#| output: true # Do show the output / results of this chunk in the rendered document\n\nprint(\"Dont run this code\")\nRStudio executes the code and displays the results below the code.\nIf you donâ€™t like seeing your plots and output in your document and would rather make use of RStudioâ€™s Console and Plot panes, you can click on the gear icon next to â€œRenderâ€ and switch to â€œChunk Output in Consoleâ€.\nA chunk should be relatively self-contained, and focused around a single task.\n\n\n\n\n\n\nExercise\n\nAdd a code chunk at the bottom of the .qmd file you created.\nAdd some simple mathematical operations.\nRun the code chunk separately, and then the whole file by pressing Render button from the top.\n\n\n\n\nCode chunk options are included in a special comment at the top of the block (lines at the top prefaced with #| are considered options). More on code chunk options here\nOptions available for customizing output include:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output).\n\n\necho\nInclude the source code in output\n\n\noutput\nInclude the results of executing the code in the output (true, false, or asis to indicate that the output is raw markdown and should not have any of Quartoâ€™s standard enclosing markdown).\n\n\nwarning\nInclude warnings in the output.\n\n\nerror\nInclude errors in the output (note that this implies that errors executing code will not halt processing of the document).\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g.Â include: false suppresses all output from the code block).\n\n\n\nYou can also add these options as global options in the YAML by writing them under execute option like:\n---\nexecute: \n  echo: true\n  inlcude: false\n---\nThe following table summarizes which types of output each option suppresses:2\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nRun code\nShow code\nOutput\nPlots\nMessages\nWarnings\n\n\n\n\neval: false\nX\n\nX\nX\nX\nX\n\n\ninclude: false\n\nX\nX\nX\nX\nX\n\n\necho: false\n\nX\n\n\n\n\n\n\nresults: hide\n\n\nX\n\n\n\n\n\nfig-show: hide\n\n\n\nX\n\n\n\n\nmessage: false\n\n\n\n\nX\n\n\n\nwarning: false\n\n\n\n\n\nX\n\n\n\n\nInline code\nWe can also embed R code into a Quarto document: directly into the text, with: ```{r} &lt;code&gt; ```.\nFor example: ```{r} (2+2)```.\n\n\n\nMarkdown Text\nMarkdown text is like any other text just with some special considerations.\nYou can see the help section from R to see some of the basic formatting tips.\n\n\n\nR Markdown Help\n\n\n\n\n\n\n\n\nThese are some of the regularly used formatting options in RMarkdown/Quarto Titles and subtitles â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n# Title 1\n\n## Title 2\n\n### Title 3\n\n\nText formatting \n------------------------------------------------------------\n\n*italic*  \n\n**bold**   \n\n`code`\n\nLists\n------------------------------------------------------------\n\n* Bulleted list item 1\n* Item 2\n  * Item 2a\n  * Item 2b\n\n1. Item 1\n2. Item 2\n\nLinks and images\n------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nPractice\nLetâ€™s try all that we learnt\n\nExercise\n\nDelete the existing code, except yaml on top, in the .qmd file that we created today.\nAdd some simple mathematical operations like addition, subtraction, or mutliplication. Now, in the chunk set the options differently. You could play with different options that we learnt above and their values. Use &lt;TAB&gt; button to see different values that you can provide to chunk options.\n\n```{r}\n#| echo: true\n#| output: asis\n\n1 + 1\n```\n[1] 2\n\nAdd two separate R chunks. In one, load the datset from the paper that you want to replicate. In second, add a simple select or filter functionality.\n\n```{r}\n#| echo: false \n#| message: false\n#| warning: false\n\n# Loading packages\nlibrary(tidyverse) # For tidyverse\nlibrary(janitor) # For Janitor\n\n# Loading Dataset\nvdem_df &lt;- readRDS(\"Datasets-mathcamp/V-Dem-CY-Full+Others-v12.rds\") %&gt;% \nclean_names() \n\n# ` %&gt;% ` is the piping operator from tidyverse universe\n\n# `clean_names` cleans the names of columns and standardizes them | from Janitor package\n```\n```{r}\n#| echo: false  # Toggle with options as well the paramters like true/false etc\n#| message: false\n#| warning: false\nvdem_2021 &lt;- vdem_df %&gt;% \n  filter(year == 2021) %&gt;%  # To filter values according to one variable\n  select(year, country_name, v2x_libdem, e_gdppc, e_pop) # To select particular variables\n```\n\nWrite the model specification that is mentioned in your paper in \\(\\LaTeX\\) in your quarto document. An example is given below. You can use the resources on latex from course Canvas page.\n\n\nModel EquationModel Latex Code\n\n\n\\(LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i\\)\n\n\n$LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i$\n\n\n\n\n\nRender the whole document into a .pdf with your name in the YAML and todayâ€™s date. You may use the following YAML with modifications.\n\n\n\n\n\n\nBelow is a yaml that you can use in your assignments and documents with modifications.\n---\ntitle: My First Latex Document\nsubtitle: Govt 8003\nauthor: &lt;Your Name&gt;\ndate: today\nformat:\n  pdf:\n    highlight-style: kate\n    citation_package: natbib\n  docx: default\nalways_allow_html: true\ngeometry: margin=1.2in\nfontsize: 12pt\nlinestretch: 1.5\nlinkcolor: blue\ntoc: true\nlink-citations: true\neditor_options: \n  chunk_output_type: console\nexecute: \n  fig-height: 6\n  fig-width: 8.5\n  fig-pos: \"!t\"\nkeep_tex: true\nwhitespace: small\n---\n\nâ€œIf you think your thought is not making sense, write it in** \\(\\LaTeX\\).\n\n\nIt will at now not make sense in a beautiful way\n\n\n-Buddha (500 B.C.E.)",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#exercise",
    "href": "latex-quarto.html#exercise",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Exercise",
    "text": "Exercise\n\nDelete the existing code, except yaml on top, in the .qmd file that we created today.\nAdd some simple mathematical operations like addition, subtraction, or mutliplication. Now, in the chunk set the options differently. You could play with different options that we learnt above and their values. Use &lt;TAB&gt; button to see different values that you can provide to chunk options.\n\n```{r}\n#| echo: true\n#| output: asis\n\n1 + 1\n```\n[1] 2\n\nAdd two separate R chunks. In one, load the datset from the paper that you want to replicate. In second, add a simple select or filter functionality.\n\n```{r}\n#| echo: false \n#| message: false\n#| warning: false\n\n# Loading packages\nlibrary(tidyverse) # For tidyverse\nlibrary(janitor) # For Janitor\n\n# Loading Dataset\nvdem_df &lt;- readRDS(\"Datasets-mathcamp/V-Dem-CY-Full+Others-v12.rds\") %&gt;% \nclean_names() \n\n# ` %&gt;% ` is the piping operator from tidyverse universe\n\n# `clean_names` cleans the names of columns and standardizes them | from Janitor package\n```\n```{r}\n#| echo: false  # Toggle with options as well the paramters like true/false etc\n#| message: false\n#| warning: false\nvdem_2021 &lt;- vdem_df %&gt;% \n  filter(year == 2021) %&gt;%  # To filter values according to one variable\n  select(year, country_name, v2x_libdem, e_gdppc, e_pop) # To select particular variables\n```\n\nWrite the model specification that is mentioned in your paper in \\(\\LaTeX\\) in your quarto document. An example is given below. You can use the resources on latex from course Canvas page.\n\n\nModel EquationModel Latex Code\n\n\n\\(LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i\\)\n\n\n$LiberalDemocracy_i = \\alpha + \\beta_1GDPpc_i + epislon_i$\n\n\n\n\n\nRender the whole document into a .pdf with your name in the YAML and todayâ€™s date. You may use the following YAML with modifications.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#r-projects",
    "href": "latex-quarto.html#r-projects",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "R Projects",
    "text": "R Projects\nWe often use the setwd() command to trace the files we need in our work. As work expands, projects will have multiple datasets to be loaded, different subsidiary scripts to be used, and multiple outputs to be saved.\nA first order problem related to both file management and reproducability of code is the usage of file paths. Using absolute paths, like ~/User/MyName/Documents/..... becomes cumbersome and also inhibits efficiency of reproducability. Every time someone else runs the script, they will have to change the file paths in all the instances in Rscripts or .qmd file to locate the related datasets as well as other objects. Similarly, there would be issues with saving objects in new places. A partially efficient way we use involves using setwd() to direct R to a new working directory; this is also called usage of relative paths\nR Projects is a built-in mechanism in RStudio for seamless file management and usage of relative paths.\nLetâ€™s start by creating a new project. Click File &gt; New Project. Name the new project govt-8003.\n\n\n\n\n\n\n\n\nFigureÂ 1: To create new project: (top) first click New Directory, then (middle) click New Project, then (bottom) fill in the directory (project) name, choose a good subdirectory for its home and click Create Project. source\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\nClose the new project just created\nGo to the folder on your system, and click the .RProj file.\nStart a new qmd file like we did before. Delete existing code except for YAML. Run getwd() command in console and see the difference.\nStart a new R code chunk (cmd + option + I) and load replication dataset. Notice the change in behavior when you press TAB inside the quotes for selecting path.\n\n\n\n\n\nhere package\nAn efficient file and folder management system is going to be crucial as we move into working with serious projects. As stressed earlier, keeping and using all the files associated with a project in a comprehensible folder system is facilitated by R Projects. You would ideally want to create your own template for folder management that you follow across projects. For starters, the folder structure below is the one created for your data essay assignment in Govt 8001 or Quant 1.\nYou can use the point-and-click functionality in your computers to create this structure. Later today, we will briefly go through an R script that do this programmatically.\nğŸ“¦ govt-8003\nâ”œâ”€Â govt-8003.RProj\nâ”œâ”€Â 000-setup.R\nâ”œâ”€Â 001-eda.qmd\nâ”œâ”€Â 002-analysis.qmd\nâ””â”€Â 003-manuscript.qmd\nâ”œâ”€Â Data\nâ”‚Â Â â”œâ”€Â Raw\nâ”‚Â Â â”‚Â Â â”œâ”€Â Dataset1\nâ”‚Â Â â”‚Â Â â”‚Â Â â”œâ”€Â dataset1.csv\nâ”‚Â Â â”‚Â Â â”‚Â Â â””â”€Â codebook-dataset1.pdf\nâ”‚Â Â â”‚Â Â â””â”€Â Dataset2\nâ”‚Â Â â”‚Â Â Â Â Â â”œâ”€Â ...dta\nâ”‚Â Â â”‚Â Â Â Â Â â””â”€Â codebook-dataset2.pdf\nâ”‚Â Â â””â”€Â Clean\nâ”‚Â Â Â Â Â â””â”€Â Merged-df1-df2.csv\nâ”œâ”€Â Scripts\nâ”‚Â Â â”œâ”€Â R-scripts\nâ”‚Â Â â”‚Â Â â”œâ”€Â plotting-some-variable.R\nâ”‚Â Â â”‚Â Â â””â”€Â exploring-different-models.R\nâ”‚Â Â â”œâ”€Â Stata-Scripts\nâ”‚Â Â â”‚Â Â â””â”€Â seeing-variable-labels.do\nâ”‚Â Â â””â”€Â Python-Scripts\nâ”‚Â Â Â Â Â â””â”€Â scraping-data-from-website.py\nâ””â”€Â Outputs\nÂ Â Â â”œâ”€Â Plots\nÂ Â Â â”‚Â Â â”œâ”€Â ...jpeg\nÂ Â Â â”‚Â Â â””â”€Â ...png\nÂ Â Â â”œâ”€Â Tables\nÂ Â Â â”‚Â Â â””â”€Â .csv\nÂ Â Â â””â”€Â Text\nÂ Â Â Â Â Â â””â”€Â ...txt\nSuggested folder structure for a new academic project\nWhile we learnt how to create or associate an .RProj with a folder, integrating it with here() function from the here package, makes things further smoother. Letâ€™s do it with the following exercise.\n\n\n\n\n\n\nExercise\n\nGo the RStudio window with govt-8003 project. Check the extreme upper left corner to see if you are in the correct window.\nIn the qmd file we were working in, add an R chunk.\nLoad the library here with the following code. Run the code line by line\n\n\nlibrary(here)\n\n\n # See the output for each of the following lines | Use your own datasets\nhere()\n\n# Make modification here after copying your dataset to this folder\n\nhere(\"Datasets-mathcamp\",\"V-Dem-CY-Full+Others-v12.rds\")\n\n# syntax is\n\n# here(\"First subfolder from the root folder\", \"second subfolder\",...., \"file\")\n\n\nvdem_new &lt;- readRDS(here(\"Datasets-mathcamp\",\"V-Dem-CY-Full+Others-v12.rds\"))\n\nThis is a cleaner syntax which when coupled with usage of R projects saves time in typing file paths and avoids issues when the project is run on some other computer system.\nNote: here() always notes the path from the main folder or the root directory where your .RProj file is located.\nSave the files and close the govt-8003 project window\n\n\n\nMake it a habit of using R Projects and here() function in your scripts for writing portable code.\nYou can read this quick and informative blogpost on using these two here.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#folder-structure",
    "href": "latex-quarto.html#folder-structure",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Folder Structure",
    "text": "Folder Structure\nWe ideally want a folder structure that is easily understandable to us and others.\nğŸ“¦ govt-8003\nâ”œâ”€Â govt-8003.RProj\nâ”œâ”€Â 000-setup.R\nâ”œâ”€Â 001-eda.qmd\nâ”œâ”€Â 002-analysis.qmd\nâ””â”€Â 003-manuscript.qmd\nâ”œâ”€Â Data\nâ”‚Â Â â”œâ”€Â Raw\nâ”‚Â Â â”‚Â Â â”œâ”€Â Dataset1\nâ”‚Â Â â”‚Â Â â”‚Â Â â”œâ”€Â dataset1.csv\nâ”‚Â Â â”‚Â Â â”‚Â Â â””â”€Â codebook-dataset1.pdf\nâ”‚Â Â â”‚Â Â â””â”€Â Dataset2\nâ”‚Â Â â”‚Â Â Â Â Â â”œâ”€Â ...dta\nâ”‚Â Â â”‚Â Â Â Â Â â””â”€Â codebook-dataset2.pdf\nâ”‚Â Â â””â”€Â Clean\nâ”‚Â Â Â Â Â â””â”€Â Merged-df1-df2.csv\nâ”œâ”€Â Scripts\nâ”‚Â Â â”œâ”€Â R-scripts\nâ”‚Â Â â”‚Â Â â”œâ”€Â plotting-some-variable.R\nâ”‚Â Â â”‚Â Â â””â”€Â exploring-different-models.R\nâ”‚Â Â â”œâ”€Â Stata-Scripts\nâ”‚Â Â â”‚Â Â â””â”€Â seeing-variable-labels.do\nâ”‚Â Â â””â”€Â Python-Scripts\nâ”‚Â Â Â Â Â â””â”€Â scraping-data-from-website.py\nâ””â”€Â Outputs\nÂ Â Â â”œâ”€Â Plots\nÂ Â Â â”‚Â Â â”œâ”€Â ...jpeg\nÂ Â Â â”‚Â Â â””â”€Â ...png\nÂ Â Â â”œâ”€Â Tables\nÂ Â Â â”‚Â Â â””â”€Â .csv\nÂ Â Â â””â”€Â Text\nÂ Â Â Â Â Â â””â”€Â ...txt\nWe can create this structure by using point and click system on our laptops. But since we might want to use the same folder structure repetitively it will make sense to be lazy and do it programmatically.\n\n\n\n\n\n\nExercise\n\nDownload the 000-setup.R from here\nPlace it in the govt-8003 folder.\nOpen it in the opened RStudio window.\n\n```{r}\n# Name: 000-setup.R\n# Author: Parushya\n# Purpose: Creates main folders, subfolders in the main project directory\n# Will also ensure that you have basic packages required to run the repository\n# Date Created: 2020/10/07\n\n\n\n# Checking if packages are installed and installing\n\n\n# check.packages function: install and load multiple R packages.\n# Found this function here: https://gist.github.com/smithdanielle/9913897 on 2019/06/17\n# Check to see if packages are installed. Install them if they are not, then load them into the R session.\n\ncheck.packages &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n  sapply(pkg, require, character.only = TRUE)\n}\n\n# Check if packages are installed and loaded:\npackages &lt;- c(\"janitor\",  \"tidyverse\", \"utils\", \"here\")\ncheck.packages(packages)\n\n\n# Setting Directories and creating subfolders\n\n\n# Creating Sub Folders\n\n## Data\ndir.create(file.path(paste0(here(\"Data\")))) # Data Folder\ndir.create(file.path(paste0(here(\"Data\",\"Raw\")))) # Raw Data sub-folder\ndir.create(file.path(paste0(here(\"Data\",\"Clean\")))) # Clean Data sub-folder\n\n\n# Scripts\ndir.create(file.path(paste0(here(\"Scripts\")))) # Scripts Folder\ndir.create(file.path(paste0(here(\"Scripts\",\"RScripts\")))) # RScripts  sub-folder\ndir.create(file.path(paste0(here(\"Scripts\",\"Stata-Scripts\")))) # Stata Scripts sub-folder\ndir.create(file.path(paste0(here(\"Scripts\",\"Python-Scripts\")))) # Python Scripts sub-folder\n\n\n# Output\ndir.create(file.path(paste0(here(\"Outputs\")))) # Outputs Folder\ndir.create(file.path(paste0(here(\"Outputs\",\"figures\")))) # Figures sub-folder\ndir.create(file.path(paste0(here(\"Outputs\",\"tables\")))) # Tables sub-folder\ndir.create(file.path(paste0(here(\"Outputs\",\"text\")))) # Text sub-folder\n\n```\n\nRun the file line-by-line. See the folder structure created in your main folder.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#plan-concept-of-a-plan",
    "href": "latex-quarto.html#plan-concept-of-a-plan",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "Plan Concept of a Plan",
    "text": "Plan Concept of a Plan\nHereâ€™s a quick workflow for starting a new project or assignment or paper.\n\nCreate a new Rstudio Project by clicking File &gt; New Project. Name it govt-&lt;coursecode&gt;-&lt;project.\nCheck if now your RStudio Window shows the project name on top right corner. If not, go to folder and double-click the .RProj file.\nPaste the 000-setup.R file in the main project folder. Open it in the same Rstudio window with the project and run the complete file. Your folder structure is created.\nCopy your raw data in Data/Raw folder. Similarly, your scripts in Scripts/RScripts folder\nStart your new .qmd file and save it in the main folder.\nRemember to use here() package extensively in both, scripts and quarto file, when loading or saving the data.\nYou can always zip the whole project folder for sharing. The receiver will just need to unzip and run the code after starting the associated .RProj file, without changing file paths on their computer.",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "latex-quarto.html#footnotes",
    "href": "latex-quarto.html#footnotes",
    "title": "Session 1 - \\(\\LaTeX\\) and Quarto",
    "section": "",
    "text": "Inspired by the summary provided by Prof Aaron Williamsâ€™ course on Data Analysis offered at McCourt School. Strongly recommended to learn good coding using Râ†©ï¸\nThis section is copied from R4DS bookâ†©ï¸",
    "crumbs": [
      "Session 1 - $\\LaTeX$ and Quarto"
    ]
  },
  {
    "objectID": "matching.html",
    "href": "matching.html",
    "title": "Matching",
    "section": "",
    "text": "Building the intuition: Stratification or Subclassification\nFor approaching the causal effects, in the language of DAGs, we have to close all the backdoor paths between our treatment and outcome of interest. Letâ€™s see that with an illustrative example from Causal Inference: The Mixtape (Cunningham, n.d.)\nResearch Question: Would being in First Class in titanic have increased the chances of survival? As a causal effect can we isolate it?\nFirst class was located higher on the ship and therefore likely led to a greater chance of getting into one of the few lifeboats onboard. We could estimate the ATE of being in first class like this:\n# Function to load data from Causal Mixtape github page\nread_data &lt;- function(df)\n{\n  full_path &lt;- paste(\"https://github.com/scunning1975/mixtape/raw/master/\", \n                     df, sep = \"\")\n  print(full_path) # To show what was the final url accessed\n  df &lt;- read_dta(full_path)\n  return(df)\n}\n# load data and create a variable \"d\" for individuals in first class\ntitanic &lt;- read_data(\"titanic.dta\") %&gt;% \n  mutate(d = case_when(class == 1 ~ 1, TRUE ~ 0))\n\n[1] \"https://github.com/scunning1975/mixtape/raw/master/titanic.dta\"\n\n# calculate expected outcome for the treatment group - first class \ney1 &lt;- titanic %&gt;% \n  filter(d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\ney1\n\n[1] 0.6246154\n\n# calculate expected outcome for the control group - all other classes onboard \ney0 &lt;- titanic %&gt;% \n  filter(d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\ney0\n\n[1] 0.2707889\n\n# calculate the simple difference in outcomes \n(sdo &lt;- ey1 - ey0)\n\n[1] 0.3538265\nSay in our research, we learn that women and children were more likely to be in first class because only wealthy families could afford this kind of travel. Additionally, the social norm during the boarding of the lifeboats was that women and children got first priority. Now we have a problem because being a woman or a child are confounders, which means that our estimate of the ATE is biased.\nWe can handle this via stratification by a bit of data wrangling\n# create a variable to indicate each of our strata - two for sex (male = 1), and two for age(adult = 1)\ntitanic = titanic %&gt;%\n  mutate(s = case_when(sex == 0 & age == 1 ~ 1,\n                       sex == 0 & age == 0 ~ 2,\n                       sex == 1 & age == 1 ~ 3,\n                       sex == 1 & age == 0 ~ 4,\n                       TRUE ~ 0))\n# create treatment variable for those in first class\ntitanic &lt;- titanic %&gt;% \n  mutate(d = case_when(class == 1 ~ 1, TRUE ~ 0))\n\n# calculate survival rate for each pair of strata and treatment\ney11 &lt;- titanic %&gt;% \n  filter(s == 1 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney10 &lt;- titanic %&gt;% \n  filter(s == 1 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney21 &lt;- titanic %&gt;% \n  filter(s == 2 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney20 &lt;- titanic %&gt;% \n  filter(s == 2 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney31 &lt;- titanic %&gt;% \n  filter(s == 3 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney30 &lt;- titanic %&gt;% \n  filter(s == 3 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney41 &lt;- titanic %&gt;% \n  filter(s == 4 & d == 1) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\ney40 &lt;- titanic %&gt;% \n  filter(s == 4 & d == 0) %&gt;%\n  pull(survived) %&gt;% \n  mean()\n\n# calculate differences between strata\ndiff1 = ey11 - ey10\ndiff2 = ey21 - ey20\ndiff3 = ey31 - ey30\ndiff4 = ey41 - ey40\n\n# calculate number of control units \nobs = nrow(titanic %&gt;% filter(d == 0))\n\n# create weights for each strata\nwt1 &lt;- titanic %&gt;% \n  filter(s == 1 & d == 0) %&gt;%\n  nrow(.)/obs\n\nwt2 &lt;- titanic %&gt;% \n  filter(s == 2 & d == 0) %&gt;%\n  nrow(.)/obs\n\nwt3 &lt;- titanic %&gt;% \n  filter(s == 3 & d == 0) %&gt;%\n  nrow(.)/obs\n\nwt4 &lt;- titanic %&gt;% \n  filter(s == 4 & d == 0) %&gt;%\n  nrow(.)/obs\n\n# calculate weighted average treatment effect\n(wate = diff1*wt1 + diff2*wt2 + diff3*wt3 + diff4*wt4)\n\n[1] 0.1887847",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#exact-matching",
    "href": "matching.html#exact-matching",
    "title": "Matching",
    "section": "Exact Matching",
    "text": "Exact Matching\nExample from Causal Inference: The Mixtape by Scott Cunningham.\nDo training programs have a causal effect on earnings?.\nBut, in this very simple example, we notice that individuals who participated in the training program are younger on average than those in the control group (What type of bias is this?).\nThis violates the conditional independence assumption. We can address this using exact matching on age as follows:\n\n# load data \ntraining_example &lt;- read_data(\"training_example.dta\") %&gt;% \n  slice(1:20)\n\n[1] \"https://github.com/scunning1975/mixtape/raw/master/training_example.dta\"\n\n\n```{r}\n#| echo: true\n#| output: asis\n\n# view data table to see what matching on age looks like \nView(training_example)\n```\nThis dataset is for illustration where we have exact matches for all units in the treated group on the covariate of age from the control group.\n\n\n\n\n\n\n\n\nLetâ€™s see the distribution of covariate age in the two groups.\n::: {.panel-tabset} ### Treatment Group\n\n# histogram of age for treatment group - mean at red line\nggplot(training_example, aes(x=age_treat)) +\n  geom_histogram(bins = 10, na.rm = TRUE) +\n  geom_vline(aes(xintercept = mean(age_treat, na.rm = T)), color = \"red\")\n\n\n\n\n\n\n\n\n\nControl Group\n\n# histogram of age for control group - mean at red line\nggplot(training_example, aes(x=age_control)) +\n  geom_histogram(bins = 10, na.rm = TRUE) +\n  geom_vline(aes(xintercept = mean(age_control, na.rm = T)), color = \"red\")\n\n\n\n\n\n\n\n\n::: \nAfter exact matching\n\n# histogram of age for matched group - mean at red line\nggplot(training_example, aes(x=age_matched))+\n  geom_histogram(bins = 10, na.rm = TRUE) +\n  geom_vline(aes(xintercept = mean(age_matched, na.rm = T)), color = \"red\")\n\n\n\n\n\n\n\n# calculate mean wages for treatment group \ntrained = training_example %&gt;% \n  filter(!is.na(age_treat)) %&gt;% \n  mutate(earnings_treat = as.numeric(earnings_treat)) %&gt;% \n  pull(earnings_treat) %&gt;% \n  mean()\n\n# calculate mean wages for matched control group \nnontrained = training_example %&gt;% \n  filter(!is.na(age_matched)) %&gt;% \n  mutate(earnings_matched = as.numeric(earnings_matched)) %&gt;% \n  pull(earnings_matched) %&gt;% \n  mean()\n\natt_train = trained - nontrained # Notice - This is the ATT \natt_train\n\n[1] 1695",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#propensity-score-matching",
    "href": "matching.html#propensity-score-matching",
    "title": "Matching",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\nReplication data for Samuels and Shugartâ€™s book â€œPresidents, Parties and Prime Ministers: How the Separation of Powers Affects Party Organization and Behaviorâ€ Cambridge University press, 2010 (Data extended by students from U-Mich).\nTreatment here is the type of system i.e.Â Presidential or Parlimentary.\n\n# load data\ndata &lt;- read_csv(\"data/matching_data.csv\")\n\n# summarize the data\nsummary(data$purepres) # treatment\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.3068  1.0000  1.0000 \n\nsummary(data$pureparl) # control\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.4205  1.0000  1.0000 \n\n\n\nExploratory Data Analysis\n\n# create a table to see the count of each type of system in our data set\ndata &lt;- data %&gt;% \n  mutate(\n    type = case_when( purepres==1 ~ \"PRES\",\n                      pureparl==1 ~ \"PARL\",\n                      presparlpres==1 | premiprespres==1 |presparlpm == 1 ~ \"MIXED\",\n                      TRUE ~ NA\n                      )\n  )\n\n# Checking Distribution\ntable(data$type)\n\n\nMIXED  PARL  PRES \n    9    37    27 \n\n\n\n\n\n\n\n\nLetâ€™s check the distribution stratified by continents\n```{r}\n\n(data %&gt;% select(type, continent, damerica) %&gt;% \n  group_by(continent, type) %&gt;% \n  summarise(n = n())\n)\n```\n\n\n\n\n\n\n\n\n\nWhy might this distribution create a problem? (Hint: Think about balance/overlap)\n\nWrangling DatasetSummary of Data\n\n\n\n# create dataset of covariates of interest\n\n\nX &lt;- data %&gt;% \n  select(indviol, year_indep, dindep_decol, dindep_seces,\n               chrstprotpct, chrstcatpct, judgenpct,\n               islmgenpct, budgenpct,\n               a_ethnic, a_ling, a_relig,\n               gini,\n               land_kilometers,\n               life_expectency_1800, gdp_per_capita_1800,\n               density,\n               damerica, dsouthamerica, deurope, dafrica,\n               dasia, doceania,\n               colony_esp, colony_gbr, colony_fra, colony_prt, colony_oeu,\n               rugged\n  )\n\n\n# rename the variables \ncolnames(X) = c(\"Violent Independence\", \"Year of Independence\", \"Independency by Decolonization\",\n                \"Independence by Secession\", \"Christian Protestant Pct\", \"Christian Catholic Pct\", \n                \"Jewish Pct\", \"Islam Pct\", \"Budhist Pct\",\n                \"Ethic Fractionalization\", \"Linguistic Fractionalization\", \"Religious Fractionalization\",\n                \"Gini\",\n                \"Land (km)\",\n                \"Life expectancy 1800\", \"GDP per cap 1800\",\n                \"Population density\",\n                \"Country in America\", \"Country in South America\", \"Country in Europe\", \"Country in Africa\", \"Country in Asia\", \"Country in Oceania\",\n                \"Spanish colony\", \"British colony\", \"France colony\", \"Portugal colony\", \"OEU colony\",\n                \"Ruggedness\")\n\n```{r}\n#we summarize X\nsummary(X)\n```\n\n\n\n#we summarize X\nsummary(X)\n\n Violent Independence Year of Independence Independency by Decolonization\n Min.   :0.0000       Min.   : 943         Min.   :0.0000                \n 1st Qu.:0.0000       1st Qu.:1827         1st Qu.:0.0000                \n Median :1.0000       Median :1918         Median :1.0000                \n Mean   :0.5568       Mean   :1843         Mean   :0.5114                \n 3rd Qu.:1.0000       3rd Qu.:1960         3rd Qu.:1.0000                \n Max.   :1.0000       Max.   :1993         Max.   :1.0000                \n                                                                         \n Independence by Secession Christian Protestant Pct Christian Catholic Pct\n Min.   :0.0000            Min.   :0.0000           Min.   :0.00000       \n 1st Qu.:0.0000            1st Qu.:0.0015           1st Qu.:0.00905       \n Median :0.0000            Median :0.0181           Median :0.17000       \n Mean   :0.2727            Mean   :0.1179           Mean   :0.38277       \n 3rd Qu.:1.0000            3rd Qu.:0.0735           3rd Qu.:0.88380       \n Max.   :1.0000            Max.   :0.9900           Max.   :0.98040       \n                           NA's   :1                NA's   :1             \n   Jewish Pct        Islam Pct       Budhist Pct      Ethic Fractionalization\n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.001998       \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.181835       \n Median :0.00020   Median :0.0022   Median :0.00000   Median :0.406181       \n Mean   :0.01297   Mean   :0.1309   Mean   :0.03390   Mean   :0.395852       \n 3rd Qu.:0.00205   3rd Qu.:0.1146   3rd Qu.:0.00035   3rd Qu.:0.591322       \n Max.   :0.87530   Max.   :0.9947   Max.   :0.88540   Max.   :0.879100       \n NA's   :1         NA's   :1        NA's   :1                                \n Linguistic Fractionalization Religious Fractionalization      Gini      \n Min.   :0.002113             Min.   :0.002755            Min.   :19.40  \n 1st Qu.:0.087104             1st Qu.:0.220886            1st Qu.:36.50  \n Median :0.303091             Median :0.408493            Median :45.10  \n Mean   :0.338181             Mean   :0.423653            Mean   :43.65  \n 3rd Qu.:0.546679             3rd Qu.:0.615734            3rd Qu.:51.70  \n Max.   :0.873408             Max.   :0.860260            Max.   :73.90  \n NA's   :1                                                NA's   :7      \n   Land (km)        Life expectancy 1800 GDP per cap 1800 Population density \n Min.   :    2030   Min.   :25.10        Min.   : 368     Min.   :  0.02199  \n 1st Qu.:   60625   1st Qu.:30.16        1st Qu.: 740     1st Qu.:  1.13435  \n Median :  228780   Median :32.90        Median : 976     Median :  8.99563  \n Mean   :  964830   Mean   :32.67        Mean   :1099     Mean   : 15.92361  \n 3rd Qu.:  656352   3rd Qu.:35.73        3rd Qu.:1442     3rd Qu.: 19.66593  \n Max.   :16376870   Max.   :40.00        Max.   :2892     Max.   :121.29887  \n                                         NA's   :1                           \n Country in America Country in South America Country in Europe\n Min.   :0.00       Min.   :0.0000           Min.   :0.0000   \n 1st Qu.:0.00       1st Qu.:0.0000           1st Qu.:0.0000   \n Median :0.00       Median :0.0000           Median :0.0000   \n Mean   :0.25       Mean   :0.1136           Mean   :0.3409   \n 3rd Qu.:0.25       3rd Qu.:0.0000           3rd Qu.:1.0000   \n Max.   :1.00       Max.   :1.0000           Max.   :1.0000   \n                                                              \n Country in Africa Country in Asia  Country in Oceania Spanish colony  \n Min.   :0.0000    Min.   :0.0000   Min.   :0.00000    Min.   :0.0000  \n 1st Qu.:0.0000    1st Qu.:0.0000   1st Qu.:0.00000    1st Qu.:0.0000  \n Median :0.0000    Median :0.0000   Median :0.00000    Median :0.0000  \n Mean   :0.1591    Mean   :0.2045   Mean   :0.04545    Mean   :0.2152  \n 3rd Qu.:0.0000    3rd Qu.:0.0000   3rd Qu.:0.00000    3rd Qu.:0.0000  \n Max.   :1.0000    Max.   :1.0000   Max.   :1.00000    Max.   :1.0000  \n                                                       NA's   :9       \n British colony   France colony     Portugal colony     OEU colony     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.2785   Mean   :0.05063   Mean   :0.02532   Mean   :0.02532  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n NA's   :9        NA's   :9         NA's   :9         NA's   :9        \n   Ruggedness    \n Min.   :0.0370  \n 1st Qu.:0.4255  \n Median :1.0130  \n Mean   :1.2591  \n 3rd Qu.:1.7745  \n Max.   :5.0430  \n NA's   :9       \n\n\n\n\n\n\n\n\nChecking Balance of Covariates\n\n# balance function to calculate normalized distance\n# Look through this function in detail what each of the arguments are doing\n\n\nbalance &lt;- function(x, Tr, xname, alpha=0.05) {\n  Tr = Tr[!is.na(x)]\n  x  =  x[!is.na(x)]     \n  mut = mean(x[Tr==1])\n  muc = mean(x[Tr==0])\n  s2t  = var(x[Tr==1])\n  s2c  = var(x[Tr==0])    \n  delta = (mut - muc)/sqrt((s2t+s2c)/2) # this is the normalized difference\n  \n  ql = quantile(x[Tr==0], probs = alpha/2) # lower tail for control\n  qh = quantile(x[Tr==0], probs = 1-alpha/2) # upper tail for control\n  pi = sum(x[Tr==1] &lt; ql)/(length(Tr==1)) + sum(x[Tr==1] &gt; qh)/(length(Tr==1)) # fraction\n  \n  cat(\"------------------------------------------\\n\")    \n  cat(\"Variable \", xname, \"\\n\")\n  cat(\"Mean Treatment (Pres) \", mut, \"\\n\")\n  cat(\"Mean Control (Parl)\", muc, \"\\n\")\n  cat(\"Normalized difference\", delta, \"\\n\")        \n  cat(\"------------------------------------------\\n\")\n  cat(\"Mass of treated distribution in the control tails\", round(pi*100, 2), \"% \\n\")    \n  cat(\"------------------------------------------\\n\")\n  return(list(mut=mut,muc=muc, s2t=s2t, s2c=s2c, delta=delta, pi=pi))\n}\n\n\nCodeResult\n\n\n```{r}\n# (1) Let's look at normalized differences and Mass of Treated Distribution in Control Tailes\n# Also Recall Rubin's rule: if above |0.25|, then OLS likely too sensitive to misspecification\n\n# create vector for treatment index\nTr = data$purepres\n\n# check for balance on treatment\nresult &lt;- imap(X, ~ balance(x = .x, Tr = Tr, xname = .y))\n```\n\n\n\n\n------------------------------------------\nVariable  Violent Independence \nMean Treatment (Pres)  0.6666667 \nMean Control (Parl) 0.5081967 \nNormalized difference 0.3218476 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Year of Independence \nMean Treatment (Pres)  1872.444 \nMean Control (Parl) 1829.754 \nNormalized difference 0.2299536 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Independency by Decolonization \nMean Treatment (Pres)  0.5925926 \nMean Control (Parl) 0.4754098 \nNormalized difference 0.2333725 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Independence by Secession \nMean Treatment (Pres)  0.3703704 \nMean Control (Parl) 0.2295082 \nNormalized difference 0.3066766 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Christian Protestant Pct \nMean Treatment (Pres)  0.04634815 \nMean Control (Parl) 0.1500733 \nNormalized difference -0.5113226 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Christian Catholic Pct \nMean Treatment (Pres)  0.6333444 \nMean Control (Parl) 0.2700083 \nNormalized difference 0.974737 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Jewish Pct \nMean Treatment (Pres)  0.003403704 \nMean Control (Parl) 0.01728167 \nNormalized difference -0.173385 \n------------------------------------------\nMass of treated distribution in the control tails 2.3 % \n------------------------------------------\n------------------------------------------\nVariable  Islam Pct \nMean Treatment (Pres)  0.08145926 \nMean Control (Parl) 0.15313 \nNormalized difference -0.2808659 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Budhist Pct \nMean Treatment (Pres)  0.006037037 \nMean Control (Parl) 0.046435 \nNormalized difference -0.3403212 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Ethic Fractionalization \nMean Treatment (Pres)  0.4554665 \nMean Control (Parl) 0.3694653 \nNormalized difference 0.3641619 \n------------------------------------------\nMass of treated distribution in the control tails 3.41 % \n------------------------------------------\n------------------------------------------\nVariable  Linguistic Fractionalization \nMean Treatment (Pres)  0.3310051 \nMean Control (Parl) 0.3412398 \nNormalized difference -0.03638428 \n------------------------------------------\nMass of treated distribution in the control tails 4.6 % \n------------------------------------------\n------------------------------------------\nVariable  Religious Fractionalization \nMean Treatment (Pres)  0.3815061 \nMean Control (Parl) 0.4423086 \nNormalized difference -0.2694968 \n------------------------------------------\nMass of treated distribution in the control tails 2.27 % \n------------------------------------------\n------------------------------------------\nVariable  Gini \nMean Treatment (Pres)  49.35769 \nMean Control (Parl) 40.95091 \nNormalized difference 0.759161 \n------------------------------------------\nMass of treated distribution in the control tails 1.23 % \n------------------------------------------\n------------------------------------------\nVariable  Land (km) \nMean Treatment (Pres)  1139389 \nMean Control (Parl) 887566 \nNormalized difference 0.1041268 \n------------------------------------------\nMass of treated distribution in the control tails 1.14 % \n------------------------------------------\n------------------------------------------\nVariable  Life expectancy 1800 \nMean Treatment (Pres)  31.2861 \nMean Control (Parl) 33.27707 \nNormalized difference -0.5445587 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  GDP per cap 1800 \nMean Treatment (Pres)  918.2593 \nMean Control (Parl) 1180.317 \nNormalized difference -0.5806425 \n------------------------------------------\nMass of treated distribution in the control tails 1.15 % \n------------------------------------------\n------------------------------------------\nVariable  Population density \nMean Treatment (Pres)  7.215313 \nMean Control (Parl) 19.77811 \nNormalized difference -0.6009871 \n------------------------------------------\nMass of treated distribution in the control tails 2.27 % \n------------------------------------------\n------------------------------------------\nVariable  Country in America \nMean Treatment (Pres)  0.6666667 \nMean Control (Parl) 0.06557377 \nNormalized difference 1.570273 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in South America \nMean Treatment (Pres)  0.3333333 \nMean Control (Parl) 0.01639344 \nNormalized difference 0.9015721 \n------------------------------------------\nMass of treated distribution in the control tails 10.23 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Europe \nMean Treatment (Pres)  0 \nMean Control (Parl) 0.4918033 \nNormalized difference -1.379766 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Africa \nMean Treatment (Pres)  0.1851852 \nMean Control (Parl) 0.147541 \nNormalized difference 0.09979829 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Asia \nMean Treatment (Pres)  0.1481481 \nMean Control (Parl) 0.2295082 \nNormalized difference -0.2063768 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Country in Oceania \nMean Treatment (Pres)  0 \nMean Control (Parl) 0.06557377 \nNormalized difference -0.3715509 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Spanish colony \nMean Treatment (Pres)  0.6666667 \nMean Control (Parl) 0.01818182 \nNormalized difference 1.833951 \n------------------------------------------\nMass of treated distribution in the control tails 20.25 % \n------------------------------------------\n------------------------------------------\nVariable  British colony \nMean Treatment (Pres)  0.2083333 \nMean Control (Parl) 0.3090909 \nNormalized difference -0.2282852 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  France colony \nMean Treatment (Pres)  0.04166667 \nMean Control (Parl) 0.05454545 \nNormalized difference -0.05934487 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n------------------------------------------\nVariable  Portugal colony \nMean Treatment (Pres)  0.04166667 \nMean Control (Parl) 0.01818182 \nNormalized difference 0.1357614 \n------------------------------------------\nMass of treated distribution in the control tails 1.27 % \n------------------------------------------\n------------------------------------------\nVariable  OEU colony \nMean Treatment (Pres)  0.04166667 \nMean Control (Parl) 0.01818182 \nNormalized difference 0.1357614 \n------------------------------------------\nMass of treated distribution in the control tails 1.27 % \n------------------------------------------\n------------------------------------------\nVariable  Ruggedness \nMean Treatment (Pres)  1.204625 \nMean Control (Parl) 1.282836 \nNormalized difference -0.08265628 \n------------------------------------------\nMass of treated distribution in the control tails 0 % \n------------------------------------------\n\n\n\n\n\n\nThe â€œMass of treated distribution in the control tailsâ€ is a percentage that indicates how much of the treated groupâ€™s data lies in the extreme tails of the control groupâ€™s distribution. A high value suggests imbalance between the two groups, as it indicates that the treated group has many values in regions of the distribution that are less typical for the control group.\nWhy Is This Important?\nIn balance checks meant for attribution of effect to treatment, as done under assumpotion of uncoundedness or selection on observables, we want the treated and control groups to be similar in distribution for all covariates (except for the treatment assignment). When thereâ€™s a large portion of the treated group in the tails of the control distribution, it suggests that the two groups are not well balanced for that variable, meaning there may be substantial differences in their distribution. This would raise concerns about whether we can attribute the differences in outcomes to the treatment alone, rather than to the covariates being imbalanced.\n\n\nPropensity Score Matching on Treatment and Control\n\nCreating Propensity ScoresPlot of Propensity ScoresIdeal Overlap Simulation\n\n\n\n# calculate the propensity scores with full data set \npscore&lt;-glm(Tr~as.matrix(X),family=binomial(link=logit)) # why the warnings?\n\n# create smaller X vector - taking out variables like continent and colonial history \n\n\nX2 &lt;- data %&gt;%\n  select(\n    Violent_Independence = indviol,\n    Year_of_Independence = year_indep,\n    Independency_by_Decolonization = dindep_decol,\n    Independence_by_Secession = dindep_seces,\n    Christian_Protestant_Pct = chrstprotpct,\n    Christian_Catholic_Pct = chrstcatpct,\n    Jewish_Pct = judgenpct,\n    Islam_Pct = islmgenpct,\n    Budhist_Pct = budgenpct,\n    Ethnic_Fractionalization = a_ethnic,\n    Linguistic_Fractionalization = a_ling,\n    Religious_Fractionalization = a_relig,\n    gini,\n    Land_kilometers = land_kilometers,\n    Life_expectancy_1800 = life_expectency_1800,\n    GDP_per_capita_1800 = gdp_per_capita_1800,\n    Population_density = density,\n    Country_in_Africa = dafrica,\n    Country_in_Asia = dasia,\n    Ruggedness = rugged\n  )\n\n\n# create index for complete observations and select those \nindx = complete.cases(X2)\n\n# filter our treatment vector on complete cases\nTr2 = Tr[indx]\n\n# calculate propensity score on this subset \npscore&lt;-glm(Tr2 ~ ., data= X2[indx,],family=binomial(link=logit))\n\n# pull out propensity score for graphing\nphat&lt;-pscore$fitted.values\n\n\n\n\n# histogram of propensity scores \nggplot(data.frame(phat), aes(x = phat)) +\n  geom_histogram(binwidth = 0.05, fill = \"darkgreen\", color = \"black\") +\n  labs(title = \"Histogram of PScores\", \n       x = \"Predicted Propensity Scores (phat)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# plot pscore by treatment\nggplot(data)+\n  geom_density(data=data.frame(x=phat[Tr2==1]), aes(x = x, fill=\"Tr\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\")+\n  geom_density(data=data.frame(x=phat[Tr2==0]), aes(x = x, fill=\"Co\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\")+\n  scale_fill_manual(values=c(\"Tr\"=\"blue\", \"Co\"=\"red\"), labels = c(\"Presidential (T = 1) \", \"Parliamentary (T = 0)\"))+\n  theme_bw()+\n  labs(\n    x = \"Probability of Being in Treatment\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nCompare to what the histogram would look under simulated â€œrandom assignmentâ€ if both groups were identical on average\n\n# Generate T using runif\nT0 &lt;- as.numeric(runif(n = length(Tr2), min = 0, max = 1) &gt;= 0.5) # Create T variable\n\n# Create a data frame combining phat and T\ndata2 &lt;- data.frame(phat = phat, group = factor(T0, labels = c(\"Parliamentary\", \"Presidential\")))\n\n# Plot the density using ggplot2\nggplot(data2, aes(x = phat, fill = group)) +\n  geom_density(alpha = 0.5, color = \"black\") +\n  xlim(-0.3, 1.3) +\n  ylim(0, 10) +\n  labs(title = \"Density of Estimated Propensity Score\", \n       x = \"Predicted Propensity Score (phat)\", \n       y = \"Density\") +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  theme(legend.position = \"topright\")\n\n\n\n\n\n\n\n\nThe simulation shows ideal random assignment.\nComparing this to actual plots from propensity scores reveals the problem.\n\n\n\n\n\n\nTaking a different (smaller) set of covariates\n\n## Now include fewer variables to avoid perfect separation\nX2 &lt;- data %&gt;% \n  select(indviol, year_indep, dindep_decol, dindep_seces,\n           a_ethnic, a_ling, a_relig,\n           life_expectency_1800, gdp_per_capita_1800,\n           damerica, dsouthamerica, deurope, dafrica,\n         dasia, doceania)\n\ncolnames(X2) = c(\"Violent Independenc\", \"Year of Independence\", \"Independency by Decolonization\", \"Independence by Secession\",\n                 \"Ethic Fractionalization\", \"Linguistic Fractionalization\", \"Religious Fractionalization\",\n                 \"Life expectancy 1800\", \"GDP per cap 1800\",\n                 \"Country in America\", \"Country in South America\", \"Country in Europe\", \"Country in Africa\", \"Country in Asia\", \"Country in Oceania\")\n\n# index on complete cases\nindx = complete.cases(X2)\n# index treatment vector on complete cases\nTr2 = Tr[indx]\n# calculate propensity score\npscore&lt;-glm(Tr2~ . , data = X2[indx,],family=binomial(link=logit)) \nphat&lt;-pscore$fitted.values\n\nggplot(data.frame(phat), aes(x = phat)) +\n  geom_histogram(binwidth = 0.05, fill = \"pink\", color = \"black\") +\n  labs(title = \"Histogram of PScores\", \n       x = \"Predicted Propensity Scores (phat)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nComparing again\n\n# Create a data frame combining phat and Tr2\ndata3 &lt;- data.frame(phat = phat, Tr2 = factor(Tr2, labels = c(\"Parliamentary (Co)\", \"Presidential (Tr)\")))\n\n# Set up the plot layout\n# First plot: density plots\np1 &lt;- ggplot(data3, aes(x = phat, fill = Tr2)) +\n  geom_density(alpha = 0.5, position = \"identity\") +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"Density of Predicted Propensity Scores\", \n       x = \"Predicted Propensity Score (phat)\", \n       y = \"Density\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n# Second plot: boxplot\np2 &lt;- ggplot(data3, aes(x = Tr2, y = phat)) +\n  geom_boxplot(outlier.shape = NA) + # Suppressing outlier shapes for clarity\n  geom_jitter(width = 0.2, aes(color = Tr2), alpha = 0.6) + # Adding jitter for visibility\n  labs(title = \"Boxplot of Predicted Propensity Scores by Group\", \n       x = \"Group\", \n       y = \"Predicted Propensity Score (phat)\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal()+\n  theme(\n    legend.position = \"bottom\"\n  )\n\n# Combine the two plots using gridExtra\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n# These are all different ways to see balance on propensity scores\n\n\n\nPropesnity Score Density\n\n# Pscore Density plots\ndat &lt;- data.frame(phat = c(phat[Tr2==1], phat[Tr2==0]), Tr=Tr2, groups = as.character(c(rep(\"Tr\", length(Tr2==1)), rep(\"Co\", length(Tr2==0)))))\n\nggplot(dat)+\n  geom_density(data=data.frame(x=phat[Tr2==1]), aes(x = x, fill=\"Tr\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\") +\n  geom_density(data=data.frame(x=phat[Tr2==0]), aes(x = x, fill=\"Co\"), trim=FALSE, adjust=1/3, from=0, to=1, alpha=0.2,  bw = \"nrd0\", kernel = \"gaussian\") +\n  scale_fill_manual(values=c(\"Tr\"=\"red\", \"Co\"=\"blue\"), labels = c(\"Presidential (T = 1) \", \"Parliamentary (T = 0)\"))+\n  theme_bw()+\n  labs(\n    x = \"Probability of Being in Treatment\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# balance in propensity score\nresult &lt;- balance(x=phat, Tr= Tr2, xname=\"phat\")\n\n------------------------------------------\nVariable  phat \nMean Treatment (Pres)  0.6811678 \nMean Control (Parl) 0.1381606 \nNormalized difference 2.205072 \n------------------------------------------\nMass of treated distribution in the control tails 19.77 % \n------------------------------------------\n\n\nAs you can see, it is still quite unbalanced.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#how-to-use-matching",
    "href": "matching.html#how-to-use-matching",
    "title": "Matching",
    "section": "How to Use Matching",
    "text": "How to Use Matching\n\nMatch on Covariates\n\n# set up \n# install.packages(\"Matching\")\nlibrary(Matching)\n\n# match on covariates\nmout1 &lt;- Match(Tr=Tr[indx], X = X2[indx,], estimand=\"ATT\", ties=FALSE)\nsummary(mout1)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  26 \nMatched number of observations...............  26 \nMatched number of observations  (unweighted).  26 \n\n\n\nRunning MatchBalance()Output\n\n\n```{r}\nMatchBalance(Tr[indx] ~ ., data = X2[indx,], match.out=mout1)\n\n# What does the output mean?\n# Let's look at the documentation of ?MatchBalance\n\n# Let's look at the results again\n# See what rows of value for covariates change\n```\n\n\n\n\n\n***** (V1) `Violent Independenc` *****\n                       Before Matching       After Matching\nmean treatment........    0.65385           0.65385 \nmean control..........        0.5           0.69231 \nstd mean diff.........      31.71           -7.9275 \n\nmean raw eQQ diff.....    0.15385          0.038462 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.076923          0.019231 \nmed  eCDF diff........   0.076923          0.019231 \nmax  eCDF diff........    0.15385          0.038462 \n\nvar ratio (Tr/Co).....    0.92585            1.0625 \nT-test p-value........    0.18817           0.65734 \n\n\n***** (V2) `Year of Independence` *****\n                       Before Matching       After Matching\nmean treatment........     1873.7            1873.7 \nmean control..........     1833.8            1900.4 \nstd mean diff.........     61.639           -41.192 \n\nmean raw eQQ diff.....     130.81            27.077 \nmed  raw eQQ diff.....       63.5                 6 \nmax  raw eQQ diff.....        840                82 \n\nmean eCDF diff........    0.16815           0.10743 \nmed  eCDF diff........    0.15769          0.076923 \nmax  eCDF diff........    0.36538           0.30769 \n\nvar ratio (Tr/Co).....   0.064641            1.1792 \nT-test p-value........    0.26113          0.030679 \nKS Bootstrap p-value..      0.006             0.118 \nKS Naive p-value......  0.0091303           0.13814 \nKS Statistic..........    0.36538           0.30769 \n\n\n***** (V3) `Independency by Decolonization` *****\n                       Before Matching       After Matching\nmean treatment........    0.61538           0.61538 \nmean control..........    0.48333           0.73077 \nstd mean diff.........     26.616           -23.257 \n\nmean raw eQQ diff.....    0.15385           0.11538 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.066026          0.057692 \nmed  eCDF diff........   0.066026          0.057692 \nmax  eCDF diff........    0.13205           0.11538 \n\nvar ratio (Tr/Co).....    0.96928             1.203 \nT-test p-value........    0.26482          0.077437 \n\n\n***** (V4) `Independence by Secession` *****\n                       Before Matching       After Matching\nmean treatment........    0.34615           0.34615 \nmean control..........    0.23333           0.26923 \nstd mean diff.........     23.254            15.855 \n\nmean raw eQQ diff.....    0.11538          0.076923 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.05641          0.038462 \nmed  eCDF diff........    0.05641          0.038462 \nmax  eCDF diff........    0.11282          0.076923 \n\nvar ratio (Tr/Co).....     1.2939            1.1504 \nT-test p-value........    0.31057           0.31759 \n\n\n***** (V5) `Ethic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.46538           0.46538 \nmean control..........    0.37542           0.48415 \nstd mean diff.........     37.869           -7.9049 \n\nmean raw eQQ diff.....    0.10566          0.066566 \nmed  raw eQQ diff.....   0.099907          0.061492 \nmax  raw eQQ diff.....       0.21           0.14438 \n\nmean eCDF diff........    0.11999          0.077963 \nmed  eCDF diff........    0.10769          0.076923 \nmax  eCDF diff........    0.28718           0.23077 \n\nvar ratio (Tr/Co).....     1.0573            1.0826 \nT-test p-value........    0.11067           0.69009 \nKS Bootstrap p-value..      0.084              0.42 \nKS Naive p-value......   0.079403           0.46218 \nKS Statistic..........    0.28718           0.23077 \n\n\n***** (V6) `Linguistic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.33101           0.33101 \nmean control..........    0.34663           0.40658 \nstd mean diff.........    -5.0586           -24.468 \n\nmean raw eQQ diff.....   0.070562          0.099725 \nmed  raw eQQ diff.....   0.070461          0.059251 \nmax  raw eQQ diff.....    0.18848           0.28053 \n\nmean eCDF diff........   0.081112           0.11435 \nmed  eCDF diff........   0.080769          0.076923 \nmax  eCDF diff........    0.20128           0.34615 \n\nvar ratio (Tr/Co).....     1.5366            1.3935 \nT-test p-value........    0.82096          0.030313 \nKS Bootstrap p-value..      0.374             0.076 \nKS Naive p-value......    0.39378          0.065032 \nKS Statistic..........    0.20128           0.34615 \n\n\n***** (V7) `Religious Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.38249           0.38249 \nmean control..........    0.44067           0.34795 \nstd mean diff.........    -25.495            15.139 \n\nmean raw eQQ diff.....   0.080581          0.055022 \nmed  raw eQQ diff.....   0.072014           0.04163 \nmax  raw eQQ diff.....    0.20034           0.16627 \n\nmean eCDF diff........   0.092695           0.11331 \nmed  eCDF diff........   0.064103          0.076923 \nmax  eCDF diff........    0.29744           0.38462 \n\nvar ratio (Tr/Co).....    0.99339            0.8127 \nT-test p-value........    0.28348           0.39388 \nKS Bootstrap p-value..      0.038             0.038 \nKS Naive p-value......   0.063197          0.035888 \nKS Statistic..........    0.29744           0.38462 \n\n\n***** (V8) `Life expectancy 1800` *****\n                       Before Matching       After Matching\nmean treatment........     31.386            31.386 \nmean control..........     33.225            32.671 \nstd mean diff.........    -53.291           -37.245 \n\nmean raw eQQ diff.....     1.7649            1.8473 \nmed  raw eQQ diff.....       1.83              2.05 \nmax  raw eQQ diff.....        3.4              3.51 \n\nmean eCDF diff........    0.14984           0.15012 \nmed  eCDF diff........    0.12821          0.076923 \nmax  eCDF diff........    0.38462           0.46154 \n\nvar ratio (Tr/Co).....    0.78917           0.87149 \nT-test p-value........   0.033452          0.051485 \nKS Bootstrap p-value.. &lt; 2.22e-16             0.004 \nKS Naive p-value......  0.0053601         0.0056297 \nKS Statistic..........    0.38462           0.46154 \n\n\n***** (V9) `GDP per cap 1800` *****\n                       Before Matching       After Matching\nmean treatment........     914.12            914.12 \nmean control..........     1180.3            1009.1 \nstd mean diff.........    -81.331           -29.025 \n\nmean raw eQQ diff.....     286.88            130.38 \nmed  raw eQQ diff.....        193               113 \nmax  raw eQQ diff.....        796               452 \n\nmean eCDF diff........    0.14666            0.1237 \nmed  eCDF diff........   0.094872           0.11538 \nmax  eCDF diff........    0.40128           0.34615 \n\nvar ratio (Tr/Co).....     0.3525            1.5045 \nT-test p-value........  0.0069009           0.14223 \nKS Bootstrap p-value..      0.006             0.054 \nKS Naive p-value......  0.0038375          0.068177 \nKS Statistic..........    0.40128           0.34615 \n\n\n***** (V10) `Country in America` *****\n                       Before Matching       After Matching\nmean treatment........    0.65385           0.65385 \nmean control..........   0.066667           0.42308 \nstd mean diff.........     121.03            47.565 \n\nmean raw eQQ diff.....    0.57692           0.23077 \nmed  raw eQQ diff.....          1                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.29359           0.11538 \nmed  eCDF diff........    0.29359           0.11538 \nmax  eCDF diff........    0.58718           0.23077 \n\nvar ratio (Tr/Co).....     3.7199           0.92727 \nT-test p-value........ 1.9372e-06          0.009874 \n\n\n***** (V11) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.34615           0.34615 \nmean control..........   0.016667           0.34615 \nstd mean diff.........     67.912                 0 \n\nmean raw eQQ diff.....    0.30769                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.16474                 0 \nmed  eCDF diff........    0.16474                 0 \nmax  eCDF diff........    0.32949                 0 \n\nvar ratio (Tr/Co).....     14.123                 1 \nT-test p-value........  0.0020845                 1 \n\n\n***** (V12) `Country in Europe` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........        0.5          0.076923 \nstd mean diff.........       -Inf              -Inf \n\nmean raw eQQ diff.....        0.5          0.076923 \nmed  raw eQQ diff.....        0.5                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........       0.25          0.038462 \nmed  eCDF diff........       0.25          0.038462 \nmax  eCDF diff........        0.5          0.076923 \n\nvar ratio (Tr/Co).....          0                 0 \nT-test p-value........ 1.8965e-10           0.15351 \n\n\n***** (V13) `Country in Africa` *****\n                       Before Matching       After Matching\nmean treatment........    0.19231           0.19231 \nmean control..........       0.15           0.19231 \nstd mean diff.........     10.526                 0 \n\nmean raw eQQ diff.....   0.038462                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.021154                 0 \nmed  eCDF diff........   0.021154                 0 \nmax  eCDF diff........   0.042308                 0 \n\nvar ratio (Tr/Co).....     1.2459                 1 \nT-test p-value........    0.64616                 1 \n\n\n***** (V14) `Country in Asia` *****\n                       Before Matching       After Matching\nmean treatment........    0.15385           0.15385 \nmean control..........    0.21667           0.30769 \nstd mean diff.........    -17.073           -41.812 \n\nmean raw eQQ diff.....   0.076923           0.15385 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.03141          0.076923 \nmed  eCDF diff........    0.03141          0.076923 \nmax  eCDF diff........   0.062821           0.15385 \n\nvar ratio (Tr/Co).....    0.78439           0.61111 \nT-test p-value........    0.48777           0.03936 \n\n\n***** (V15) `Country in Oceania` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........   0.066667                 0 \nstd mean diff.........       -Inf                 0 \n\nmean raw eQQ diff.....   0.076923                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.033333                 0 \nmed  eCDF diff........   0.033333                 0 \nmax  eCDF diff........   0.066667                 0 \n\nvar ratio (Tr/Co).....          0               NaN \nT-test p-value........   0.044528                 1 \n\n\nBefore Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Life expectancy 1800`  Number(s): 8 \n\nAfter Matching Minimum p.value: 0.004 \nVariable Name(s): `Life expectancy 1800`  Number(s): 8 \n\n\n\n\n\n\n\n\nMatch on Propensity Scores\n\n# match on propensity score\nmout2 &lt;- Match(Tr=Tr[indx], X=phat, estimand=\"ATT\", ties=FALSE)\nsummary(mout2)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  26 \nMatched number of observations...............  26 \nMatched number of observations  (unweighted).  26 \n\nMatchBalance(Tr[indx] ~ phat, match.out=mout2)\n\n\n***** (V1) phat *****\n                       Before Matching       After Matching\nmean treatment........    0.68117           0.68117 \nmean control..........    0.13816           0.67938 \nstd mean diff.........     196.78           0.64781 \n\nmean raw eQQ diff.....     0.5399          0.045756 \nmed  raw eQQ diff.....    0.54148          0.036657 \nmax  raw eQQ diff.....    0.84298            0.1225 \n\nmean eCDF diff........    0.41987          0.090498 \nmed  eCDF diff........    0.48333          0.038462 \nmax  eCDF diff........    0.71154           0.34615 \n\nvar ratio (Tr/Co).....     1.6869           0.97716 \nT-test p-value........ 6.1436e-11           0.87615 \nKS Bootstrap p-value.. &lt; 2.22e-16              0.06 \nKS Naive p-value...... 2.0098e-09           0.07132 \nKS Statistic..........    0.71154           0.34615 \n\n\n\n\nExplore the matches\n\n## examine matches \nmdataTr = data[indx,][mout2$index.treated,]\nmdataCo = data[indx,][mout2$index.control,]\n\n```{r}\nView(data.frame(mdataTr$country, mdataCo$country))\n\n```\nNote how Peru appears as the chosen control country for every Latin American country\n\n\n\n\n\n\n\n\nThat is because Peru is coded as not being pure presidential.\n\ndata$purepres[data$country==\"PERU\"]\n\n[1] 0\n\ndata$presparlpm[data$country==\"PERU\"]\n\n[1] 1\n\n\nIncluding Mixed Systems\nHowever, Peru is a variation of presidential system, so it may not be adequate to classify as control.\nLetâ€™s code mixed systems as treated.\n\nTr = (data$purepres == 1 | data$presparlpm==1)\nmout4 &lt;- Match(Tr=Tr[indx], X=X2[indx,], estimand=\"ATT\")\nsummary(mout4)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  30 \nMatched number of observations...............  30 \nMatched number of observations  (unweighted).  30 \n\nmdataTr = data[indx,][mout4$index.treated,]\nmdataCo = data[indx,][mout4$index.control,]\n\n```{r}\nView(data.frame(mdataTr$country, mdataCo$country))\n```\n\n\n\n\n\n\n\n\n\nMatchBalance CodeMatchBalance Output\n\n\n```{r}\nMatchBalance(Tr[indx] ~ ., data =  X2[indx,], match.out=mout4, nboots=10)\n```\n\n\n\n\n\n***** (V1) `Violent Independenc` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333           0.63333 \nmean control..........        0.5               0.4 \nstd mean diff.........     27.204            47.606 \n\nmean raw eQQ diff.....    0.13333           0.23333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.066667           0.11667 \nmed  eCDF diff........   0.066667           0.11667 \nmax  eCDF diff........    0.13333           0.23333 \n\nvar ratio (Tr/Co).....    0.94376           0.96759 \nT-test p-value........    0.23865         0.0052095 \n\n\n***** (V2) `Year of Independence` *****\n                       Before Matching       After Matching\nmean treatment........     1882.7            1882.7 \nmean control..........     1826.1            1946.2 \nstd mean diff.........     81.576           -91.426 \n\nmean raw eQQ diff.....     134.53            63.467 \nmed  raw eQQ diff.....         53              92.5 \nmax  raw eQQ diff.....        840               122 \n\nmean eCDF diff........    0.13314           0.24902 \nmed  eCDF diff........    0.11667              0.25 \nmax  eCDF diff........    0.31667           0.56667 \n\nvar ratio (Tr/Co).....   0.070412             5.908 \nT-test p-value........    0.13242        3.9478e-05 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value......   0.023689        5.8786e-05 \nKS Statistic..........    0.31667           0.56667 \n\n\n***** (V3) `Independency by Decolonization` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333           0.63333 \nmean control..........    0.46429           0.66667 \nstd mean diff.........      34.49           -6.8009 \n\nmean raw eQQ diff.....    0.16667          0.033333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.084524          0.016667 \nmed  eCDF diff........   0.084524          0.016667 \nmax  eCDF diff........    0.16905          0.033333 \n\nvar ratio (Tr/Co).....     0.9486             1.045 \nT-test p-value........    0.13617           0.31752 \n\n\n***** (V4) `Independence by Secession` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333           0.33333 \nmean control..........    0.23214           0.33333 \nstd mean diff.........     21.105                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.050595                 0 \nmed  eCDF diff........   0.050595                 0 \nmax  eCDF diff........    0.10119                 0 \n\nvar ratio (Tr/Co).....     1.2666                 1 \nT-test p-value........    0.33686                 1 \n\n\n***** (V5) `Ethic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.48523           0.48523 \nmean control..........    0.35836           0.42003 \nstd mean diff.........     55.492            28.518 \n\nmean raw eQQ diff.....     0.1415          0.076679 \nmed  raw eQQ diff.....    0.12276          0.072789 \nmax  raw eQQ diff.....     0.2612            0.1832 \n\nmean eCDF diff........    0.16375           0.10725 \nmed  eCDF diff........    0.15714          0.066667 \nmax  eCDF diff........    0.37857           0.33333 \n\nvar ratio (Tr/Co).....    0.99957            1.0037 \nT-test p-value........   0.017142          0.044288 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.2 \nKS Naive p-value......  0.0050673          0.058602 \nKS Statistic..........    0.37857           0.33333 \n\n\n***** (V6) `Linguistic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.36042           0.36042 \nmean control..........    0.33199            0.3762 \nstd mean diff.........      9.407           -5.2182 \n\nmean raw eQQ diff.....   0.052711            0.0509 \nmed  raw eQQ diff.....   0.023094          0.045404 \nmax  raw eQQ diff.....    0.18848           0.11195 \n\nmean eCDF diff........   0.055482          0.073913 \nmed  eCDF diff........   0.039881          0.066667 \nmax  eCDF diff........    0.20833           0.23333 \n\nvar ratio (Tr/Co).....     1.4833           0.94082 \nT-test p-value........    0.66065             0.568 \nKS Bootstrap p-value..        0.3               0.2 \nKS Naive p-value......    0.31127           0.37798 \nKS Statistic..........    0.20833           0.23333 \n\n\n***** (V7) `Religious Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.38572           0.38572 \nmean control..........     0.4431            0.4778 \nstd mean diff.........    -25.044           -40.192 \n\nmean raw eQQ diff.....   0.074969          0.098234 \nmed  raw eQQ diff.....   0.053677          0.065211 \nmax  raw eQQ diff.....    0.17146            0.2822 \n\nmean eCDF diff........   0.089106            0.1029 \nmed  eCDF diff........     0.0625               0.1 \nmax  eCDF diff........    0.27381               0.3 \n\nvar ratio (Tr/Co).....     1.0062           0.78179 \nT-test p-value........    0.27231          0.062384 \nKS Bootstrap p-value..        0.3        &lt; 2.22e-16 \nKS Naive p-value......   0.085347           0.12971 \nKS Statistic..........    0.27381               0.3 \n\n\n***** (V8) `Life expectancy 1800` *****\n                       Before Matching       After Matching\nmean treatment........     31.531            31.531 \nmean control..........     33.279              32.3 \nstd mean diff.........    -48.136           -21.188 \n\nmean raw eQQ diff.....     1.6086            1.0179 \nmed  raw eQQ diff.....     1.6776              1.15 \nmax  raw eQQ diff.....        3.1                 2 \n\nmean eCDF diff........    0.13347          0.097436 \nmed  eCDF diff........    0.11071          0.066667 \nmax  eCDF diff........    0.33571           0.33333 \n\nvar ratio (Tr/Co).....    0.89741           0.98357 \nT-test p-value........   0.041052           0.15781 \nKS Bootstrap p-value..        0.1               0.2 \nKS Naive p-value......   0.016592          0.055924 \nKS Statistic..........    0.33571           0.33333 \n\n\n***** (V9) `GDP per cap 1800` *****\n                       Before Matching       After Matching\nmean treatment........     884.67            884.67 \nmean control..........     1215.1            1141.5 \nstd mean diff.........    -103.91           -80.776 \n\nmean raw eQQ diff.....      327.8               287 \nmed  raw eQQ diff.....      269.5               272 \nmax  raw eQQ diff.....        796               622 \n\nmean eCDF diff........    0.17895           0.23188 \nmed  eCDF diff........    0.12738           0.23333 \nmax  eCDF diff........    0.45357           0.53333 \n\nvar ratio (Tr/Co).....    0.33049           0.73895 \nT-test p-value........ 0.00071184        0.00027375 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value...... 0.00035765        0.00027349 \nKS Statistic..........    0.45357           0.53333 \n\n\n***** (V10) `Country in America` *****\n                       Before Matching       After Matching\nmean treatment........        0.6               0.6 \nmean control..........   0.053571               0.3 \nstd mean diff.........     109.66            60.208 \n\nmean raw eQQ diff.....    0.53333               0.3 \nmed  raw eQQ diff.....          1                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.27321              0.15 \nmed  eCDF diff........    0.27321              0.15 \nmax  eCDF diff........    0.54643               0.3 \n\nvar ratio (Tr/Co).....     4.8094            1.1429 \nT-test p-value........ 1.8303e-06         0.0012161 \n\n\n***** (V11) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333           0.33333 \nmean control..........          0                 0 \nstd mean diff.........     69.522            69.522 \n\nmean raw eQQ diff.....    0.33333           0.33333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.16667           0.16667 \nmed  eCDF diff........    0.16667           0.16667 \nmax  eCDF diff........    0.33333           0.33333 \n\nvar ratio (Tr/Co).....        Inf               Inf \nT-test p-value........ 0.00067228         0.0005642 \n\n\n***** (V12) `Country in Europe` *****\n                       Before Matching       After Matching\nmean treatment........   0.033333          0.033333 \nmean control..........    0.51786           0.16667 \nstd mean diff.........    -265.38            -73.03 \n\nmean raw eQQ diff.....    0.46667           0.13333 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.24226          0.066667 \nmed  eCDF diff........    0.24226          0.066667 \nmax  eCDF diff........    0.48452           0.13333 \n\nvar ratio (Tr/Co).....    0.13112             0.232 \nT-test p-value........ 9.3222e-09          0.040171 \n\n\n***** (V13) `Country in Africa` *****\n                       Before Matching       After Matching\nmean treatment........    0.23333           0.23333 \nmean control..........      0.125           0.23333 \nstd mean diff.........     25.183                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.054167                 0 \nmed  eCDF diff........   0.054167                 0 \nmax  eCDF diff........    0.10833                 0 \n\nvar ratio (Tr/Co).....     1.6617                 1 \nT-test p-value........    0.23622                 1 \n\n\n***** (V14) `Country in Asia` *****\n                       Before Matching       After Matching\nmean treatment........    0.13333           0.13333 \nmean control..........    0.23214               0.3 \nstd mean diff.........    -28.579           -48.205 \n\nmean raw eQQ diff.....        0.1           0.16667 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.049405          0.083333 \nmed  eCDF diff........   0.049405          0.083333 \nmax  eCDF diff........    0.09881           0.16667 \n\nvar ratio (Tr/Co).....    0.65865           0.55026 \nT-test p-value........    0.24898          0.020583 \n\n\n***** (V15) `Country in Oceania` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........   0.071429                 0 \nstd mean diff.........       -Inf                 0 \n\nmean raw eQQ diff.....   0.066667                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.035714                 0 \nmed  eCDF diff........   0.035714                 0 \nmax  eCDF diff........   0.071429                 0 \n\nvar ratio (Tr/Co).....          0               NaN \nT-test p-value........   0.044453                 1 \n\n\nBefore Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Year of Independence` `Ethic Fractionalization` `GDP per cap 1800`  Number(s): 2 5 9 \n\nAfter Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Year of Independence` `Religious Fractionalization` `GDP per cap 1800`  Number(s): 2 7 9 \n\n\n\n\n\n\n\n\nIncorporating region wise exact matching\n\n## Now do exact matching\ncolnames(X2)\n\n [1] \"Violent Independenc\"            \"Year of Independence\"          \n [3] \"Independency by Decolonization\" \"Independence by Secession\"     \n [5] \"Ethic Fractionalization\"        \"Linguistic Fractionalization\"  \n [7] \"Religious Fractionalization\"    \"Life expectancy 1800\"          \n [9] \"GDP per cap 1800\"               \"Country in America\"            \n[11] \"Country in South America\"       \"Country in Europe\"             \n[13] \"Country in Africa\"              \"Country in Asia\"               \n[15] \"Country in Oceania\"            \n\nexactX = rep(FALSE, ncol(X2))\ncolnames(X2)[c(10:15)]\n\n[1] \"Country in America\"       \"Country in South America\"\n[3] \"Country in Europe\"        \"Country in Africa\"       \n[5] \"Country in Asia\"          \"Country in Oceania\"      \n\nexactX[c(10:15)] = TRUE\ncolnames(X2)[exactX]\n\n[1] \"Country in America\"       \"Country in South America\"\n[3] \"Country in Europe\"        \"Country in Africa\"       \n[5] \"Country in Asia\"          \"Country in Oceania\"      \n\n# match again with exact region pairing \nmout5 &lt;- Match(Tr=Tr[indx], X=X2[indx,], estimand=\"ATT\", exact=exactX)\nsummary(mout5)\n\n\nEstimate...  0 \nSE.........  0 \nT-stat.....  NaN \np.val......  NA \n\nOriginal number of observations..............  86 \nOriginal number of treated obs...............  30 \nMatched number of observations...............  20 \nMatched number of observations  (unweighted).  20 \n\nNumber of obs dropped by 'exact' or 'caliper'  10 \n\nMatchBalance(Tr[indx] ~ . , data = X2[indx,], match.out=mout5, nboots=10)\n\n\n***** (V1) `Violent Independenc` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333               0.6 \nmean control..........        0.5               0.2 \nstd mean diff.........     27.204            79.582 \n\nmean raw eQQ diff.....    0.13333               0.4 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.066667               0.2 \nmed  eCDF diff........   0.066667               0.2 \nmax  eCDF diff........    0.13333               0.4 \n\nvar ratio (Tr/Co).....    0.94376               1.5 \nT-test p-value........    0.23865         0.0016974 \n\n\n***** (V2) `Year of Independence` *****\n                       Before Matching       After Matching\nmean treatment........     1882.7            1912.9 \nmean control..........     1826.1            1954.7 \nstd mean diff.........     81.576           -62.617 \n\nmean raw eQQ diff.....     134.53              43.5 \nmed  raw eQQ diff.....         53              14.5 \nmax  raw eQQ diff.....        840               127 \n\nmean eCDF diff........    0.13314           0.17381 \nmed  eCDF diff........    0.11667               0.2 \nmax  eCDF diff........    0.31667               0.4 \n\nvar ratio (Tr/Co).....   0.070412            9.6233 \nT-test p-value........    0.13242          0.018361 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value......   0.023689          0.045888 \nKS Statistic..........    0.31667               0.4 \n\n\n***** (V3) `Independency by Decolonization` *****\n                       Before Matching       After Matching\nmean treatment........    0.63333               0.6 \nmean control..........    0.46429               0.9 \nstd mean diff.........      34.49           -59.687 \n\nmean raw eQQ diff.....    0.16667               0.3 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.084524              0.15 \nmed  eCDF diff........   0.084524              0.15 \nmax  eCDF diff........    0.16905               0.3 \n\nvar ratio (Tr/Co).....     0.9486            2.6667 \nT-test p-value........    0.13617         0.0086361 \n\n\n***** (V4) `Independence by Secession` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333              0.35 \nmean control..........    0.23214               0.1 \nstd mean diff.........     21.105            51.087 \n\nmean raw eQQ diff.....        0.1              0.25 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.050595             0.125 \nmed  eCDF diff........   0.050595             0.125 \nmax  eCDF diff........    0.10119              0.25 \n\nvar ratio (Tr/Co).....     1.2666            2.5278 \nT-test p-value........    0.33686          0.050785 \n\n\n***** (V5) `Ethic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.48523           0.50032 \nmean control..........    0.35836           0.51093 \nstd mean diff.........     55.492            -4.463 \n\nmean raw eQQ diff.....     0.1415          0.072524 \nmed  raw eQQ diff.....    0.12276          0.046989 \nmax  raw eQQ diff.....     0.2612           0.24974 \n\nmean eCDF diff........    0.16375          0.098333 \nmed  eCDF diff........    0.15714               0.1 \nmax  eCDF diff........    0.37857              0.25 \n\nvar ratio (Tr/Co).....    0.99957            1.6511 \nT-test p-value........   0.017142           0.76341 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.5 \nKS Naive p-value......  0.0050673           0.51186 \nKS Statistic..........    0.37857              0.25 \n\n\n***** (V6) `Linguistic Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.36042           0.45297 \nmean control..........    0.33199           0.44779 \nstd mean diff.........      9.407            1.6561 \n\nmean raw eQQ diff.....   0.052711          0.084695 \nmed  raw eQQ diff.....   0.023094           0.06169 \nmax  raw eQQ diff.....    0.18848           0.27104 \n\nmean eCDF diff........   0.055482             0.105 \nmed  eCDF diff........   0.039881               0.1 \nmax  eCDF diff........    0.20833              0.25 \n\nvar ratio (Tr/Co).....     1.4833           0.86807 \nT-test p-value........    0.66065           0.88091 \nKS Bootstrap p-value..        0.2               0.6 \nKS Naive p-value......    0.31127             0.502 \nKS Statistic..........    0.20833              0.25 \n\n\n***** (V7) `Religious Fractionalization` *****\n                       Before Matching       After Matching\nmean treatment........    0.38572           0.44798 \nmean control..........     0.4431           0.56804 \nstd mean diff.........    -25.044           -50.042 \n\nmean raw eQQ diff.....   0.074969           0.12007 \nmed  raw eQQ diff.....   0.053677          0.088513 \nmax  raw eQQ diff.....    0.17146           0.30412 \n\nmean eCDF diff........   0.089106           0.13667 \nmed  eCDF diff........     0.0625               0.1 \nmax  eCDF diff........    0.27381              0.35 \n\nvar ratio (Tr/Co).....     1.0062            1.1251 \nT-test p-value........    0.27231          0.031915 \nKS Bootstrap p-value..        0.1               0.1 \nKS Naive p-value......   0.085347           0.15913 \nKS Statistic..........    0.27381              0.35 \n\n\n***** (V8) `Life expectancy 1800` *****\n                       Before Matching       After Matching\nmean treatment........     31.531            30.726 \nmean control..........     33.279            31.717 \nstd mean diff.........    -48.136           -23.898 \n\nmean raw eQQ diff.....     1.6086            1.9916 \nmed  raw eQQ diff.....     1.6776             2.189 \nmax  raw eQQ diff.....        3.1               4.3 \n\nmean eCDF diff........    0.13347           0.15179 \nmed  eCDF diff........    0.11071             0.125 \nmax  eCDF diff........    0.33571              0.35 \n\nvar ratio (Tr/Co).....    0.89741            1.5235 \nT-test p-value........   0.041052           0.27411 \nKS Bootstrap p-value.. &lt; 2.22e-16               0.1 \nKS Naive p-value......   0.016592           0.15476 \nKS Statistic..........    0.33571              0.35 \n\n\n***** (V9) `GDP per cap 1800` *****\n                       Before Matching       After Matching\nmean treatment........     884.67             869.9 \nmean control..........     1215.1            1129.4 \nstd mean diff.........    -103.91           -70.669 \n\nmean raw eQQ diff.....      327.8             304.7 \nmed  raw eQQ diff.....      269.5               212 \nmax  raw eQQ diff.....        796               741 \n\nmean eCDF diff........    0.17895           0.21167 \nmed  eCDF diff........    0.12738               0.2 \nmax  eCDF diff........    0.45357               0.5 \n\nvar ratio (Tr/Co).....    0.33049           0.83115 \nT-test p-value........ 0.00071184         0.0096143 \nKS Bootstrap p-value.. &lt; 2.22e-16        &lt; 2.22e-16 \nKS Naive p-value...... 0.00035765          0.010558 \nKS Statistic..........    0.45357               0.5 \n\n\n***** (V10) `Country in America` *****\n                       Before Matching       After Matching\nmean treatment........        0.6               0.4 \nmean control..........   0.053571               0.4 \nstd mean diff.........     109.66                 0 \n\nmean raw eQQ diff.....    0.53333                 0 \nmed  raw eQQ diff.....          1                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.27321                 0 \nmed  eCDF diff........    0.27321                 0 \nmax  eCDF diff........    0.54643                 0 \n\nvar ratio (Tr/Co).....     4.8094                 1 \nT-test p-value........ 1.8303e-06                 1 \n\n\n***** (V11) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.33333                 0 \nmean control..........          0                 0 \nstd mean diff.........     69.522                 0 \n\nmean raw eQQ diff.....    0.33333                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.16667                 0 \nmed  eCDF diff........    0.16667                 0 \nmax  eCDF diff........    0.33333                 0 \n\nvar ratio (Tr/Co).....        Inf               NaN \nT-test p-value........ 0.00067228                 1 \n\n\n***** (V12) `Country in Europe` *****\n                       Before Matching       After Matching\nmean treatment........   0.033333              0.05 \nmean control..........    0.51786              0.05 \nstd mean diff.........    -265.38                 0 \n\nmean raw eQQ diff.....    0.46667                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.24226                 0 \nmed  eCDF diff........    0.24226                 0 \nmax  eCDF diff........    0.48452                 0 \n\nvar ratio (Tr/Co).....    0.13112                 1 \nT-test p-value........ 9.3222e-09                 1 \n\n\n***** (V13) `Country in Africa` *****\n                       Before Matching       After Matching\nmean treatment........    0.23333              0.35 \nmean control..........      0.125              0.35 \nstd mean diff.........     25.183                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.054167                 0 \nmed  eCDF diff........   0.054167                 0 \nmax  eCDF diff........    0.10833                 0 \n\nvar ratio (Tr/Co).....     1.6617                 1 \nT-test p-value........    0.23622                 1 \n\n\n***** (V14) `Country in Asia` *****\n                       Before Matching       After Matching\nmean treatment........    0.13333               0.2 \nmean control..........    0.23214               0.2 \nstd mean diff.........    -28.579                 0 \n\nmean raw eQQ diff.....        0.1                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.049405                 0 \nmed  eCDF diff........   0.049405                 0 \nmax  eCDF diff........    0.09881                 0 \n\nvar ratio (Tr/Co).....    0.65865                 1 \nT-test p-value........    0.24898                 1 \n\n\n***** (V15) `Country in Oceania` *****\n                       Before Matching       After Matching\nmean treatment........          0                 0 \nmean control..........   0.071429                 0 \nstd mean diff.........       -Inf                 0 \n\nmean raw eQQ diff.....   0.066667                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.035714                 0 \nmed  eCDF diff........   0.035714                 0 \nmax  eCDF diff........   0.071429                 0 \n\nvar ratio (Tr/Co).....          0               NaN \nT-test p-value........   0.044453                 1 \n\n\nBefore Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Year of Independence` `Ethic Fractionalization` `Life expectancy 1800` `GDP per cap 1800`  Number(s): 2 5 8 9 \n\nAfter Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): `Year of Independence` `GDP per cap 1800`  Number(s): 2 9 \n\nmdataTr = data[indx,][mout5$index.treated,]\nmdataCo = data[indx,][mout5$index.control,]\n# Now look at Jamaica\ndata.frame(mdataTr$country, mdataCo$country)\n\n      mdataTr.country     mdataCo.country\n1               BENIN        SOUTH AFRICA\n2          COSTA RICA             JAMAICA\n3              CYPRUS              ISRAEL\n4  DOMINICAN REPUBLIC             JAMAICA\n5              GAMBIA                MALI\n6               GHANA           MAURITIUS\n7           GUATEMALA TRINIDAD AND TOBAGO\n8            HONDURAS             JAMAICA\n9           INDONESIA              ISRAEL\n10        KOREA SOUTH          BANGLADESH\n11             MALAWI        SOUTH AFRICA\n12             MEXICO             JAMAICA\n13            NAMIBIA          MOZAMBIQUE\n14          NICARAGUA TRINIDAD AND TOBAGO\n15             PANAMA             JAMAICA\n16        PHILIPPINES               INDIA\n17            SENEGAL                MALI\n18            UKRAINE           MACEDONIA\n19      UNITED STATES             JAMAICA\n20             ZAMBIA        SOUTH AFRICA\n\nii = (data$country == \"JAMAICA\" | data$country == \"COSTA RICA\" | data$country == \"MEXICO\")\ndata.frame(as.character(data$country[ii]),X2[ii,])\n\n  as.character.data.country.ii.. Violent.Independenc Year.of.Independence\n1                     COSTA RICA                   1                 1838\n2                        JAMAICA                   0                 1962\n3                         MEXICO                   1                 1821\n  Independency.by.Decolonization Independence.by.Secession\n1                              0                         1\n2                              1                         0\n3                              1                         0\n  Ethic.Fractionalization Linguistic.Fractionalization\n1                0.236800                    0.0489116\n2                0.412894                    0.1098046\n3                0.541800                    0.1511190\n  Religious.Fractionalization Life.expectancy.1800 GDP.per.cap.1800\n1                   0.2409582              30.2147              812\n2                   0.6159606              34.2000             1644\n3                   0.1795571              26.9000             1420\n  Country.in.America Country.in.South.America Country.in.Europe\n1                  1                        0                 0\n2                  1                        0                 0\n3                  1                        0                 0\n  Country.in.Africa Country.in.Asia Country.in.Oceania\n1                 0               0                  0\n2                 0               0                  0\n3                 0               0                  0\n\n\n\n\nComparing OLS with Matched Difference of Means\n\n# Outcome 1: President/PM held party position immediately prior\nindx2 = indx & !is.na(data$prexpty) # index for full observations from X2 and our variable of interest\n\n# matching ATT\nmout &lt;- Match(Y=data$prexpty[indx2], Tr=Tr[indx2], X=X2[indx2,], estimand=\"ATT\")\nsummary(mout)\n\n\nEstimate...  -0.42857 \nAI SE......  0.16342 \nT-stat.....  -2.6224 \np.val......  0.0087303 \n\nOriginal number of observations..............  82 \nOriginal number of treated obs...............  28 \nMatched number of observations...............  28 \nMatched number of observations  (unweighted).  28 \n\nt.test(data$prexpty[Tr==1],data$prexpty[Tr==0])\n\n\n    Welch Two Sample t-test\n\ndata:  data$prexpty[Tr == 1] and data$prexpty[Tr == 0]\nt = -0.731, df = 64.438, p-value = 0.4674\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2316728  0.1075349\nsample estimates:\nmean of x mean of y \n 0.137931  0.200000 \n\natt_df &lt;- cbind.data.frame( prexpty = data$prexpty[indx2],\n                 Tr = as.numeric(Tr[indx2])) %&gt;% tibble()\nstats::lm(prexpty ~ Tr, data = att_df)\n\n\nCall:\nstats::lm(formula = prexpty ~ Tr, data = att_df)\n\nCoefficients:\n(Intercept)           Tr  \n    0.20370     -0.06085  \n\nsummary(lm(data$prexpty[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,])))\n\n\nCall:\nlm(formula = data$prexpty[indx2] ~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2, \n    ]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5074 -0.2009 -0.1208 -0.0032  0.9050 \n\nCoefficients: (1 not defined because of singularities)\n                                                       Estimate Std. Error\n(Intercept)                                          -6.817e-01  6.974e-01\nas.numeric(Tr[indx2])                                -2.156e-01  1.484e-01\nas.matrix(X2[indx2, ])Violent Independenc             5.575e-02  1.056e-01\nas.matrix(X2[indx2, ])Year of Independence            3.643e-04  2.885e-04\nas.matrix(X2[indx2, ])Independency by Decolonization -1.751e-02  1.580e-01\nas.matrix(X2[indx2, ])Independence by Secession      -7.567e-02  1.710e-01\nas.matrix(X2[indx2, ])Ethic Fractionalization         6.112e-02  2.946e-01\nas.matrix(X2[indx2, ])Linguistic Fractionalization   -1.617e-01  2.532e-01\nas.matrix(X2[indx2, ])Religious Fractionalization     3.356e-01  2.381e-01\nas.matrix(X2[indx2, ])Life expectancy 1800            3.274e-04  1.493e-02\nas.matrix(X2[indx2, ])GDP per cap 1800                4.305e-05  1.393e-04\nas.matrix(X2[indx2, ])Country in America              2.315e-01  3.196e-01\nas.matrix(X2[indx2, ])Country in South America       -2.853e-02  1.957e-01\nas.matrix(X2[indx2, ])Country in Europe              -5.376e-02  2.695e-01\nas.matrix(X2[indx2, ])Country in Africa               2.320e-01  2.844e-01\nas.matrix(X2[indx2, ])Country in Asia                 1.141e-01  2.581e-01\nas.matrix(X2[indx2, ])Country in Oceania                     NA         NA\n                                                     t value Pr(&gt;|t|)\n(Intercept)                                           -0.978    0.332\nas.numeric(Tr[indx2])                                 -1.453    0.151\nas.matrix(X2[indx2, ])Violent Independenc              0.528    0.599\nas.matrix(X2[indx2, ])Year of Independence             1.263    0.211\nas.matrix(X2[indx2, ])Independency by Decolonization  -0.111    0.912\nas.matrix(X2[indx2, ])Independence by Secession       -0.442    0.660\nas.matrix(X2[indx2, ])Ethic Fractionalization          0.207    0.836\nas.matrix(X2[indx2, ])Linguistic Fractionalization    -0.639    0.525\nas.matrix(X2[indx2, ])Religious Fractionalization      1.410    0.163\nas.matrix(X2[indx2, ])Life expectancy 1800             0.022    0.983\nas.matrix(X2[indx2, ])GDP per cap 1800                 0.309    0.758\nas.matrix(X2[indx2, ])Country in America               0.724    0.471\nas.matrix(X2[indx2, ])Country in South America        -0.146    0.885\nas.matrix(X2[indx2, ])Country in Europe               -0.200    0.842\nas.matrix(X2[indx2, ])Country in Africa                0.816    0.418\nas.matrix(X2[indx2, ])Country in Asia                  0.442    0.660\nas.matrix(X2[indx2, ])Country in Oceania                  NA       NA\n\nResidual standard error: 0.3958 on 66 degrees of freedom\nMultiple R-squared:  0.1563,    Adjusted R-squared:  -0.03543 \nF-statistic: 0.8152 on 15 and 66 DF,  p-value: 0.6572\n\n\n\n## OLS versus Matching (paying attention to South America dummy)\nindx2 = indx & !is.na(data$prexpty)\nmout2 &lt;- Match(Y=data$prexpty[indx2], Tr=Tr[indx2], X=X2[indx2,11], estimand=\"ATT\")\nsummary(mout2)\n\n\nEstimate...  -0.060847 \nAI SE......  0.083155 \nT-stat.....  -0.73172 \np.val......  0.46434 \n\nOriginal number of observations..............  82 \nOriginal number of treated obs...............  28 \nMatched number of observations...............  28 \nMatched number of observations  (unweighted).  1512 \n\nMatchBalance(Tr[indx2] ~. , data =  X2[indx2,11], match.out=mout2, nboots=10)\n\n\n***** (V1) `Country in South America` *****\n                       Before Matching       After Matching\nmean treatment........    0.32143           0.32143 \nmean control..........          0                 0 \nstd mean diff.........     67.585            67.585 \n\nmean raw eQQ diff.....    0.32143           0.32143 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........    0.16071           0.16071 \nmed  eCDF diff........    0.16071           0.16071 \nmax  eCDF diff........    0.32143           0.32143 \n\nvar ratio (Tr/Co).....        Inf               Inf \nT-test p-value........   0.001342         0.0011322 \n\n#South America does not has a control group\nsummary(lm(data$prexpty[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,11])))\n\n\nCall:\nlm(formula = data$prexpty[indx2] ~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2, \n    11]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2037 -0.2037 -0.2037 -0.1111  0.8889 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.20370    0.05342   3.813 0.000271 ***\nas.numeric(Tr[indx2])    -0.04581    0.10471  -0.437 0.662962    \nas.matrix(X2[indx2, 11]) -0.04678    0.15885  -0.295 0.769140    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3926 on 79 degrees of freedom\nMultiple R-squared:  0.006661,  Adjusted R-squared:  -0.01849 \nF-statistic: 0.2649 on 2 and 79 DF,  p-value: 0.768\n\nsummary(lm(data$prexpty[indx2]~ as.numeric(Tr[indx2])))\n\n\nCall:\nlm(formula = data$prexpty[indx2] ~ as.numeric(Tr[indx2]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2037 -0.2037 -0.2037 -0.1429  0.8571 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            0.20370    0.05312   3.835 0.000249 ***\nas.numeric(Tr[indx2]) -0.06085    0.09090  -0.669 0.505167    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3903 on 80 degrees of freedom\nMultiple R-squared:  0.00557,   Adjusted R-squared:  -0.00686 \nF-statistic: 0.4481 on 1 and 80 DF,  p-value: 0.5052\n\n\n\nlinreg &lt;- lm(data$prexpty[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,]))\ncat(\"Linear regression effect is\", summary(linreg)$coefficients[2,1], \" with p-value\", \n    summary(linreg)$coefficients[2,4] ,\"\\n\")\n\nLinear regression effect is -0.2155648  with p-value 0.1509716 \n\ncat(\"Matching effect is\", mout$est, \"with p-value\",  2 * (1-pnorm(abs(mout$est/mout$se))),\"\\n\")\n\nMatching effect is -0.4285714 with p-value 0.008730257 \n\n\n\n# Outcome 2: President/PM was member of parliament immediately prior to taking office \nindx2 = indx & !is.na(data$prexmp)\nmout &lt;- Match(Y=data$prexmp[indx2], Tr=Tr[indx2], X=X2[indx2,], estimand=\"ATT\")\nsummary(mout)\n\n\nEstimate...  -0.32143 \nAI SE......  0.18629 \nT-stat.....  -1.7255 \np.val......  0.084444 \n\nOriginal number of observations..............  82 \nOriginal number of treated obs...............  28 \nMatched number of observations...............  28 \nMatched number of observations  (unweighted).  28 \n\nt.test(data$prexmp[Tr==1],data$prexmp[Tr==0])\n\n\n    Welch Two Sample t-test\n\ndata:  data$prexmp[Tr == 1] and data$prexmp[Tr == 0]\nt = -1.8512, df = 64.388, p-value = 0.06873\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.40538134  0.01541269\nsample estimates:\nmean of x mean of y \n0.2413793 0.4363636 \n\nlinreg2 &lt;- summary(lm(data$prexmp[indx2]~ as.numeric(Tr[indx2]) + as.matrix(X2[indx2,])))\n\ncat(\"Linear regression effect is\", linreg2$coefficients[2,1], \" with p-value\", \n    linreg2$coefficients[2,4],\"\\n\")\n\nLinear regression effect is -0.4289868  with p-value 0.01406203 \n\ncat(\"Matching effect is\", mout$est, \"with p-value\",  2 * (1-pnorm(abs(mout$est/mout$se))),\"\\n\")\n\nMatching effect is -0.3214286 with p-value 0.08444445",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "matching.html#additional-links",
    "href": "matching.html#additional-links",
    "title": "Matching",
    "section": "Additional Links",
    "text": "Additional Links\n\nMatching in The Effect Book.\nMatching and Subclassification in Causal Inference: The Mixtape.\nMatchIt Package.\n\n\n\n\n\nCunningham, Scott. n.d. â€œCausal Inference the Mixtape - 5Â  Matching and Subclassification.â€ In. https://mixtape.scunning.com/05-matching_and_subclassification.",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Matching"
    ]
  },
  {
    "objectID": "equivalence.html",
    "href": "equivalence.html",
    "title": "Equivalence",
    "section": "",
    "text": "# Load libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(gridExtra)\n\n# Create a sequence for x-axis (both panels)\nx &lt;- seq(-4, 4, length.out = 100)\n\n# Normal distribution for both panels\ndensity &lt;- dnorm(x)\n\n# Left plot: Difference in Means Test\nplot_left &lt;- ggplot(data.frame(x = x, y = density), aes(x = x, y = y)) +\n  geom_line(size = 1) +\n  geom_area(data = data.frame(x = x[x &lt; -1.96 | x &gt; 1.96], \n                              y = density[x &lt; -1.96 | x &gt; 1.96]), \n            aes(x = x, y = y), fill = \"grey\", alpha = 0.5) +\n  geom_vline(xintercept = c(-1.96, 1.96), linetype = \"dashed\") +\n  labs(title = \"Difference in Means Test\", x = \"diff\", y = \"\") +\n  annotate(\"text\", x = -3, y = 0.02, label = \"Reject H0\\nof no difference\", hjust = 0.5) +\n  annotate(\"text\", x = 3, y = 0.02, label = \"Reject H0\\nof no difference\", hjust = 0.5) +\n  annotate(\"text\", x = 0, y = 0.1, label = \"Fail to reject H0\\nof no difference\", vjust = -0.5) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n# Right plot: Equivalence Test\nplot_right &lt;- ggplot(data.frame(x = x, y = density), aes(x = x, y = y)) +\n  geom_line(size = 1) +\n  geom_area(data = data.frame(x = x[x &gt; -0.5 & x &lt; 0.5], \n                              y = density[x &gt; -0.5 & x &lt; 0.5]), \n            aes(x = x, y = y), fill = \"grey\", alpha = 0.5) +\n  geom_vline(xintercept = c(-0.5, 0.5), linetype = \"dashed\") +\n  labs(title = \"Equivalence Test\", x = \"diff\", y = \"\") +\n  annotate(\"text\", x = 0, y = 0.05, label = \"Reject H0\\nof a difference\", vjust = -0.5) +\n  annotate(\"text\", x = -2, y = 0.02, label = \"Fail to reject H0\\nof a difference\", hjust = 0.5) +\n  annotate(\"text\", x = 2, y = 0.02, label = \"Fail to reject H0\\nof a difference\", hjust = 0.5) +\n  theme_minimal()\n\n# Combine the two plots\ngrid.arrange(plot_left, plot_right, ncol = 2)",
    "crumbs": [
      "Session 2 - Matching and Equivalence",
      "Equivalence"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cunningham, Scott. n.d. â€œCausal Inference the Mixtape - 5Â \nMatching and Subclassification.â€ In. https://mixtape.scunning.com/05-matching_and_subclassification.\n\n\nHuntington-Klein, Nick. n.d. Chapter 14 - Matching | the\nEffect. https://theeffectbook.net/ch-Matching.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "matching-equivalence.html",
    "href": "matching-equivalence.html",
    "title": "Session 2 - Matching and Equivalence",
    "section": "",
    "text": "Ackowledgements\nThe lab builds on the resources:",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  },
  {
    "objectID": "matching-equivalence.html#ackowledgements",
    "href": "matching-equivalence.html#ackowledgements",
    "title": "Session 2 - Matching and Equivalence",
    "section": "",
    "text": "Chapter 14 - Matching (Huntington-Klein, n.d.)\nChapter 5 - Matching and Subclassification (Cunningham, n.d.)\nLab Material for Govt 8003 created by Nicolo Bonifai and Ben Burnley",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  },
  {
    "objectID": "matching-equivalence.html#todays-lab",
    "href": "matching-equivalence.html#todays-lab",
    "title": "Session 2 - Matching and Equivalence",
    "section": "Todayâ€™s Lab",
    "text": "Todayâ€™s Lab\n\nMatching\n\n\nStratification\nExact Matching\nPropensity Score Matching\nChecking the Match\n- On Covariates\n- On Propensity Scores - Comparing Results\n\n\nEquivalence Testing",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  },
  {
    "objectID": "matching-equivalence.html#material",
    "href": "matching-equivalence.html#material",
    "title": "Session 2 - Matching and Equivalence",
    "section": "Material",
    "text": "Material\n\nDownload the 8003 Matching and Equivalence Test Lab Zipped Folder.\nRun the .RProj file.\nOpen matching.qmd in Rstudio.\n\n\n\n\n\nCunningham, Scott. n.d. â€œCausal Inference the Mixtape - 5Â  Matching and Subclassification.â€ In. https://mixtape.scunning.com/05-matching_and_subclassification.\n\n\nHuntington-Klein, Nick. n.d. Chapter 14 - Matching | the Effect. https://theeffectbook.net/ch-Matching.html.",
    "crumbs": [
      "Session 2 - Matching and Equivalence"
    ]
  }
]